---
title: "Getting Started"
author: "Emmeke Aarts"
bibliography: bibliography.bib
link-citations: yes
header-includes:
- \usepackage{fancyhdr}
- \pagestyle{fancy}
- \fancyfoot[CO,CE]{Tutorial to the website of R package `mHMMbayes`}
- \fancyfoot[LE,LO]{E. Aarts}
- \fancyfoot[RE,RO]{\thepage}
output: 
  pdf_document:
  citation_package: biblatex    
vignette: >
  %\VignetteIndexEntry{Estimation of the multilevel hidden Markov model}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---


```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```
<br> <br>
<div style="text-align: justify;">
*This tutorial is a compact version of more extensive and detailed [mHMM tutorial](tutorial-mhmm.html). For more detailed descriptions we send user there.*

## Introduction
The development of this package is motivated by the area of social sciences since it becomes increasingly easy to collect long sequences of data on behavior. That is, we can monitor behavior as it unfolds in real-time. When applying HMMs to such behavioral data, they are used to extract latent behavioral states over time and model the dynamics of behavior over time. \

The package allows modelling using the multilevel framework [see e.g., @altman2007; @shirley2010; @rueda2013; @zhang2014; @deHaan2017]. That way, we can simultaneously model several sequences (e.g., sequences of different persons), while accommodating heterogeneity between persons. We can therefore easily compare the parameters between groups, and investigate, for instance, whether the dynamics between a patient and therapist are different between patients with good and less favorable therapeutic outcomes. \

This tutorial shows how to use the `mHMMbayes` package through an extensive example. We touch on the issues of determining the number of hidden states and checking model convergence, as well as instruct how to simulate data from mHMM with the package. Further information on the estimation methods and algorithms used in the package is given in the vignette [Estimation of the multilevel hidden Markov model](estimation-mhmm.html).


## First steps with `mHMMbayes`
The tutorial makes a use of the embedded example data `nonverbal`. The data contains the nonverbal communication of 10 patient-therapist couples, recorded for 15 minutes at a frequency of 1 observation per second (= 900 observations per couple). The following variables are contained in the dataset:  

* `id`: id variable of patient - therapist couple to distinguish which observation belongs to which couple.
* `p_verbalizing`: verbalizing behavior of the patient, consisting of 1 = not verbalizing, 2 = verbalizing, 3 = back channeling.
* `p_looking`: looking behavior of the patient, consisting of 1 = not looking at therapist, 2 = looking at therapist.
* `t_verbalizing`: verbalizing behavior of the therapist, consisting of 1 = not verbalizing, 2 = verbalizing, 3 = back channeling.
* `t_looking`: looking behavior of the therapist, consisting of 1 = not looking at patient, 2 = looking at patient.
The top 6 rows of the dataset are provided below. 

```{r load package and data, include = FALSE, warning=FALSE}
library(mHMMbayes)
nonverbal <- data.frame(nonverbal)
head(nonverbal)
old_par <- graphics::par(no.readonly =TRUE)
```

The plotted data of the first 5 minutes (= the first 300 observations) of the first couple, looks as follows: 

```{r plot observed data, fig.width = 7.2, fig.height = 3.5, echo = FALSE,warning=FALSE}
# set labels and colors for the observed behavioral categorical outcomes
library(RColorBrewer)
Voc_lab <- c("Not Speaking", "Speaking", "Back channeling")
Look_lab <-  c("Not looking", "Looking")
Voc_col <- c(brewer.pal(3,"PuBuGn")[c(1,3,2)])
Look_col <- c(brewer.pal(3,"YlOrRd")[-3])
cols = list(Voc_col, Look_col, Voc_col, Look_col)

time_s  <- seq(1,900)
couple1 <- cbind(nonverbal[nonverbal$id == 1,], time_s)

par(mar = c(4.3, 6.6, 2.1, 1.1))
plot(x = 1, xlim = c(0,300), ylim = c(0.5,6), type = "n", las = 1, xlab = "Time in minutes", xaxt = "n", yaxt = "n", ylab = "")
axis(2, at = seq(1,4), tick = FALSE, labels = c("P_vocalizing", "P_Looking", "T_vocalizing", "T_Looking"), las = 1)
axis(1, at = seq(0,300,60), tick = TRUE, las = 1, labels = FALSE)
axis(1, at = seq(0,300,60), tick = FALSE, las = 1, labels = seq(1,6,1))
abline(v = seq(0,300,60), col = "gray85")

for(j in 2:5){
  for(i in 1:max(nonverbal[,j])){
    points(x = couple1$time_s[1:300][couple1[1:300,j] == i], 
           y = rep(j-1, sum(couple1[1:300,j] == i)), 
           pch = "|", col = cols[[j-1]][i])
  }
}

legend("topright", bty = "n", fill = Voc_col, legend = Voc_lab)
legend("topleft", bty = "n", fill = Look_col, legend = Look_lab)

graphics::par(old_par)

```

We can observe that both the patient and the therapist are mainly looking at each other during the inspected 5 minutes. During the first minute, the patient is primarily speaking. During the second minute, the therapists starts, after which the patient takes over while the therapist is back channeling. 

### A simple model
To fit a simple 2 state multilevel model with the function `mHMM`, one first has to specify some general model properties and starting values:  
```{r settings and load 2 state model, include = FALSE}
# specifying general model properties:
m <- 2
n_dep <- 4
q_emiss <- c(3, 2, 3, 2)

# specifying starting values
start_TM <- diag(.8, m)
start_TM[lower.tri(start_TM) | upper.tri(start_TM)] <- .2
start_EM <- list(matrix(c(0.05, 0.90, 0.05, 
                          0.90, 0.05, 0.05), byrow = TRUE,
                         nrow = m, ncol = q_emiss[1]), # vocalizing patient
                  matrix(c(0.1, 0.9, 
                           0.1, 0.9), byrow = TRUE, nrow = m,
                         ncol = q_emiss[2]), # looking patient
                  matrix(c(0.90, 0.05, 0.05, 
                           0.05, 0.90, 0.05), byrow = TRUE,
                         nrow = m, ncol = q_emiss[3]), # vocalizing therapist
                  matrix(c(0.1, 0.9, 
                           0.1, 0.9), byrow = TRUE, nrow = m,
                         ncol = q_emiss[4])) # looking therapist

load("nonv_2st_1000it.rda")
out_2st <- out1
```

```{r show specifying 2 state model, eval = FALSE}
library(mHMMbayes)
# specifying general model properties:
m <- 2
n_dep <- 4
q_emiss <- c(3, 2, 3, 2)

# specifying starting values
start_TM <- diag(.8, m)
start_TM[lower.tri(start_TM) | upper.tri(start_TM)] <- .2
start_EM <- list(matrix(c(0.05, 0.90, 0.05, 
                          0.90, 0.05, 0.05), byrow = TRUE,
                         nrow = m, ncol = q_emiss[1]), # vocalizing patient
                  matrix(c(0.1, 0.9, 
                           0.1, 0.9), byrow = TRUE, nrow = m,
                         ncol = q_emiss[2]), # looking patient
                  matrix(c(0.90, 0.05, 0.05, 
                           0.05, 0.90, 0.05), byrow = TRUE,
                         nrow = m, ncol = q_emiss[3]), # vocalizing therapist
                  matrix(c(0.1, 0.9, 
                           0.1, 0.9), byrow = TRUE, nrow = m,
                         ncol = q_emiss[4])) # looking therapist
```
The first line of code loads the `mHMMbayes` package and the `nonverbal` data. Next we specify the general model properties: the number of states used is set by `m <- 2`, the number of dependent variables in the dataset used to infer the hidden states is specified by `n_dep <- 4`, and the number of categorical outcomes for each of the dependent variables is specified by `q_emiss <- c(3, 2, 3, 2)`. 

#### Starting values
The subsequent lines of code specify starting values for both the transition probability matrix `start_TM` and the emission distribution(s) `start_EM`, which are given to the model in the argument `start_val` in model fitting stage. These starting values are used for the first run of the forward backward algorithm and should be based on ones expectation and experience with te data to prevent 'label switching' and speed up convergence. Note that it is strongly advised to check model convergence and label switching (see the section *Checking model convergence and label switching* for an example). For more information on the label switching we send to  [Estimation of the multilevel hidden Markov model](estimation-mhmm.html)) vignette.

#### Prior distributions
As the estimation proceeds within a Bayesian context, a (hyper-)prior distribution has to be defined for the group level parameters, i.e., the group level emission and transition probabilities. Default, non-informative priors are used unless specified otherwise by the user.  

To specify user specific prior distributions, one uses the input option `emiss_hyp_prior` for the emission distribution and `gamma_hyp_prior` for the transition probabilities in the function `mHMM`. These input arguments take an object from the class `mHMM_prior_emiss` and `mHMM_prior_gamma` created by the functions `prior_emiss_cat` and `prior_gamma`, respectively.  Both objects are a list, containing the following key elements:

 * `mu0`, a lists containing the hypothesized hyper-prior mean values of the intercepts of the Multinomial logit model.
 * `K0`,  the number of hypothetical prior subjects on which the set of hyper-prior mean intercepts specified in `mu0` are based.
 * `nu`, degrees of freedom of the hyper-prior Inverse Wishart distribution on the covariance of the Multinomial logit intercepts.
 * `V`, the variance-covariance of the hyper-prior Inverse Wishart distribution on the covariance of the Multinomial logit intercepts.  

Note that `K0`, `nu` and `V` are assumed equal over the states. The mean values of the intercepts (and regression coefficients of the covariates) denoted by `mu0` are allowed to vary over the states. All elements in the list either have the prefix `gamma_` or `emiss_`, depending on which list they belong to. When specifying prior distributions, note that the first element of each row in the probability domain does not have an intercept, as it serves as baseline category in the Multinomial logit regression model. This means, for example, that if we would specify a model with 3 states, `mu0` is a vector with 2 elements, `K0` and `nu` contain 1 element and `V` is a 2 by 2 matrix.  

For a more elaborate explanation on the used (hyper-)prior distributions and their parameters, see the vignette [Estimation of the multilevel hidden Markov model](estimation-mhmm.html). \

#### Fitting the model
The multilevel HMM is fitted using the function `mHMM`: 
```{r show fitting 2 state model, eval = FALSE}
# Run a model without covariate(s) and default priors:
set.seed(14532)
out_2st <- mHMM(s_data = nonverbal, 
                    gen = list(m = m, n_dep = n_dep, q_emiss = q_emiss), 
                    start_val = c(list(start_TM), start_EM),
                    mcmc = list(J = 1000, burn_in = 200))

```
The call to `mHMM` specifies the model with several arguments. The `s_data` argument specifies the input data used to infer the hidden states over time. The `gen` and `start_val` argument specify the general model properties and the starting values, as discussed above. The arguments needed for the MCMC algorithm are given in `mcmc`: `J` specifies the number of iterations used by the hybrid metropolis within Gibbs algorithm and `burn_in` specifies the number of iterations to discard when obtaining the model parameter summary statistics. The function `mHMM` returns an object of class `mHMM`, which has `print` and `summary` methods to see the results. The `print` method provides basic information on the model fitted. That is, the number of subjects in the dataset analyzed, the number of iterations and burn-in period, the average log likelihood over all subjects and model fit indices AIC, the number of states specified, and the number of dependent variables the states are based on:   

```{r show print model}
out_2st
```
The `summary` method provides information on the estimated parameters. That is, the point estimates of the posterior distribution for the transition probability matrix and the emission distribution of each of the dependent variables at the group level: 
```{r show summary model}
summary(out_2st)
```
The resulting model indicates 2 well separated states: one in which the patient is speaking and one in which the therapist is speaking. Looking behavior is quite similar for both the patient and the therapist in the 2 states. Information on the estimated parameters can also be obtained using the function `obtain_gamma` and `obtain_emiss`. These functions allow the user not only to inspect the estimated parameters at the group level, but for each subject individually as well, by specifying the input variable `level = "subject"`: \
```{r show obtain gamma function}
# When not specified, level defaults to "group"
gamma_pop <- obtain_gamma(out_2st)
gamma_pop

# To obtain the subject specific parameter estimates:
gamma_subj <- obtain_gamma(out_2st, level = "subject")
gamma_subj
```
An additional option that the functions `obtain_gamma` and `obtain_emiss` offer is changing the burn-in period used for obtaining the summary statistics, using the input variable `burn_in`. 

### Graphically displaying outcomes
The package includes several plot functions to display the fitted model and its parameters. First, one can plot the posterior densities of a fitted model, for both the transition probability matrix gamma and for the emission distribution probabilities. The posterior densities are plotted for the group level and the subject level simultaneously. For example, for the emission distribution for the variable `p_vocalizing`: 
```{r show plot posterior densities, fig.width = 7.2, fig.height = 4}
library(RColorBrewer)
Voc_col <- c(brewer.pal(3,"PuBuGn")[c(1,3,2)])
Voc_lab <- c("Not Speaking", "Speaking", "Back channeling")

plot(out_2st, component = "emiss", dep = 1, col = Voc_col, 
     dep_lab = c("Patient vocalizing"), cat_lab = Voc_lab)
```
Here, `component` specifies whether we want to visualize the posterior densities for the transition probability matrix gamma (`component = "gamma"`) or for the emission distribution probabilities (`component = "emiss"`), when using `component = "emiss"` the input variable `dep` specifies which dependent variable we want to inspect (as the variable `p_vocolizing` is the first variable in the set, we set `dep = 1`), `col` specifies the colors to be used when plotting the lines, `dep_lab` denotes the label of the dependent variable we are plotting, and `cat_lab` denotes the labels of the categorical outcomes in the dependent variable. In the plot, the solid line visualizes the posterior density at the group level, while each of the dotted lines visualizes the posterior density of one subject. \
Second, one can plot the transition probabilities obtained with the function `obtain_gamma` with a riverplot: 
```{r show plot transition prob, fig.show='hold'}
# Transition probabilities at the group level and for subject number 1, respectively:
plot(gamma_pop, col = rep(rev(brewer.pal(3,"PiYG"))[-2], each = m))
plot(gamma_subj, subj_nr = 1, col = rep(rev(brewer.pal(3,"PiYG"))[-2], each = m))
```

Note that graphically displaying the transition probabilities becomes more informative as the number of states increase. \



```{r load 3 and 4 state models, include = FALSE}
# load("nonv_3st_1000it.rda")
# out_3st <- out2
load("nonv_4st_1000it.rda")
out_4st <- out3
```

### Determining the number of hidden states
The first step in developing a HMM is to determine the number of states $m$ that best describes the observed data, and is a model selection problem. We suggest using a combination of the Akaike Information Criterion (AIC) and the theoretical interpretability of the estimated states to choose between models. 
In the example, the 2, 3 and 4 state model result in an AIC of 3279, 3087, and 2959, respectively. According to model fit indices, the 4 state model is clearly the best model. We check the composition of the states for the model, and the transition probabilities: 
```{r show 4 state model, fig.width = 5, fig.height = 3}
summary(out_4st)

m <- 4
plot(obtain_gamma(out_4st), cex = .5, col = rep(rev(brewer.pal(5,"PiYG"))[-3], each = m))
```

We can see that we have a state in which the patient speaks and the therapist is silent (state 1), a state in which the patient is silent and the therapist speaks (state 2), a state in which both the patient and therapist speak (state 3) and a state in which the therapist speaks but does not look at the patient (in contrast to the looking behavior in all other states), and the patient is silent. In addition, all states are quite stable as the probability of remaining in the same state is above .6 for all states.

### Determining the most likely state sequence 
In order to determine the most likely state sequence, one can either use local decoding, in which the probabilities of the hidden state sequence are obtained simultaneously with the model parameters estimates, or the well-known Viterbi algorithm [@viterbi1967; @forney1973]. In local decoding, the most likely state is determined separately at each time point $t$, in contrast to the Viterbi algorithm in which one determines the joint probability of the complete sequence of observations $O_{1:T}$ and the complete sequence of hidden states $S_{1:T}$. \
In the package, local decoding can be achieved by saving the sampled hidden state sequence at each iteration of the Gibbs sampler, by setting the input variable `return_path = TRUE` for the function `mHMM`. This will result in very large output files, however. Global decoding can be performed by using the function `vit_mHMM`: 
```{r using viterbi algorithm}
state_seq <- vit_mHMM(out_2st, s_data = nonverbal)
 head(state_seq)
```
The function returns the hidden state sequence for each subject in a matrix, where each row represents a point in time and each column represents a subject. We can inspect the obtained hidden state sequence by for example plotting it together with the observed data. Below, the first 5 minutes of the first couple is plotted again, with the addition of the estimated state sequence:

```{r plotting observed data plus inferred states, fig.width = 7.2, fig.height = 4, echo = FALSE}
# set labels and colors for the observed behavioral categorical outcomes
Voc_lab <- c("Not Speaking", "Speaking", "Back channeling")
Look_lab <-  c("Not looking", "Looking")
Voc_col <- c(brewer.pal(3,"PuBuGn")[c(1,3,2)])
Look_col <- c(brewer.pal(3,"YlOrRd")[-3])
cols = list(Voc_col, Look_col, Voc_col, Look_col)

State_col <- c(rev(brewer.pal(3,"PiYG"))[-2])

time_s  <- seq(1,900)
couple1 <- cbind(nonverbal[nonverbal$id == 1,], time_s)

par(mar = c(4.3, 6.6, 2.1, 1.1))
plot(x = 1, xlim = c(0,300), ylim = c(-0.5,6), type = "n", las = 1, xlab = "Time in minutes", xaxt = "n", yaxt = "n", ylab = "")
axis(2, at = seq(0,4), tick = FALSE, labels = c("State", "P_vocalizing", "P_Looking", "T_vocalizing", "T_Looking"), las = 1)
axis(1, at = seq(0,300,60), tick = TRUE, las = 1, labels = FALSE)
axis(1, at = seq(0,300,60), tick = FALSE, las = 1, labels = seq(1,6,1))
abline(v = seq(0,300,60), col = "gray85")

for(j in 2:5){
  for(i in 1:max(nonverbal[,j])){
    points(x = couple1$time_s[1:300][couple1[1:300,j] == i], 
           y = rep(j-1, sum(couple1[1:300,j] == i)), 
           pch = "|", col = cols[[j-1]][i])
  }
}
for(i in 1:2){
  points(x = couple1$time_s[1:300][state_seq[1:300,1] == i], 
           y = rep(0, sum(state_seq[1:300,1] == i)), 
           pch = "|", col = State_col[i])
}

legend("topright", bty = "n", fill = c(Look_col, "white", Voc_col), legend = c(Look_lab, "",Voc_lab),
       ncol = 2, border = c(rep("black", 2), "white", rep("black", 3)))
legend("topleft", bty = "n", fill = State_col, legend = c("State 1", "State 2"))

graphics::par(old_par)

```

### Checking model convergence and label switching
When using Bayesian estimation procedures, it is strongly advised to check model convergence and label switching. That is, one should check if the algorithm reaches the same solution when a set of different (but often conceptually similar) starting values are used, and if label switching is not a problem. With label switching, the label (i.e., which state represents what) ordering of the states switches over the iterations of the estimation algorithm. For example, what started out as state 1, now becomes state 2. One can check model convergence and label switching visually by inspecting the trace plots of parameters of a set of identical models that used varying starting values. Trace plots are plots of the sampled parameter values over the iterations. First, we fit the model with 2 states again, but with different starting values:
```{r loading model convergence, include = FALSE}
# specifying general model properties
m <-2
n_dep <- 4
q_emiss <- c(3, 2, 3, 2)

# specifying different starting values
start_TM <- diag(.8, m)
start_TM[lower.tri(start_TM) | upper.tri(start_TM)] <- .2
start_EM_b <- list(matrix(c(0.2, 0.6, 0.2,
                            0.6, 0.2, 0.2), byrow = TRUE,
                        nrow = m, ncol = q_emiss[1]), # vocalizing patient
                 matrix(c(0.4, 0.6,
                          0.4, 0.6), byrow = TRUE, nrow = m,
                        ncol = q_emiss[2]), # looking patient
                 matrix(c(0.6, 0.2, 0.2,
                          0.2, 0.6, 0.2), byrow = TRUE,
                        nrow = m, ncol = q_emiss[3]), # vocalizing therapist
                 matrix(c(0.4, 0.6,
                          0.4, 0.6), byrow = TRUE, nrow = m,
                        ncol = q_emiss[4])) # looking therapist

load("nonv_2stb_1000it.rda")
out_2st_b <- out1b
```

```{r showing model convergence I, eval= FALSE}
# specifying general model properties
m <-2
n_dep <- 4
q_emiss <- c(3, 2, 3, 2)

# specifying different starting values
start_TM <- diag(.8, m)
start_TM[lower.tri(start_TM) | upper.tri(start_TM)] <- .2
start_EM_b <- list(matrix(c(0.2, 0.6, 0.2,
                            0.6, 0.2, 0.2), byrow = TRUE,
                        nrow = m, ncol = q_emiss[1]), # vocalizing patient
                 matrix(c(0.4, 0.6,
                          0.4, 0.6), byrow = TRUE, nrow = m,
                        ncol = q_emiss[2]), # looking patient
                 matrix(c(0.6, 0.2, 0.2,
                          0.2, 0.6, 0.2), byrow = TRUE,
                        nrow = m, ncol = q_emiss[3]), # vocalizing therapist
                 matrix(c(0.4, 0.6,
                          0.4, 0.6), byrow = TRUE, nrow = m,
                        ncol = q_emiss[4])) # looking therapist

# Run a model identical to out_2st, but with different starting values:
set.seed(9843)
out_2st_b <- mHMM(s_data = nonverbal, 
                      gen = list(m = m, n_dep = n_dep, q_emiss = q_emiss), 
                      start_val = c(list(start_TM), start_EM),
                      mcmc = list(J = 1000, burn_in = 200))

```
The group level parameter estimates of the emission probabilities and the transition probability matrix at each iteration of the estimation algorithm are stored in the objects `emiss_prob_bar` and `gamma_prob_bar`, respectively. The subject level parameter estimates are stored in the object `PD_subj`, where PD is an abbreviation for posterior density. If we, for example, want to inspect the trace plots for the emission probabilities for looking behavior of the patient at the group level, we use the following code:
```{r showing model convergence II trace plots, fig.width = 7.2, fig.height = 7}
par(mfrow = c(m,q_emiss[2]))
for(i in 1:m){
  for(q in 1:q_emiss[2]){
     plot(x = 1:1000, y = out_2st$emiss_prob_bar[[2]][,(i-1) * q_emiss[2] + q], 
          ylim = c(0,1.4), yaxt = 'n', type = "l", ylab = "Transition probability",
          xlab = "Iteration", main = paste("Patient", Look_lab[q], "in state", i), col = "#8da0cb") 
    axis(2, at = seq(0,1, .2), las = 2)
    lines(x = 1:1000, y = out_2st_b$emiss_prob_bar[[2]][,(i-1) * q_emiss[2] + q], col = "#e78ac3")
    legend("topright", col = c("#8da0cb", "#e78ac3"), lwd = 2, 
           legend = c("Starting value set 1", "Starting value set 2"), bty = "n")
  }
}
```

It can be observed that the parameter estimates converge to the same parameter space, and that the chains mix well. Also, there is no evidence of label switching. 

## Simulating data
The `mHMMBayes` package also includes `sim_mHMM` function allowing for simulating data from multilevel hidden markov model.\
In order to simulate data one has to specify some general model properties in `gen` argument: the number of states used is set by `m <- 3`, the number of dependent variables in the dataset used to infer the hidden states is specified by `n_dep <- 1`, and the number of categorical outcomes for each of the dependent variables is specified by `q_emiss <- 4`. Population level parameters' matrices of transition probabilities `gamma` and emission probabilities `emiss_distr` need to be pass with the variances between subjects in the transition probabilities matrix `var_gamma = 1`, and emission distribution(s) `var_emiss = 1` accordingly.
```{r}
# simulating data for 10 subjects with each 100 observations for 1 dependent variable
n_t     <- 100
n       <- 10
m       <- 3
n_dep   <- 1
q_emiss <- 4
#transition probabilities matrix
gamma   <- matrix(c(0.8, 0.1, 0.1,
                    0.2, 0.7, 0.1,
                    0.2, 0.2, 0.6), ncol = m, byrow = TRUE)
#emission distribution matrix
emiss_distr <- list(matrix(c(0.5, 0.5, 0.0, 0.0,
                             0.1, 0.1, 0.8, 0.0,
                             0.0, 0.0, 0.1, 0.9), 
                           nrow = m, ncol = q_emiss, byrow = TRUE))
#call sim_mHMM() function
data1 <- sim_mHMM(n_t = n_t, n = n, 
                  gen = list(m = m, n_dep = n_dep, q_emiss = q_emiss),
                  gamma = gamma, emiss_distr = emiss_distr, 
                  var_gamma = 1, var_emiss = 1)
#the first rows simulated subject-specific observations
head(data1$obs)
# local decoding of states based on the simulated data
head(data1$states)
```

We could also simulated subject-specific transition probability matrices and emission distributions only by simply setting the length of the observed sequence `n_t` to $0$.
```{r}
# Subject-specific transition probability matrices and emission distributions only
n_t <- 0
n <- 5
m <- 3
n_dep   <- 1
q_emiss <- 4
gamma <- matrix(c(0.8, 0.1, 0.1,
                  0.2, 0.7, 0.1,
                  0.2, 0.2, 0.6), ncol = m, byrow = TRUE)
emiss_distr <- list(matrix(c(0.5, 0.5, 0.0, 0.0,
                             0.1, 0.1, 0.8, 0.0,
                             0.0, 0.0, 0.1, 0.9), 
                           nrow = m, ncol = q_emiss, byrow = TRUE))

data2 <- sim_mHMM(n_t = n_t, n = n, 
                  gen = list(m = m, n_dep = n_dep, q_emiss = q_emiss),
                  gamma = gamma, emiss_distr = emiss_distr, 
                  var_gamma = 1, var_emiss = 1)
data2
```


```{r, include = FALSE}
graphics::par(old_par)
```
## References

</div>

