[{"path":"/articles/online_only/estimation-mhmm.html","id":"introduction","dir":"Articles > Online_only","previous_headings":"","what":"Introduction","title":"Estimation of the multilevel hidden Markov model","text":"several methods estimate parameters hidden Markov model (HMM). complete, discuss three methods typically used assumed data generated one (uni-level) model. , Maximum Likelihood approach, Baum Welch algorithm utilizing forward backward probabilities, Bayesian estimation method. Note methods assume number states known context application, .e., specified user. issue determining number states discussed vignette “tutorial-mhmm.” discussing simplified case estimating parameters data consists one observed sequence (, multiple sequences, assuming data generated one identical model), proceed elaborating estimating parameters multilevel hidden Markov model.","code":""},{"path":[]},{"path":"/articles/online_only/estimation-mhmm.html","id":"maximum-likelihood-ml","dir":"Articles > Online_only","previous_headings":"Estimating the parameters of the HMM","what":"Maximum likelihood (ML)","title":"Estimation of the multilevel hidden Markov model","text":"ML estimation can used estimate parameters HMM. relevant likelihood function convenient form: \\[\\begin{equation} L_T = \\mathbf{\\delta P}(o_1) \\mathbf{\\Gamma P}(o_2)\\mathbf{\\Gamma P}(o_3) \\ldots \\mathbf{\\Gamma P}(o_T) \\mathbf{1'}.  \\label{HMMlik} \\end{equation}\\] equation , \\(\\mathbf{P}(o_t)\\) denotes diagonal matrix state-dependent conditional probabilities observing \\(O_t = o\\) entries, \\(\\mathbf{\\delta}\\) denotes distribution initial probabilities \\(\\pi_i\\), \\(\\mathbf{\\Gamma}\\) denotes transition probability matrix, \\(\\mathbf{1'}\\) column vector consisting \\(m\\) (.e., number distinct states) elements value one. See vignette “tutorial-mhmm” explanation quantities. Direct maximization log-likelihood poses problems even long sequences, provided measures taken avoid numerical underflow.","code":""},{"path":"/articles/online_only/estimation-mhmm.html","id":"expectation-maximization-em-or-baum-welch-algorithm","dir":"Articles > Online_only","previous_headings":"Estimating the parameters of the HMM","what":"Expectation Maximization (EM) or Baum-Welch algorithm","title":"Estimation of the multilevel hidden Markov model","text":"EM algorithm (Dempster, Laird, Rubin 1977), context also known Baum-Welch algorithm (Baum et al. 1970; Rabiner 1989), can also used maximize log-likelihood function. , unobserved latent states treated missing data, quantities known forward backward probabilities used obtain ‘complete-data log-likelihood’ HMM parameters: log-likelihood based observed event sequence unobserved, “missing,” latent states. forward probabilities \\(\\boldsymbol{\\alpha}_t ()\\) denote joint probability observed event sequence time point 1 \\(t\\) state \\(S\\) time point \\(t\\) \\(\\): \\[\\begin{equation} \\label{forward} \\boldsymbol{\\alpha}_t () = Pr(O_1 = o_1, O_2 = o_2, \\ldots, O_t = o_t, S_t = ). \\end{equation}\\] name “forward probabilities” derives fact computing forward probabilities \\(\\boldsymbol{\\alpha}_t\\), one evaluates sequence hidden states chronological order (.e., forward time) time point \\(t\\). backward probabilities \\(\\boldsymbol{\\beta}_t ()\\) denote conditional probability observed event sequence time point \\(t\\) end, \\(t+1, t+2, \\ldots ,T\\), given state \\(S\\) time point \\(t\\) equals \\(\\):\\[\\begin{equation} \\boldsymbol{\\beta}_t () = Pr(O_{t+1} = o_{t+1}, O_{t+2} = o_{t+2}, \\ldots, O_T = o_T \\mid S_t = ). \\end{equation}\\] computing backward probabilities \\(\\boldsymbol{\\beta}_t\\), one evaluates sequence hidden states reversed order, .e., \\(S_T, S_{T-1}, \\ldots, S_{t+1}\\). forward backward probabilities together cover complete event sequence \\(t = 1\\) \\(T\\), combined give joint probability complete event sequence state \\(S\\) time point \\(t\\) \\(\\): \\[\\begin{equation} \\boldsymbol{\\alpha}_t ()\\boldsymbol{\\beta}_t () = Pr(O_{1} = o_{1}, O_{2} = o_{2}, \\ldots, O_T = o_T, S_t = ). \\label{eqFW} \\end{equation}\\] refer Cappé (2005) discussion advantages combining forward backward probability information EM algorithm direct maximization likelihood HMM.","code":""},{"path":"/articles/online_only/estimation-mhmm.html","id":"bayesian-estimation","dir":"Articles > Online_only","previous_headings":"Estimating the parameters of the HMM","what":"Bayesian estimation","title":"Estimation of the multilevel hidden Markov model","text":"third approach use Bayesian estimation infer parameters HMM. refer e.g., Gelman et al. (2014), Lynch (2007), Rossi, Allenby, McCulloch (2012) -depth exposition Bayesian statistics. general terms, difference frequentist Bayesian estimation following. frequentist estimation, view parameters fixed entities population, subject sampling fluctuation (quantified standard error estimate). Bayesian estimation, however, assume parameter follows given distribution. general shape distribution determined beforehand, using prior distribution. prior distribution determines shape parameter distribution, also allows giving information model respect likely values parameter estimated. arrive final distribution parameter - called posterior distribution -, prior distribution combined likelihood function data using Bayes’ theorem. , likelihood function provides us probability data given parameters. aspect distributions may interest, emphasis usually mean (median) posterior distribution, serves point estimate parameter interest (analogous frequentist parameter estimates). event one vague expectations possible parameter values, one can specify “non-informative” priors (e.g., uniform distribution). , one can choose parameters prior distributions, called hyper-parameters, parameters may assume wide range possible values. Non-informative priors therefore express lack knowledge. implemented hidden Markov model, transitions state \\(\\) time point \\(t\\) states time point \\(t + 1\\) observed outcomes within state \\(\\) follow categorical distribution, parameter sets \\(\\mathbf{\\Gamma}_i\\) (.e., probabilities row \\(\\) transition probability matrix \\(\\mathbf{\\Gamma}\\)) \\(\\boldsymbol{\\theta}_i\\) (.e., state \\(\\) dependent probabilities observing act). convenient (conjugate) prior distribution parameters categorical distribution (symmetric) Dirichlet distribution. assume rows \\(\\mathbf{\\Gamma}\\) state-dependent probabilities \\(\\boldsymbol{\\theta}_i\\) independent. , \\[\\begin{align} S_{t = 2, \\ldots, T} \\: \\sim \\mathbf{\\Gamma}_{S_{t-1}} \\quad &\\text{} \\quad \\mathbf{\\Gamma}_i \\: \\sim \\text{Dir}(\\mathbf{}_{10}) \\quad \\text{} \\label{gammaDirPrior}\\\\  O_{t = 1, \\ldots, T} \\: \\sim\\boldsymbol{\\theta}_{S_t} \\quad &\\text{} \\quad \\boldsymbol{\\theta}_i    \\: \\sim \\text{Dir}(\\mathbf{}_{20}), \\label{pDirPrior} \\end{align}\\] probability distribution current state \\(S_t\\) given row transition probability matrix \\(\\mathbf{\\Gamma}\\) corresponding previous state hidden state sequence \\(S_{t-1}\\). probability distribution \\(S_t\\) given \\(\\mathbf{\\Gamma}\\) holds states first time point, .e., t starts 2 previous state hidden state sequence state \\(S\\) \\(t = 1\\). probability first state hidden state sequence \\(S_1\\) given initial probabilities states \\(\\pi_i\\). probability distribution observed event \\(O_t\\) given state-dependent probabilities \\(\\boldsymbol{\\theta}_i\\) corresponding current state \\(S_t\\). hyper-parameter \\(\\mathbf{}_{10}\\) prior Dirichlet distribution \\(\\mathbf{\\Gamma}_i\\) vector length equal number states \\(m\\), hyper-parameter \\(\\mathbf{}_{20}\\) prior Dirichlet distribution \\(\\boldsymbol{\\theta}_i\\) vector length equal number categorical outcomes \\(q\\). Note model, hyper-parameter values assumed invariant states \\(\\). initial probabilities states \\(\\pi_i\\) assumed coincide stationary distribution \\(\\boldsymbol{\\Gamma}\\) therefore independent (--estimated) parameters. Given distributions, goal construct joint posterior distribution hidden state sequence parameter estimates, given observed event sequence hyper-parameters \\[\\begin{equation} \\Pr\\big((S_t), \\mathbf{\\Gamma}_i ,  \\boldsymbol{\\theta}_i \\mid (O_t)\\big) \\propto \\Pr\\big((O_t) \\mid (S_t), \\boldsymbol{\\theta}_i  \\big) \\Pr\\big((S_t) \\mid \\mathbf{\\Gamma}_i  \\big) \\Pr\\big(\\mathbf{\\Gamma}_i  \\mid \\mathbf{}_{10} \\big) \\Pr\\big(\\boldsymbol{\\theta}_i \\mid \\mathbf{}_{20} \\big) % \\mathbf{}_{10}, \\mathbf{}_{20} \\end{equation}\\] drawing samples posterior distribution. applying Gibbs sampler, can iteratively sample appropriate conditional posterior distributions \\(S_t\\), \\(\\mathbf{\\Gamma}_i\\) \\(\\boldsymbol{\\theta}_i\\), given remaining parameters model. short, Gibbs sampler iterates following two steps: first hidden state sequence \\(S_1, S_2, \\ldots, S_T\\) sampled, given, observed event sequence \\(O_1, O_2, \\ldots, O_T\\), current values parameters \\(\\mathbf{\\Gamma}\\) \\(\\boldsymbol{\\theta}_i\\). Subsequently, remaining parameters model (\\(\\mathbf{\\Gamma}_i\\) \\(\\boldsymbol{\\theta}_i\\)) updated sampling conditional sampled hidden state sequence \\(S_1, S_2, \\ldots, S_T\\) observed event sequence \\(O_1, O_2, \\ldots, O_T\\). Sampling hidden state sequence HMM means Gibbs sampler can performed various ways. , use approach outlined Scott (2002). , use forward-backward Gibbs sampler, first forward probabilities \\(\\boldsymbol{\\alpha}_{t}()\\) (.e., joint probability state \\(S = \\) time point \\(t\\) observed event sequence time point 1 \\(t\\)) given equation  obtained, hidden state sequence sampled backward run (.e., drawing \\(S_T, S_{T-1}, \\ldots, S_1\\)) using corresponding forward probabilities \\(\\boldsymbol{\\alpha}_{T:1}\\). forward-backward Gibbs sampler produces sampled values rapidly represent complete area posterior distribution, produces useful quantities byproducts, log-likelihood observed data given current draws parameters iteration (Scott 2002). section “” , provide detailed description Gibbs sampler proceeds HMM. generally takes number iterations Gibbs sampler converges appropriate region posterior distribution, initial iterations usually discarded ‘burn-’ period. remaining sampled values \\(\\mathbf{\\Gamma}_i\\) \\(\\boldsymbol{\\theta}_i\\) provide posterior distributions respective parameters. problem can arise using Bayesian estimation context “label switching,” .e., hidden states HMM priori ordering interpretation, labels (.e., state represents ) can switch iterations Gibbs sampler, without affecting likelihood model (see e.g., Scott 2002; Jasra, Holmes, Stephens 2005). result, marginal posterior distributions parameters impossible interpret represent distribution multiple states. Sometimes, using reasonable starting values (.e., user-specified parameter values “zero-th” iteration used start MCMC sampler) suffices prevent label switching. Otherwise, possible solutions set constraints parameters state-dependent distribution, use (weakly) informative priors state-dependent distributions (Scott 2002). Hence, making inferences obtained marginal distributions, one first assess problem label switching present (e.g., using plots sampled parameter values state-dependent distributions iterations), necessary, take steps prevent problem label switching. experience, use reasonable starting values always sufficed prevent label switching. EM Bayesian Gibbs sampling viable inferential procedures HMMs, complex HMMs multilevel HMMs, Bayesian estimation method several advantages (e.g., lower computational cost, less computation time) EM algorithm. refer Rydén (2008) comparison frequentist (.e., EM algorithm) Bayesian approaches.","code":""},{"path":[]},{"path":"/articles/online_only/estimation-mhmm.html","id":"bayesian-estimation-of-multilevel-models","dir":"Articles > Online_only","previous_headings":"Estimating the parameters of the multilevel HMM","what":"Bayesian estimation of multilevel models","title":"Estimation of the multilevel hidden Markov model","text":"Bayesian estimation particularly suited model multilevel models. multilevel model, multi-layered structure parameters. HMM, subject level parameters first level pertaining observations within subject, group level parameters second level describe mean variation within group, inferred sample subjects. illustrate multilevel model, suppose \\(K\\) subjects \\(H\\) observations number cups coffee consumed per day \\(y\\), .e., subject \\(k \\\\{1, 2, \\ldots, K\\}\\) observation \\(h \\\\{1, 2, \\ldots, H\\}\\). Hence, first level, daily observations coffee consumption within subjects: \\(y_{11}\\), \\(y_{12}\\), \\(\\ldots\\), \\(y_{1H}\\), \\(y_{21}\\), \\(y_{22}\\), \\(\\ldots\\), \\(y_{2H}\\), \\(y_{K1}\\), \\(y_{K2}\\), \\(\\ldots\\), \\(y_{KH}\\). Using multilevel model, observations subject distributed according distribution \\(Q\\), subject parameter set \\(\\theta_k\\). : \\[\\begin{equation} y_{kh} \\sim Q(\\theta_k).  \\end{equation}\\] addition, subject-specific parameter sets \\(\\theta_k\\) realizations common group level distribution \\(W\\) parameter set \\(\\Lambda\\): \\[\\begin{equation} \\theta_k \\sim W(\\Lambda). \\end{equation}\\] , multilevel model, subject level model parameters pertain observations within subject assumed random draws given distribution, , , denoted “random,” independent used estimation method (.e., Bayesian classical frequentist estimation). multi-layered structure fits naturally Bayesian paradigm since Bayesian estimation, model parameters definition viewed random. , parameters follow given distribution, prior distribution expresses prior expectations respect likely values model parameters. multilevel model, prior expectations subject level model parameter values reflected group level distribution. Hence, Bayesian estimation, prior distribution subject level parameters, given group level distribution. group level distribution provides information location (e.g., mean) subject level (.e., subject-specific) parameters, variation subject level parameters. Normal distribution flexible distribution parameters easily relate interpretation, group level distribution often taken normal distribution. illustrate notion group level (prior) distribution, suppose assume Poisson distribution observations daily coffee consumption within subject k, Normal group level distribution Poisson mean. case, set hyper-parameters (.e., parameters group level distribution, mean (\\(\\Lambda_\\mu\\)) variance (\\(\\Lambda_{\\sigma^2}\\)) Normal distribution) Poisson mean denote group mean number cups coffee consumed per day subjects, variation mean number cups coffee consumed per day subjects. Finally, fitting multilevel model using Bayesian estimation, prior distribution placed hyper-parameters. Prior distributions hyper-parameters referred hyper-priors allow hyper-parameters distribution instead fixed. , parameters characterize group level prior distribution (.e., hyper-parameters) now also quantities interest (.e., --estimated), viewed random Bayesian estimation methods. randomness hyper-parameters thus specific Bayesian estimation method multilevel model, contrast randomness subject level parameters. continue example, hyper-prior mean Normal prior distribution subject level mean cups coffee consumed daily denote prior belief mean number cups coffee consumed per day group. hyper-prior variance Normal prior distribution subject level mean cups coffee consumed per day denote prior belief much mean number cups coffee varies subjects. Often, hyper-prior distribution values chosen vague (.e., informative), like uniform distribution: \\[\\begin{align} \\Lambda_\\mu & \\: \\sim U(0, 20),\\\\ \\nonumber \\Lambda_{\\sigma^2} & \\:  \\sim U(0, 500). \\end{align}\\] See e.g., Gelman et al. (2014), Lynch (2007), Rossi, Allenby, McCulloch (2012) -depth exposition various multilevel Bayesian models e.g. Snijders Bosker (2011), Hox, Moerbeek, Schoot (2017), Goldstein (2011) coverage classical, frequentist approach multilevel (also called hierarchical random effects) models. present implemented multilevel model pertains data comprised (multivariate) categorical observations, , possibly, time invariant covariates (.e., covariate, one value per subject). , data comprised \\(O_{d, k, t}\\) observations categorical outcome(s) categorical outcome variable \\(d = 1, 2, \\ldots, D\\) subject \\(k = 1, 2, \\ldots, K\\) time point \\(t = 1, 2, \\ldots, T\\). addition, matrix \\(\\boldsymbol{X}\\) consists \\(k\\) covariate vectors length \\(p \\times 1\\), \\(\\boldsymbol{X}_k = (X_{k1}, X_{k2}, \\ldots, X_{kp})\\). yet, explanation estimation procedure vignette restricted univariate case simplicity. , instance one observed categorical outcome variable per subject, outcome data comprised \\(O_{kt}\\) observations. However, explanation extents quite naturally multivariate case. Given observations, construct multilevel model parameters HMM \\(q\\) observable categorical outcomes, \\(m\\) hidden states, possibly predicted \\(p\\) covariates. Using multilevel framework, parameter assumed follow given distribution, parameter value given subject represents draw common (.e., group level) distribution. Hence, multilevel Bayesian HMM, parameters : subject-specific transition probability matrix \\(\\boldsymbol{\\Gamma}_k\\) transition probabilities \\(\\gamma_{kij}\\) subject-specific state-dependent probability distribution denoting subject-specific probabilities \\(\\boldsymbol{\\theta}_{ki}\\) categorical outcomes within state \\(\\). initial probabilities states \\(\\pi_{k,j}\\) estimated \\(\\pi_{k}\\) assumed stationary distribution \\(\\boldsymbol{\\Gamma}_k\\). Subsequently, parameter values subjects assumed realizations model component state specific (multivariate) Normal distribution. discuss multilevel model two components HMM (\\(\\boldsymbol{\\Gamma}_k\\) \\(\\boldsymbol{\\theta}_{ki}\\) separately. Table  provides overview used symbols multilevel models related two components HMM. use subscript 0 denote values hyper-prior distribution parameters.","code":""},{"path":"/articles/online_only/estimation-mhmm.html","id":"multilevel-model-for-the-state-dependent-probabilities-boldsymboltheta_ki","dir":"Articles > Online_only","previous_headings":"Estimating the parameters of the multilevel HMM","what":"Multilevel model for the state-dependent probabilities \\(\\boldsymbol{\\theta}_{ki}\\)","title":"Estimation of the multilevel hidden Markov model","text":"standard (non-multilevel) Bayesian HMM estimation, specified Dirichlet prior distribution state-dependent probabilities \\(\\boldsymbol{\\theta}_{}\\). provide flexible model allows inclusion random effects (time invariant) covariates, follow Altman (2007) extend subject-specific state-dependent probabilities \\(\\boldsymbol{\\theta}_{ki}\\) Multinomial logit (MNL) model. Hence, utilize linear predictor function estimate probability observing categorical outcome \\(l\\) within state \\(\\). state \\(\\) specific linear predictor function subject level consists \\(q-1\\) random intercepts (.e., subject intercept). , categorical outcome \\(l\\) intercept, exception first categorical outcome set intercept omitted reasons model identification (.e., probabilities can estimated freely within subject \\(k\\) state \\(\\), probabilities need add 1). making intercepts random (.e., subject intercept), accommodate heterogeneity subjects state conditional probabilities. Hence, MNL model \\(\\boldsymbol{\\theta}_{ki}\\), subject \\(k\\)’s probabilities observing categorical outcome \\(l \\\\{1, 2, \\ldots, q \\}\\) within state \\(\\\\{1, 2, \\ldots , m\\}\\), \\({\\theta}_{kil}\\), modeled using \\(m\\) batches \\(q-1\\) random intercepts, \\(\\boldsymbol{\\alpha}_{(O)ki} = (\\alpha_{(O)ki2}, \\alpha_{(O)ki3}, \\ldots, \\alpha_{(O)kiq})\\). , \\[\\begin{equation} {\\theta}_{kil} = \\frac{\\text{exp}(\\alpha_{(O)kil})}{1 + \\sum_{\\bar{l} = 2}^q \\text{exp}(\\alpha_{(O)ki\\bar{l}})}, \\end{equation}\\] \\(K\\), \\(m\\), \\(q\\) number subjects, states, categorical outcomes, respectively. numerator set equal one \\(l = 1\\), making first categorical outcome set baseline category every state. group level, subject-level intercepts (possibly) partly determined covariates differentiate subjects. Thus, addition subject level random intercepts \\(\\boldsymbol{\\alpha}_{(O)ki}\\), \\(m\\) matrices \\(p * (q-1)\\) fixed regression coefficients, \\(\\boldsymbol{\\beta}_{(O)}\\), \\(p\\) denotes number used covariates. columns \\(\\boldsymbol{\\beta}_{(O)}\\) \\(\\boldsymbol{\\beta}_{(O)il} = (\\beta_{(O)il1}, \\beta_{(O)il2}, \\ldots, \\beta_{(O)ilp})\\) model random intercepts state \\(\\) categorical outcome \\(l\\) given \\(p\\) covariates. Combining terms, batch random intercepts \\(\\boldsymbol{\\alpha}_{(O)ki}\\) (.e., batch \\(q-1\\) intercepts state \\(\\) conditional probabilities categorical outcome subject \\(k\\)) come state \\(\\) specific population level multivariate Normal distribution, mean vector \\(\\boldsymbol{\\bar{\\alpha}}_{(O)} + \\boldsymbol{X_{k}^\\top} \\boldsymbol{\\beta_{(O)il}}\\) length \\(q-1\\), covariance \\(\\Phi_{}\\) denotes covariance \\(q-1\\) state \\(\\) specific intercepts subjects models dependence probabilities categorical outcomes within state \\(\\) (.e., specify state specific multivariate Normal prior distribution subject-specific \\(\\boldsymbol{\\alpha_{(O)ki}}\\) parameters). convenient hyper-prior hyper-parameters group level prior distribution multivariate Normal distribution mean vector \\(\\boldsymbol{\\bar{\\alpha}}_{(O)}\\) fixed regression coefficients \\(\\boldsymbol{\\beta_{(O)il}}\\), Inverse Wishart distribution covariance \\(\\Phi_{}\\) (see e.g., Gelman et al. 2014). , \\[\\begin{align} O_{kt} \\: \\sim \\boldsymbol{\\theta}_{k, S_{kt}}  & \\quad \\text{} \\quad \\boldsymbol{\\theta}_{ki} \\: \\sim \\text{MNL}\\big(\\boldsymbol{\\alpha}_{(O)ki}\\big), \\label{pPriorHier}\\\\ \\boldsymbol{\\alpha}_{(O)ki}     \\: \\sim \\text{N}\\big(\\boldsymbol{\\bar{\\alpha}}_{(O)} + \\boldsymbol{X_{k}^\\top} \\boldsymbol{\\beta_{(O)il}}, \\Phi_{}\\big)   & \\quad \\text{} \\quad  \\boldsymbol{\\bar{\\alpha}}_{(O)} \\: \\sim \\text{N}\\big(\\boldsymbol{\\alpha}_{(O)0}, \\tfrac{1}{K_0}\\Phi_{} \\big), \\label{popPHier}\\\\                         & \\quad \\text{} \\quad  \\boldsymbol{\\beta}_{(O)} \\: \\sim \\text{N}\\big(\\boldsymbol{\\beta}_{(O)0}, \\tfrac{1}{K_0}\\Phi_{} \\big), \\\\                                                 & \\quad \\text{} \\quad  \\: \\Phi_{} \\: \\sim \\text{IW}\\big(\\Phi_0, df_0 \\big), \\nonumber                                \\end{align}\\] probability distribution observed categorical outcomes \\(O_{kt}\\) given subject \\(k\\) specific state-dependent probabilities \\(\\boldsymbol{\\theta}_{ki}\\) corresponding current state \\(S_{kt}\\) (\\(t\\) indicates time point, see Table ). parameters \\(\\boldsymbol{\\alpha}_{(O)0}\\), \\(\\boldsymbol{\\beta}_{(O)0}\\), \\(K_0\\) denote values parameters hyper-prior group (mean) vector \\(\\boldsymbol{\\bar{\\alpha}}_{(O)}\\) \\(\\boldsymbol{\\beta}_{(O)}\\), respectively. , \\(\\boldsymbol{\\alpha}_{(O)0}\\) \\(\\boldsymbol{\\beta}_{(O)0}\\) represent vector means \\(K_0\\) denotes number observations (.e., number hypothetical prior subjects) prior mean vector \\(\\boldsymbol{\\alpha}_{(O)0}\\) \\(\\boldsymbol{\\beta}_{(O)0}\\) based, .e., \\(K_0\\) determines weight prior \\(\\boldsymbol{\\bar{\\alpha}}_{(O)}\\) \\(\\boldsymbol{\\beta}_{(O)}\\). parameters \\(\\Phi_0\\) \\(df_0\\), respectively, denote values covariance degrees freedom hyper-prior Inverse Wishart distribution population variance \\(\\Phi_{}\\) subject-specific random intercepts \\(\\boldsymbol{\\alpha}_{(O)ki}\\). Note chose values parameters hyper-prior distributions result uninformative hyper-prior distributions, values parameters hyper-priors assumed invariant states \\(\\).","code":""},{"path":"/articles/online_only/estimation-mhmm.html","id":"multilevel-model-for-the-transition-probability-matrix-boldsymbolgamma_k-with-transition-probabilities-boldsymbolgamma_kij","dir":"Articles > Online_only","previous_headings":"Estimating the parameters of the multilevel HMM","what":"Multilevel model for the transition probability matrix \\(\\boldsymbol{\\Gamma_k}\\) with transition probabilities \\(\\boldsymbol{\\gamma_{kij}}\\)","title":"Estimation of the multilevel hidden Markov model","text":"Similar state-dependent probabilities \\(\\boldsymbol{\\theta}_{ki}\\), extend set state \\(\\) specific state transition probabilities \\(\\gamma_{kij}\\) MNL model allow inclusion random effects (time invariant) covariates. Hence, use linear predictor function estimate probability transition behavioral state \\(\\) state \\(j\\). linear predictor function consists \\(m-1\\) random intercepts allow heterogeneity subjects probabilities switch states. , within row \\(\\) transition probability matrix \\(\\boldsymbol{\\Gamma_k}\\), state \\(j\\) intercept, intercept relates transitioning first state set omitted reasons model identification (.e., probabilities can estimated freely row-probabilities need add 1). Hence, subject’s probability transition behavioral state \\(\\\\{1, 2, \\ldots, m \\}\\) state \\(j \\\\{1, 2, \\ldots, m \\}\\) modeled using \\(m\\) batches \\(m-1\\) random intercepts, \\(\\boldsymbol{\\alpha}_{(S)ki} = (\\alpha_{(S)k13}, \\ldots, \\alpha_{(S)k1m}, \\alpha_{(S)k23}, \\ldots, \\alpha_{(S)k2m},\\) \\(\\ldots, \\alpha_{(S)km2},\\) \\(\\dots,\\) \\(\\alpha_{(S)km(m-1)})\\). , \\[\\begin{equation} \\gamma_{kij} = \\frac{\\text{exp}(\\alpha_{(S)kij})}{1 + \\sum_{\\bar{j} \\Z} \\text{exp}(\\alpha_{(S)ki\\bar{j}})}, \\end{equation}\\] \\[\\begin{equation} \\text{} \\ Z \\\\{2, \\ldots, m\\} \\nonumber \\end{equation}\\] \\(K\\) \\(m\\) number subjects dataset, distinct number states, respectively. numerator set equal 1 \\(j = 1\\), making first state every row transition probability matrix \\(\\boldsymbol{\\Gamma}_k\\) baseline category. group level, subject-level intercepts (possibly) partly determined covariates differentiate subjects. Thus, addition subject level random intercepts \\(\\boldsymbol{\\alpha}_{(S)ki}\\), \\(m\\) matrices \\(p * (q-1)\\) fixed regression coefficients, \\(\\boldsymbol{\\beta}_{(S)}\\), \\(p\\) denotes number used covariates. columns \\(\\boldsymbol{\\beta}_{(S)}\\) \\(\\boldsymbol{\\beta}_{(S)ij} = (\\beta_{(S)ij1}, \\beta_{(S)ij2}, \\ldots, \\beta_{(S)ijp})\\) model random intercepts denoting probability transitioning behavioral state \\(\\) state \\(j\\) given \\(p\\) covariates. Combining terms, batch random intercepts \\(\\boldsymbol{\\alpha}_{(S)ki}\\) come state \\(\\) specific population level multivariate Normal distribution, mean vector \\(\\boldsymbol{\\bar{\\alpha}}_{(S)} + \\boldsymbol{X_{k}^\\top} \\boldsymbol{\\beta_{(S)ij}}\\) length \\(q-1\\), covariance \\(\\Psi_i{}\\) denotes covariance \\(q-1\\) state \\(\\) specific intercepts subjects, models dependency probabilities states within random intercept batch \\(\\boldsymbol{\\alpha}_{(S)ki}\\) (.e., specify state specific multivariate Normal prior distribution subject-specific \\(\\boldsymbol{\\alpha}_{(S)ki}\\) parameters). convenient hyper-prior hyper-parameters group level prior distribution multivariate Normal distribution mean vector \\(\\boldsymbol{\\bar{\\alpha}}_{(S)}\\) fixed regression coefficients \\(\\boldsymbol{\\beta_{(S)il}}\\) Inverse Wishart distribution covariance \\(\\Psi_i\\). , \\[\\begin{align} S_{k, t = 2, \\ldots, T} \\: \\sim \\boldsymbol{\\Gamma}_{k, S_{k, t-1}} \\quad &\\text{} \\quad \\boldsymbol{\\Gamma}_{k, }  \\: \\sim \\text{MNL}\\big(\\boldsymbol{\\alpha}_{(S)ki}\\big), \\label{tpmPriorHier}\\\\ \\boldsymbol{\\alpha}_{(S)ki} \\: \\sim \\text{N}\\big(\\boldsymbol{\\bar{\\alpha}}_{(S)} + \\boldsymbol{X_{k}^\\top} \\boldsymbol{\\beta_{(S)il}}, \\Psi_i\\big) \\quad &\\text{} \\quad   \\boldsymbol{\\bar{\\alpha}}_{(S)} \\: \\sim \\text{N}\\big(\\boldsymbol{\\alpha}_{(S)0}, \\tfrac{1}{K_0}\\Psi_i \\big),  \\label{popTmpHier} \\\\         &\\text{} \\quad  \\boldsymbol{\\beta}_{(S)} \\: \\sim \\text{N}\\big(\\boldsymbol{\\beta}_{(S)0}, \\tfrac{1}{K_0}\\Psi_{} \\big), \\\\         &\\text{} \\quad \\:   \\Psi_i \\: \\sim \\text{IW}\\big(\\Psi_0, df_0 \\big), \\nonumber \\end{align}\\] subject-specific probability distribution current state \\(S_{kt}\\) given row transition probability matrix \\(\\mathbf{\\Gamma}_k\\) corresponding previous state hidden state sequence \\(S_{k, t-1}\\). probability distribution \\(S_{kt}\\) given \\(\\mathbf{\\Gamma}_k\\) holds states first time point, .e., \\(t\\) starts 2 previous state hidden state sequence state \\(S_{kt}\\) \\(t\\) = 1. probability first state hidden state sequence \\(S_{k,1}\\) given initial probabilities states \\(\\pi_{k,j}\\). parameters \\(\\boldsymbol{\\alpha}_{(S)0}\\), \\(\\boldsymbol{\\beta}_{(S)0}\\), \\(K_0\\) denote values parameters hyper-prior group (mean) vector \\(\\boldsymbol{\\bar{\\alpha}}_{(S)}\\) \\(\\boldsymbol{\\beta}_{(S)}\\), respectively. , \\(\\boldsymbol{\\alpha}_{(S)0}\\) \\(\\boldsymbol{\\beta}_{(S)0}\\) represent vector means \\(K_0\\) denotes number observations (.e., number hypothetical prior subjects) prior mean vector \\(\\boldsymbol{\\alpha}_{(S)0}\\) \\(\\boldsymbol{\\beta}_{(S)0}\\) based. parameters \\(\\Psi_0\\) \\(df_0\\), respectively, denote values covariance degrees freedom hyper-prior Inverse Wishart distribution group variance \\(\\Psi_i\\) subject-specific random intercepts \\(\\boldsymbol{\\alpha}_{(S)ki}\\).","code":""},{"path":"/articles/online_only/estimation-mhmm.html","id":"hybrid-metropolis-within-gibbs-sampler-used-to-fit-the-multilevel-hmm","dir":"Articles > Online_only","previous_headings":"","what":"Hybrid Metropolis within Gibbs sampler used to fit the multilevel HMM","title":"Estimation of the multilevel hidden Markov model","text":"Given distributions, goal construct joint posterior distribution parameters - .e., subject-specific hidden state sequences, subject level (.e., subject-specific) parameters group level parameter estimates - given observations (.e., observed event sequences \\(k\\) subjects analyzed simultaneously one group, hyper-prior parameter values) \\[\\begin{align} & \\Pr\\big(S_{kt}, \\mathbf{\\Gamma}_{ki}, \\boldsymbol{\\alpha}_{(S)ki}, \\boldsymbol{\\bar{\\alpha}}_{(S)}, \\boldsymbol{\\beta}_{(S)}, \\Psi_i, \\boldsymbol{\\theta}_{ki}, \\boldsymbol{\\alpha}_{(O)ki}, \\boldsymbol{\\bar{\\alpha}}_{(O)},\\boldsymbol{\\beta}_{(O)ki}, \\Phi_{} \\mid O_{kt}, \\boldsymbol{X}\\big) \\propto \\Pr\\big(O_{kt} \\mid S_{kt},  \\boldsymbol{\\theta}_{ki}\\big)  \\Pr\\big(S_{kt} \\mid \\mathbf{\\Gamma}_{ki}\\big) \\Pr\\big( \\boldsymbol{\\theta}_{} \\mid \\boldsymbol{\\alpha}_{(O)ki} \\big)  \\Pr\\big(\\mathbf{\\Gamma}_i  \\mid \\boldsymbol{\\alpha}_{(S)ki} \\big)\\Pr\\big( \\boldsymbol{\\alpha}_{(O)ki} \\mid \\boldsymbol{\\bar{\\alpha}}_{(O)}, \\boldsymbol{X}, \\boldsymbol{\\beta}_{(O)}, \\Phi_{}\\big)\\Pr\\big(\\boldsymbol{\\alpha}_{(S)ki} \\mid  \\boldsymbol{\\bar{\\alpha}}_{(S)}, \\boldsymbol{X}, \\boldsymbol{\\beta}_{(S)}, \\Psi_i \\big) \\Pr\\big(\\boldsymbol{\\bar{\\alpha}}_{(O)} \\mid  \\boldsymbol{\\alpha}_{(O)0}, K_0, \\Phi_i \\big) \\Pr\\big(\\boldsymbol{{\\beta}}_{(O)} \\mid  \\boldsymbol{\\beta}_{(O)0}, K_0, \\Phi_i \\big)\\Pr\\big(\\Phi_{} \\mid \\Phi_0, df_0 \\big)\\Pr\\big(\\boldsymbol{\\bar{\\alpha}}_{(S)} \\mid  \\boldsymbol{\\alpha}_{(S)0}, K_0, \\Psi_{} \\big) \\Pr\\big(\\boldsymbol{{\\beta}}_{(S)} \\mid  \\boldsymbol{\\beta}_{(S)0}, K_0, \\Psi_{} \\big) \\Pr\\big(\\Psi_i \\mid \\Psi_0, df_0 \\big) \\nonumber \\end{align}\\] drawing samples posterior distribution. follow MCMC sampler algorithm iteratively sample appropriate conditional posterior distributions \\(\\boldsymbol{\\alpha}_{(O)ki}\\), \\(\\boldsymbol{\\alpha}_{(S)ki}\\), \\(\\boldsymbol{\\bar{\\alpha}}_{(O)}\\), \\(\\boldsymbol{{\\beta}}_{(O)}\\), \\(\\Phi_{}\\), \\(\\boldsymbol{\\bar{\\alpha}}_{(S)}\\), \\(\\boldsymbol{{\\beta}}_{(S)}\\), \\(\\Psi_i\\) given remaining parameters model (see ). conditional posterior distributions parameters provided Section “” Bayesian estimation, preferable use natural conjugate prior prior distribution, conveniently results closed form expression (conditional) posterior distribution(s), making Gibbs sampling possible. However, non-conjugate Normal prior provides much intuitive interpretation prior group level distribution compared using natural conjugate prior MNL model, since asymptotic Normal approximation excellent MNL likelihood (Rossi, Allenby, McCulloch 2012), opt former use conjugate prior MNL model. Therefore, use Gibbs sampler update parameters subject-specific state-dependent distributions subject-specific transition probabilities, \\(\\boldsymbol{\\alpha}_{(O)ki}\\) \\(\\boldsymbol{\\alpha}_{(S)ki}\\), respectively. Instead, use combination Gibbs sampler Metropolis algorithm, .e., Hybrid Metropolis within Gibbs sampler. , use Metropolis sampler update \\(\\boldsymbol{\\alpha}_{(O)ki}\\) \\(\\boldsymbol{\\alpha}_{(S)ki}\\), use Gibbs sampler update model parameters. various types Metropolis algorithms, type involves specific choices. Simulation studies showed , line Rossi, Allenby, McCulloch (2012), Random Walk (RW) Metropolis sampler outperformed Independence Metropolis sampler terms efficiency estimating parameters (multilevel) HMM, chose use RW Metropolis sampler update parameters subject-specific state-dependent distributions (\\(\\boldsymbol{\\alpha}_{(O)ki}\\)) subject-specific state transition probabilities (\\(\\boldsymbol{\\alpha}_{(S)ki}\\)) Hybrid Metropolis within Gibbs sampler. Hybrid Metropolis within Gibbs sampler multilevel HMM proceeds similar fashion Gibbs sampler HMM: first hidden state sequences sampled (subject separately), (subject level group level) parameters sampled given observed event sequence (subject, \\(O_{kt}\\)), sampled hidden state sequences (subject, \\(S_{kt}\\)), current values remaining parameters model. provide stepwise walkthrough hybrid Metropolis within Gibbs sampler multilevel HMM .","code":""},{"path":"/articles/online_only/estimation-mhmm.html","id":"stepwise-walkthrough-of-the-used-hybrid-metropolis-within-gibbs-sampler","dir":"Articles > Online_only","previous_headings":"Hybrid Metropolis within Gibbs sampler used to fit the multilevel HMM","what":"Stepwise walkthrough of the used hybrid Metropolis within Gibbs sampler","title":"Estimation of the multilevel hidden Markov model","text":"Hybrid Metropolis within Gibbs sampler used fit multilevel HMM proceeds described . use subscript \\(c\\) denote current (.e., updated using combination value hyper-prior data) parameters conditional posterior distributions. steps repeated large number iterations, , discarding first iterations “burn-” period, sampled parameter estimates provide empirical posterior distribution model parameters. Regarding acceptance rate RW Metropolis sampler subject-specific sets intercepts \\(\\boldsymbol{\\alpha}_{(O)ki}\\) \\(\\boldsymbol{\\alpha}_{(S)ki}\\) (.e., related subject-specific state-dependent probabilities \\(\\boldsymbol{\\theta}_{ki}\\) subject-specific state transition probability matrix \\(\\boldsymbol{\\Gamma}_k\\), respectively), acceptance rate \\(\\sim23\\%\\) considered optimal many parameters updated (Gelman et al. 2014). Within R package mHMMbayes, number accepted draws model stored emiss_naccept gamma_naccept conditional distributions transition probabilities, respectively.","code":""},{"path":"/articles/online_only/estimation-mhmm.html","id":"scaling-the-proposal-distribution-of-the-rw-metropolis-sampler","dir":"Articles > Online_only","previous_headings":"Hybrid Metropolis within Gibbs sampler used to fit the multilevel HMM","what":"Scaling the proposal distribution of the RW Metropolis sampler","title":"Estimation of the multilevel hidden Markov model","text":"obtain scale parameter \\(\\Sigma_{\\alpha(O)ki}\\) \\(\\Sigma_{\\alpha(S) ki}\\) proposal distributions RW Metropolis sampler \\(\\boldsymbol{\\alpha}_{(O)ki}\\) \\(\\boldsymbol{\\alpha}_{(S)ki}\\), respectively, followed method outlined Rossi, Allenby, McCulloch (2012), several advantages discussed . general challenge RW Metropolis sampler “tuned” choosing scale symmetric proposal distribution (e.g., variance covariance Normal multivariate Normal proposal distribution, respectively). scale proposal distribution composed covariance matrix \\(\\Sigma\\), tuned multiplying scaling factor \\(s^2\\). Hence denote scale proposal distribution \\(s^2\\Sigma\\). scale \\(s^2\\Sigma\\) set drawn parameter estimates cover entire area posterior distribution (.e., scale \\(\\Sigma\\) set narrow candidate parameters close proximity current parameter drawn), remains reasonably efficient (.e., scale \\(\\Sigma\\) set wide many candidate parameters rejected resulting slowly progressing chain drawn parameter estimates). various options covariance matrix \\(\\Sigma\\). Often, covariance matrix \\(\\Sigma\\) set resembles covariance matrix actual posterior distribution. capture curvature subject’s conditional posterior distribution, scale RW Metropolis proposal distribution customized subject. also facilitates possibility let amount information available within data subject parameter determine degree group level distribution dominates estimation subject-specific parameters. Hence, approximate conditional posterior distribution subject, covariance matrix set combination covariance matrix obtained subject data group level covariance matrix \\(\\Phi_{}\\) \\(\\Psi_i\\). estimate covariance matrix subject data, used proposal distribution RW Metropolis sampler, simply use Maximum Likelihood Estimate (MLE), quantity used purpose scaling proposal distribution part estimated parameter values constitute posterior distribution. MLE estimate covariance matrix obtained maximizing likelihood Multinomial Logit (MNL) model data, retrieving Hessian matrix \\(H_{ki}\\) (.e., second order partial derivatives likelihood function respect parameters). covariance matrix parameters inverse Hessian matrix, \\(H_{ki}^{-1}\\). Hence, covariance matrices \\(\\boldsymbol{\\alpha}_{(O)ki}\\) \\(\\boldsymbol{\\alpha}_{(S)ki}\\), defined \\(\\Sigma_{\\boldsymbol{\\alpha}_{(O)ki}} = (H_{\\alpha(O) ki} + \\Phi_{}^{-1})^{-1}\\) \\(\\Sigma_{\\boldsymbol{\\alpha}_{(S)ki}} = (H_{\\alpha(S) ki} + \\Psi_i^{-1})^{-1}\\), respectively. \\(\\boldsymbol{\\alpha}_{(O)ki}\\), data Hessian obtained frequency categorical outcome observed state \\(\\) subject \\(k\\). \\(\\boldsymbol{\\alpha}_{(S)ki}\\), data frequency state \\(\\) transitions another state within subject \\(k\\). Hence, subject-specific covariance matrix (.e., inverse Hessian matrix) based sampled hidden state sequence. Therefore, MLE estimates subject-specific covariance matrices used RW Metropolis proposal distributions obtained iteration, sampled hidden state sequence changes iteration. potential problem maximizing log-likelihood subject’s data, certain state might sampled subject. circumvent problem, modify subject likelihood function adding -called regularizing likelihood function defined maximum subject-level likelihood function. maximize resulting pooled likelihood function order obtain MLE estimates. , use likelihood function combined data subjects considered part one group regularizing likelihood function. pooled likelihood function scaled \\(1-w \\times \\text{subject-level likelihood} + w \\times \\text{overall likelihood}^{n.obs_k / N.obs}\\), overall likelihood function dominate subject-level likelihood function, \\(n.obs_k\\) number data observations subject \\(k\\) \\(N.obs\\) total number data observations subjects group. Now defined covariance matrix \\(\\Sigma\\) scale RW Metropolis sampler proposal distribution, define scalar factor \\(s^2\\) obtain scale \\(s^2\\Sigma\\) proposal distribution. Rossi, Allenby, McCulloch (2012)}, adopt scaling proposal Roberts, Rosenthal, others (2001), set scaling \\(s^2 = (2.93 / \\sqrt{\\text{n.param}})^2\\), \\(n.param\\) number parameters estimated \\(\\boldsymbol{\\alpha}_{(S)ki}\\) \\(\\boldsymbol{\\alpha}_{(O)ki}\\) RW Metropolis sampler, equals \\(m - 1\\) case \\(\\boldsymbol{\\alpha}_{(S)ki}\\) (\\(m\\) denotes number states) \\(q - 1\\) case \\(\\boldsymbol{\\alpha}_{(O)ki}\\) (\\(q\\) denotes number categorical outcomes). summary, scale parameter \\(s^2\\Sigma_{\\alpha(O) ki}\\) \\(s^2\\Sigma_{\\alpha(S) ki}\\) proposal distributions RW Metropolis sampler \\(\\boldsymbol{\\alpha}_{(O)ki}\\) \\(\\boldsymbol{\\alpha}_{(S)ki}\\) defined : \\[\\begin{align} s^2\\Sigma_{\\alpha(O) ki} = (2.93 / \\sqrt{q-1})^2 & \\: \\times \\: (H_{\\alpha(O) ki} + \\Phi_{}^{-1})^{-1}, \\ \\text{} \\\\ s^2\\Sigma_{\\alpha(S) ki} = (2.93 / \\sqrt{m-1})^2 & \\: \\times \\: (H_{\\alpha(S) ki} + \\Psi_i^{-1})^{-1} , \\end{align}\\] \\(H_{\\alpha(O) ki}\\) Hessian \\(k^{th}\\) subject’s data frequency categorical outcome observed within state \\(\\) evaluated MLE pooled likelihood, \\(H_{\\alpha(S) ki}\\) Hessian \\(k^{th}\\) subject’s data frequency state \\(\\) transitions another state evaluated MLE pooled likelihood, \\(\\Phi_{}^{-1}\\) \\(\\Psi_i^{-1}\\) inverses group level covariance matrices. provides us \\(m\\) pairs scale parameters closely resemble scale subject-level conditional posterior distribution, 1) automatically tuned (.e., require experimentation determine \\(s^2\\) tune covariance matrix), 2) allow amount information available within data specific subject determine degree group level distribution dominates estimation subject’s level parameters, 3) require state sampled hidden state sequence subject-level likelihood required maximum.","code":""},{"path":"/articles/online_only/estimation-mhmm.html","id":"full-conditional-posterior-distributions-of-the-multilevel-hmm","dir":"Articles > Online_only","previous_headings":"Hybrid Metropolis within Gibbs sampler used to fit the multilevel HMM","what":"Full conditional posterior distributions of the multilevel HMM","title":"Estimation of the multilevel hidden Markov model","text":"random intercepts \\(\\boldsymbol{\\alpha}_{(O)ki}\\) \\(\\boldsymbol{\\alpha}_{(S)ki}\\), related subject-specific state-dependent probabilities observing categorical outcome \\(\\boldsymbol{\\theta}_{ki}\\) subject-specific state transition probability matrix \\(\\boldsymbol{\\Gamma}_{k}\\), respectively, choice prior distributions result closed form expressions full conditional posterior distributions. , subject-specific sets intercepts \\(\\boldsymbol{\\alpha}_{(O)ki}\\) related subject-specific state-dependent probabilities observing categorical outcome within state \\(\\), full conditional posterior distribution assess standard multivariate normal prior : \\[\\begin{align}   Pr(\\boldsymbol{\\alpha}_{(O)ki} \\mid ) \\: & \\propto L\\big(\\boldsymbol{\\alpha}_{(O)ki} \\mid O_{kt}, S_{kt} = \\big) \\Pr\\big(\\boldsymbol{\\alpha}_{(O)ki} \\mid \\boldsymbol{\\bar{\\alpha}}_{(O)}, \\boldsymbol{\\beta_{(O)}}, \\Phi_{}\\big),\\\\   \\Pr\\big(\\boldsymbol{\\alpha}_{(O)ki} \\mid \\boldsymbol{\\bar{\\alpha}}_{(O)}, \\boldsymbol{\\beta_{(O)}}, \\Phi_{}\\big) \\: & \\sim \\text{N}( \\boldsymbol{\\bar{\\alpha}}_{(O)} + \\boldsymbol{X^\\top} \\boldsymbol{\\beta_{(O)}}, \\Phi_{}\\big)), \\nonumber   \\end{align}\\] likelihood product probabilities observed outcomes \\(O_{kt} = l \\\\{1, 2, \\ldots, q\\}\\) within sampled states \\(S = \\) subject \\(k\\) time points \\(t\\): \\[\\begin{align}   L\\big(\\boldsymbol{\\alpha}_{(O)ki} \\mid O_{kt}, S_{kt} = \\big) \\: & = \\prod_{{t}} Pr( O_{k,{t}} = l \\mid S_{kt} = , \\boldsymbol{\\alpha}_{(O)ki}), \\\\   Pr( O_{k,{t}} = l \\mid S_{kt} = , \\boldsymbol{\\alpha}_{(O)ki}) \\: & = \\frac{\\text{exp}(\\alpha_{(O)kil})}{1 + \\sum_{\\bar{l} = 2}^q \\text{exp}(\\alpha_{(O)ki\\bar{l}})}, \\nonumber   \\end{align}\\] product restricted set time points coincide sampled state \\(S\\) subject \\(k\\) time point \\(t\\) \\(\\), \\(q\\) number categorical outcomes. numerator set equal one \\(l = 1\\), making first categorical outcome set baseline category every state. subject-specific sets intercepts \\(\\boldsymbol{\\alpha}_{(S)ki}\\) related state-transition probabilities transition state \\(\\) states \\(j \\\\{1, 2, \\ldots,\\) \\(m\\}\\), full conditional posterior distribution assess standard multivariate normal prior : \\[\\begin{align}   Pr(\\boldsymbol{\\alpha}_{(S)ki} \\mid ) \\: & \\propto L\\big(\\boldsymbol{\\alpha}_{(S)ki} \\mid S_{kt}, S_{k(t-1)} = \\big) \\Pr\\big(\\boldsymbol{\\alpha}_{(S)ki} \\mid \\boldsymbol{\\bar{\\alpha}}_{(S)}, \\boldsymbol{\\beta_{(S)}}, \\Psi_i\\big),\\\\   \\Pr\\big(\\boldsymbol{\\alpha}_{(S)ki} \\mid \\boldsymbol{\\bar{\\alpha}}_{(S)}, \\boldsymbol{\\beta_{(S)}}, \\Psi_{}\\big) \\: & \\sim \\text{N}(\\boldsymbol{\\bar{\\alpha}}_{(S)} + \\boldsymbol{X^\\top} \\boldsymbol{\\beta_{(S)}}, \\Psi_{}), \\nonumber   \\end{align}\\] likelihood product probabilities observed transitions state \\(\\) previous time point \\(t-1\\) states \\(S_{kt} = j\\) time points \\(t\\) subject \\(k\\):\\[\\begin{align}   L\\big(\\boldsymbol{\\alpha}_{(S)ki} \\mid S_{kt}, S_{k(t-1)} = \\big) \\: & = \\prod_{{n}} Pr( S_{k,{t}} = j \\mid S_{k(t-1)} = , \\boldsymbol{\\alpha}_{(S)ki}), \\\\   Pr( S_{k,{t}} = j \\mid S_{k(t-1)} = , \\boldsymbol{\\alpha}_{(S)ki}) \\: & =  \\frac{\\text{exp}(\\alpha_{(S)kij})}{1 + \\sum_{\\bar{j} \\Z} \\text{exp}(\\alpha_{(S)k \\bar{j}})}, \\nonumber   \\end{align}\\] \\[\\begin{equation}   \\text{} \\ Z \\\\{1, 2, \\ldots, m, \\ Z \\neq 1 \\} \\nonumber   \\end{equation}\\] product restricted set time points coincide sampled state \\(S\\) previous time point \\(t-1\\) \\(\\) subject \\(k\\), \\(m\\) number states. numerator set equal 1 \\(j = 1\\), making first state every row transition probability matrix \\(\\boldsymbol{\\Gamma}_k\\) baseline category. conditional posterior distributions \\(\\boldsymbol{\\alpha}_{(O)ki}\\) \\(\\boldsymbol{\\alpha}_{(S)ki}\\) result closed form expression know distribution, directly sample values \\(\\boldsymbol{\\alpha}_{(O)ki}\\) \\(\\boldsymbol{\\alpha}_{(S)ki}\\) conditional posterior distributions pre-defined equations obtain current (.e., updated using combination value hyper-prior data) parameters conditional posterior distributions. Instead, new values \\(\\boldsymbol{\\alpha}_{(O)ki}\\) \\(\\boldsymbol{\\alpha}_{(S)ki}\\) sampled using RW Metropolis sampler, described .","code":""},{"path":[]},{"path":"/articles/online_only/get_started.html","id":"introduction","dir":"Articles > Online_only","previous_headings":"","what":"Introduction","title":"Getting Started","text":"development package motivated area social sciences since becomes increasingly easy collect long sequences data behavior. , can monitor behavior unfolds real-time. applying HMMs behavioral data, used extract latent behavioral states time model dynamics behavior time. package allows modelling using multilevel framework (see e.g., Altman 2007; Shirley et al. 2010; Rueda, Rueda, Diaz-Uriarte 2013; Zhang Berhane 2014; Haan-Rietdijk et al. 2017). way, can simultaneously model several sequences (e.g., sequences different persons), accommodating heterogeneity persons. can therefore easily compare parameters groups, investigate, instance, whether dynamics patient therapist different patients good less favorable therapeutic outcomes. tutorial shows use mHMMbayes package extensive example. touch issues determining number hidden states checking model convergence, well instruct simulate data mHMM package. information estimation methods algorithms used package given vignette Estimation multilevel hidden Markov model.","code":""},{"path":"/articles/online_only/get_started.html","id":"first-steps-with-mhmmbayes","dir":"Articles > Online_only","previous_headings":"","what":"First steps with mHMMbayes","title":"Getting Started","text":"tutorial makes use embedded example data nonverbal. data contains nonverbal communication 10 patient-therapist couples, recorded 15 minutes frequency 1 observation per second (= 900 observations per couple). following variables contained dataset: id: id variable patient - therapist couple distinguish observation belongs couple. p_verbalizing: verbalizing behavior patient, consisting 1 = verbalizing, 2 = verbalizing, 3 = back channeling. p_looking: looking behavior patient, consisting 1 = looking therapist, 2 = looking therapist. t_verbalizing: verbalizing behavior therapist, consisting 1 = verbalizing, 2 = verbalizing, 3 = back channeling. t_looking: looking behavior therapist, consisting 1 = looking patient, 2 = looking patient. top 6 rows dataset provided . plotted data first 5 minutes (= first 300 observations) first couple, looks follows:  can observe patient therapist mainly looking inspected 5 minutes. first minute, patient primarily speaking. second minute, therapists starts, patient takes therapist back channeling.","code":""},{"path":"/articles/online_only/get_started.html","id":"a-simple-model","dir":"Articles > Online_only","previous_headings":"First steps with mHMMbayes","what":"A simple model","title":"Getting Started","text":"fit simple 2 state multilevel model function mHMM, one first specify general model properties starting values: first line code loads mHMMbayes package nonverbal data. Next specify general model properties: number states used set m <- 2, number dependent variables dataset used infer hidden states specified n_dep <- 4, number categorical outcomes dependent variables specified q_emiss <- c(3, 2, 3, 2).","code":"library(mHMMbayes) # specifying general model properties: m <- 2 n_dep <- 4 q_emiss <- c(3, 2, 3, 2)  # specifying starting values start_TM <- diag(.8, m) start_TM[lower.tri(start_TM) | upper.tri(start_TM)] <- .2 start_EM <- list(matrix(c(0.05, 0.90, 0.05,                            0.90, 0.05, 0.05), byrow = TRUE,                          nrow = m, ncol = q_emiss[1]), # vocalizing patient                   matrix(c(0.1, 0.9,                             0.1, 0.9), byrow = TRUE, nrow = m,                          ncol = q_emiss[2]), # looking patient                   matrix(c(0.90, 0.05, 0.05,                             0.05, 0.90, 0.05), byrow = TRUE,                          nrow = m, ncol = q_emiss[3]), # vocalizing therapist                   matrix(c(0.1, 0.9,                             0.1, 0.9), byrow = TRUE, nrow = m,                          ncol = q_emiss[4])) # looking therapist"},{"path":"/articles/online_only/get_started.html","id":"starting-values","dir":"Articles > Online_only","previous_headings":"First steps with mHMMbayes > A simple model","what":"Starting values","title":"Getting Started","text":"subsequent lines code specify starting values transition probability matrix start_TM emission distribution(s) start_EM, given model argument start_val model fitting stage. starting values used first run forward backward algorithm based ones expectation experience te data prevent ‘label switching’ speed convergence. Note strongly advised check model convergence label switching (see section Checking model convergence label switching example). information label switching send Estimation multilevel hidden Markov model) vignette.","code":""},{"path":"/articles/online_only/get_started.html","id":"prior-distributions","dir":"Articles > Online_only","previous_headings":"First steps with mHMMbayes > A simple model","what":"Prior distributions","title":"Getting Started","text":"estimation proceeds within Bayesian context, (hyper-)prior distribution defined group level parameters, .e., group level emission transition probabilities. Default, non-informative priors used unless specified otherwise user. specify user specific prior distributions, one uses input option emiss_hyp_prior emission distribution gamma_hyp_prior transition probabilities function mHMM. input arguments take object class mHMM_prior_emiss mHMM_prior_gamma created functions prior_emiss_cat prior_gamma, respectively. objects list, containing following key elements: mu0, lists containing hypothesized hyper-prior mean values intercepts Multinomial logit model. K0, number hypothetical prior subjects set hyper-prior mean intercepts specified mu0 based. nu, degrees freedom hyper-prior Inverse Wishart distribution covariance Multinomial logit intercepts. V, variance-covariance hyper-prior Inverse Wishart distribution covariance Multinomial logit intercepts. Note K0, nu V assumed equal states. mean values intercepts (regression coefficients covariates) denoted mu0 allowed vary states. elements list either prefix gamma_ emiss_, depending list belong . specifying prior distributions, note first element row probability domain intercept, serves baseline category Multinomial logit regression model. means, example, specify model 3 states, mu0 vector 2 elements, K0 nu contain 1 element V 2 2 matrix. elaborate explanation used (hyper-)prior distributions parameters, see vignette Estimation multilevel hidden Markov model.","code":""},{"path":"/articles/online_only/get_started.html","id":"fitting-the-model","dir":"Articles > Online_only","previous_headings":"First steps with mHMMbayes > A simple model","what":"Fitting the model","title":"Getting Started","text":"multilevel HMM fitted using function mHMM: call mHMM specifies model several arguments. s_data argument specifies input data used infer hidden states time. gen start_val argument specify general model properties starting values, discussed . arguments needed MCMC algorithm given mcmc: J specifies number iterations used hybrid metropolis within Gibbs algorithm burn_in specifies number iterations discard obtaining model parameter summary statistics. function mHMM returns object class mHMM, print summary methods see results. print method provides basic information model fitted. , number subjects dataset analyzed, number iterations burn-period, average log likelihood subjects model fit indices AIC, number states specified, number dependent variables states based : summary method provides information estimated parameters. , point estimates posterior distribution transition probability matrix emission distribution dependent variables group level: resulting model indicates 2 well separated states: one patient speaking one therapist speaking. Looking behavior quite similar patient therapist 2 states. Information estimated parameters can also obtained using function obtain_gamma obtain_emiss. functions allow user inspect estimated parameters group level, subject individually well, specifying input variable level = \"subject\": additional option functions obtain_gamma obtain_emiss offer changing burn-period used obtaining summary statistics, using input variable burn_in.","code":"# Run a model without covariate(s) and default priors: set.seed(14532) out_2st <- mHMM(s_data = nonverbal,                      gen = list(m = m, n_dep = n_dep, q_emiss = q_emiss),                      start_val = c(list(start_TM), start_EM),                     mcmc = list(J = 1000, burn_in = 200)) out_2st #> Number of subjects: 10  #>  #> 1000 iterations used in the MCMC algorithm with a burn in of 200  #> Average Log likelihood over all subjects: -1624.389  #> Average AIC over all subjects: 3276.777  #>  #> Number of states used: 2  #>  #> Number of dependent variables used: 4 summary(out_2st) #> State transition probability matrix  #>  (at the group level):  #>   #>              To state 1 To state 2 #> From state 1      0.929      0.071 #> From state 2      0.074      0.926 #>  #>   #> Emission distribution for each of the dependent variables  #>  (at the group level):  #>   #> $p_vocalizing #>         Category 1 Category 2 Category 3 #> State 1      0.018      0.957      0.024 #> State 2      0.795      0.052      0.153 #>  #> $p_looking #>         Category 1 Category 2 #> State 1      0.248      0.752 #> State 2      0.096      0.904 #>  #> $t_vocalizing #>         Category 1 Category 2 Category 3 #> State 1      0.806      0.075      0.119 #> State 2      0.034      0.945      0.020 #>  #> $t_looking #>         Category 1 Category 2 #> State 1      0.047      0.953 #> State 2      0.277      0.723 # When not specified, level defaults to \"group\" gamma_pop <- obtain_gamma(out_2st) gamma_pop #>              To state 1 To state 2 #> From state 1      0.929      0.071 #> From state 2      0.074      0.926  # To obtain the subject specific parameter estimates: gamma_subj <- obtain_gamma(out_2st, level = \"subject\") gamma_subj #> $`Subject 1` #>              To state 1 To state 2 #> From state 1      0.942      0.058 #> From state 2      0.048      0.952 #>  #> $`Subject 2` #>              To state 1 To state 2 #> From state 1      0.936      0.064 #> From state 2      0.060      0.940 #>  #> $`Subject 3` #>              To state 1 To state 2 #> From state 1      0.969      0.031 #> From state 2      0.054      0.946 #>  #> $`Subject 4` #>              To state 1 To state 2 #> From state 1      0.934      0.066 #> From state 2      0.046      0.954 #>  #> $`Subject 5` #>              To state 1 To state 2 #> From state 1      0.942      0.058 #> From state 2      0.058      0.942 #>  #> $`Subject 6` #>              To state 1 To state 2 #> From state 1      0.942      0.058 #> From state 2      0.087      0.913 #>  #> $`Subject 7` #>              To state 1 To state 2 #> From state 1      0.929      0.071 #> From state 2      0.043      0.958 #>  #> $`Subject 8` #>              To state 1 To state 2 #> From state 1       0.93      0.070 #> From state 2       0.08      0.919 #>  #> $`Subject 9` #>              To state 1 To state 2 #> From state 1      0.948      0.052 #> From state 2      0.058      0.942 #>  #> $`Subject 10` #>              To state 1 To state 2 #> From state 1      0.960      0.040 #> From state 2      0.068      0.932"},{"path":"/articles/online_only/get_started.html","id":"graphically-displaying-outcomes","dir":"Articles > Online_only","previous_headings":"First steps with mHMMbayes","what":"Graphically displaying outcomes","title":"Getting Started","text":"package includes several plot functions display fitted model parameters. First, one can plot posterior densities fitted model, transition probability matrix gamma emission distribution probabilities. posterior densities plotted group level subject level simultaneously. example, emission distribution variable p_vocalizing: , component specifies whether want visualize posterior densities transition probability matrix gamma (component = \"gamma\") emission distribution probabilities (component = \"emiss\"), using component = \"emiss\" input variable dep specifies dependent variable want inspect (variable p_vocolizing first variable set, set dep = 1), col specifies colors used plotting lines, dep_lab denotes label dependent variable plotting, cat_lab denotes labels categorical outcomes dependent variable. plot, solid line visualizes posterior density group level, dotted lines visualizes posterior density one subject. Second, one can plot transition probabilities obtained function obtain_gamma riverplot:  Note graphically displaying transition probabilities becomes informative number states increase.","code":"library(RColorBrewer) Voc_col <- c(brewer.pal(3,\"PuBuGn\")[c(1,3,2)]) Voc_lab <- c(\"Not Speaking\", \"Speaking\", \"Back channeling\")  plot(out_2st, component = \"emiss\", dep = 1, col = Voc_col,       dep_lab = c(\"Patient vocalizing\"), cat_lab = Voc_lab) # Transition probabilities at the group level and for subject number 1, respectively: plot(gamma_pop, col = rep(rev(brewer.pal(3,\"PiYG\"))[-2], each = m)) plot(gamma_subj, subj_nr = 1, col = rep(rev(brewer.pal(3,\"PiYG\"))[-2], each = m))"},{"path":"/articles/online_only/get_started.html","id":"determining-the-number-of-hidden-states","dir":"Articles > Online_only","previous_headings":"First steps with mHMMbayes","what":"Determining the number of hidden states","title":"Getting Started","text":"first step developing HMM determine number states \\(m\\) best describes observed data, model selection problem. suggest using combination Akaike Information Criterion (AIC) theoretical interpretability estimated states choose models. example, 2, 3 4 state model result AIC 3279, 3087, 2959, respectively. According model fit indices, 4 state model clearly best model. check composition states model, transition probabilities:  can see state patient speaks therapist silent (state 1), state patient silent therapist speaks (state 2), state patient therapist speak (state 3) state therapist speaks look patient (contrast looking behavior states), patient silent. addition, states quite stable probability remaining state .6 states.","code":"summary(out_4st) #> State transition probability matrix  #>  (at the group level):  #>   #>              To state 1 To state 2 To state 3 To state 4 #> From state 1      0.910      0.031      0.040      0.019 #> From state 2      0.044      0.815      0.056      0.084 #> From state 3      0.183      0.097      0.666      0.054 #> From state 4      0.024      0.220      0.019      0.738 #>  #>   #> Emission distribution for each of the dependent variables  #>  (at the group level):  #>   #> $p_vocalizing #>         Category 1 Category 2 Category 3 #> State 1      0.011      0.976      0.013 #> State 2      0.729      0.046      0.225 #> State 3      0.184      0.665      0.150 #> State 4      0.876      0.065      0.059 #>  #> $p_looking #>         Category 1 Category 2 #> State 1      0.237      0.763 #> State 2      0.061      0.939 #> State 3      0.439      0.561 #> State 4      0.094      0.906 #>  #> $t_vocalizing #>         Category 1 Category 2 Category 3 #> State 1      0.889      0.013      0.097 #> State 2      0.019      0.971      0.011 #> State 3      0.332      0.593      0.074 #> State 4      0.087      0.845      0.068 #>  #> $t_looking #>         Category 1 Category 2 #> State 1      0.030      0.970 #> State 2      0.045      0.955 #> State 3      0.074      0.926 #> State 4      0.949      0.051  m <- 4 plot(obtain_gamma(out_4st), cex = .5, col = rep(rev(brewer.pal(5,\"PiYG\"))[-3], each = m))"},{"path":"/articles/online_only/get_started.html","id":"determining-the-most-likely-state-sequence","dir":"Articles > Online_only","previous_headings":"First steps with mHMMbayes","what":"Determining the most likely state sequence","title":"Getting Started","text":"order determine likely state sequence, one can either use local decoding, probabilities hidden state sequence obtained simultaneously model parameters estimates, well-known Viterbi algorithm (Viterbi 1967; Forney Jr 1973). local decoding, likely state determined separately time point \\(t\\), contrast Viterbi algorithm one determines joint probability complete sequence observations \\(O_{1:T}\\) complete sequence hidden states \\(S_{1:T}\\). package, local decoding can achieved saving sampled hidden state sequence iteration Gibbs sampler, setting input variable return_path = TRUE function mHMM. result large output files, however. Global decoding can performed using function vit_mHMM: function returns hidden state sequence subject matrix, row represents point time column represents subject. can inspect obtained hidden state sequence example plotting together observed data. , first 5 minutes first couple plotted , addition estimated state sequence:","code":"state_seq <- vit_mHMM(out_2st, s_data = nonverbal)  head(state_seq) #>      Subj_1 Subj_2 Subj_3 Subj_4 Subj_5 Subj_6 Subj_7 Subj_8 Subj_9 Subj_10 #> [1,]      1      2      2      2      1      2      1      1      1       1 #> [2,]      1      2      2      2      1      2      1      1      1       1 #> [3,]      1      2      2      2      2      2      1      1      1       1 #> [4,]      1      2      2      2      2      2      1      1      1       1 #> [5,]      1      2      2      2      2      2      2      1      1       1 #> [6,]      1      2      2      2      2      1      2      1      1       1"},{"path":"/articles/online_only/get_started.html","id":"checking-model-convergence-and-label-switching","dir":"Articles > Online_only","previous_headings":"First steps with mHMMbayes","what":"Checking model convergence and label switching","title":"Getting Started","text":"using Bayesian estimation procedures, strongly advised check model convergence label switching. , one check algorithm reaches solution set different (often conceptually similar) starting values used, label switching problem. label switching, label (.e., state represents ) ordering states switches iterations estimation algorithm. example, started state 1, now becomes state 2. One can check model convergence label switching visually inspecting trace plots parameters set identical models used varying starting values. Trace plots plots sampled parameter values iterations. First, fit model 2 states , different starting values: group level parameter estimates emission probabilities transition probability matrix iteration estimation algorithm stored objects emiss_prob_bar gamma_prob_bar, respectively. subject level parameter estimates stored object PD_subj, PD abbreviation posterior density. , example, want inspect trace plots emission probabilities looking behavior patient group level, use following code:  can observed parameter estimates converge parameter space, chains mix well. Also, evidence label switching.","code":"# specifying general model properties m <-2 n_dep <- 4 q_emiss <- c(3, 2, 3, 2)  # specifying different starting values start_TM <- diag(.8, m) start_TM[lower.tri(start_TM) | upper.tri(start_TM)] <- .2 start_EM_b <- list(matrix(c(0.2, 0.6, 0.2,                             0.6, 0.2, 0.2), byrow = TRUE,                         nrow = m, ncol = q_emiss[1]), # vocalizing patient                  matrix(c(0.4, 0.6,                           0.4, 0.6), byrow = TRUE, nrow = m,                         ncol = q_emiss[2]), # looking patient                  matrix(c(0.6, 0.2, 0.2,                           0.2, 0.6, 0.2), byrow = TRUE,                         nrow = m, ncol = q_emiss[3]), # vocalizing therapist                  matrix(c(0.4, 0.6,                           0.4, 0.6), byrow = TRUE, nrow = m,                         ncol = q_emiss[4])) # looking therapist  # Run a model identical to out_2st, but with different starting values: set.seed(9843) out_2st_b <- mHMM(s_data = nonverbal,                        gen = list(m = m, n_dep = n_dep, q_emiss = q_emiss),                        start_val = c(list(start_TM), start_EM),                       mcmc = list(J = 1000, burn_in = 200)) par(mfrow = c(m,q_emiss[2])) for(i in 1:m){   for(q in 1:q_emiss[2]){      plot(x = 1:1000, y = out_2st$emiss_prob_bar[[2]][,(i-1) * q_emiss[2] + q],            ylim = c(0,1.4), yaxt = 'n', type = \"l\", ylab = \"Transition probability\",           xlab = \"Iteration\", main = paste(\"Patient\", Look_lab[q], \"in state\", i), col = \"#8da0cb\")      axis(2, at = seq(0,1, .2), las = 2)     lines(x = 1:1000, y = out_2st_b$emiss_prob_bar[[2]][,(i-1) * q_emiss[2] + q], col = \"#e78ac3\")     legend(\"topright\", col = c(\"#8da0cb\", \"#e78ac3\"), lwd = 2,             legend = c(\"Starting value set 1\", \"Starting value set 2\"), bty = \"n\")   } }"},{"path":"/articles/online_only/get_started.html","id":"simulating-data","dir":"Articles > Online_only","previous_headings":"","what":"Simulating data","title":"Getting Started","text":"mHMMBayes package also includes sim_mHMM function allowing simulating data multilevel hidden markov model. order simulate data one specify general model properties gen argument: number states used set m <- 3, number dependent variables dataset used infer hidden states specified n_dep <- 1, number categorical outcomes dependent variables specified q_emiss <- 4. Population level parameters’ matrices transition probabilities gamma emission probabilities emiss_distr need pass variances subjects transition probabilities matrix var_gamma = 1, emission distribution(s) var_emiss = 1 accordingly. also simulated subject-specific transition probability matrices emission distributions simply setting length observed sequence n_t \\(0\\).","code":"# simulating data for 10 subjects with each 100 observations for 1 dependent variable n_t     <- 100 n       <- 10 m       <- 3 n_dep   <- 1 q_emiss <- 4 #transition probabilities matrix gamma   <- matrix(c(0.8, 0.1, 0.1,                     0.2, 0.7, 0.1,                     0.2, 0.2, 0.6), ncol = m, byrow = TRUE) #emission distribution matrix emiss_distr <- list(matrix(c(0.5, 0.5, 0.0, 0.0,                              0.1, 0.1, 0.8, 0.0,                              0.0, 0.0, 0.1, 0.9),                             nrow = m, ncol = q_emiss, byrow = TRUE)) #call sim_mHMM() function data1 <- sim_mHMM(n_t = n_t, n = n,                    gen = list(m = m, n_dep = n_dep, q_emiss = q_emiss),                   gamma = gamma, emiss_distr = emiss_distr,                    var_gamma = 1, var_emiss = 1) #the first rows simulated subject-specific observations head(data1$obs) #>      subj observation 1 #> [1,]    1             2 #> [2,]    1             2 #> [3,]    1             1 #> [4,]    1             2 #> [5,]    1             2 #> [6,]    1             4 # local decoding of states based on the simulated data head(data1$states) #>      subj state #> [1,]    1     1 #> [2,]    1     1 #> [3,]    1     1 #> [4,]    1     1 #> [5,]    1     1 #> [6,]    1     3 # Subject-specific transition probability matrices and emission distributions only n_t <- 0 n <- 5 m <- 3 n_dep   <- 1 q_emiss <- 4 gamma <- matrix(c(0.8, 0.1, 0.1,                   0.2, 0.7, 0.1,                   0.2, 0.2, 0.6), ncol = m, byrow = TRUE) emiss_distr <- list(matrix(c(0.5, 0.5, 0.0, 0.0,                              0.1, 0.1, 0.8, 0.0,                              0.0, 0.0, 0.1, 0.9),                             nrow = m, ncol = q_emiss, byrow = TRUE))  data2 <- sim_mHMM(n_t = n_t, n = n,                    gen = list(m = m, n_dep = n_dep, q_emiss = q_emiss),                   gamma = gamma, emiss_distr = emiss_distr,                    var_gamma = 1, var_emiss = 1) data2 #> $subject_gamma #> $subject_gamma[[1]] #>        [,1]   [,2]   [,3] #> [1,] 0.7705 0.0844 0.1451 #> [2,] 0.2870 0.5603 0.1526 #> [3,] 0.0294 0.0060 0.9646 #>  #> $subject_gamma[[2]] #>        [,1]   [,2]   [,3] #> [1,] 0.8268 0.0756 0.0977 #> [2,] 0.3957 0.3457 0.2587 #> [3,] 0.0407 0.2411 0.7182 #>  #> $subject_gamma[[3]] #>        [,1]   [,2]   [,3] #> [1,] 0.4287 0.4437 0.1277 #> [2,] 0.1558 0.8244 0.0198 #> [3,] 0.3281 0.3159 0.3560 #>  #> $subject_gamma[[4]] #>        [,1]   [,2]   [,3] #> [1,] 0.8259 0.0452 0.1288 #> [2,] 0.1773 0.6623 0.1603 #> [3,] 0.0915 0.1454 0.7631 #>  #> $subject_gamma[[5]] #>        [,1]   [,2]   [,3] #> [1,] 0.5860 0.1112 0.3028 #> [2,] 0.1570 0.7134 0.1295 #> [3,] 0.0953 0.5966 0.3080 #>  #>  #> $subject_emiss #> $subject_emiss[[1]] #> $subject_emiss[[1]][[1]] #>        [,1]   [,2]   [,3]   [,4] #> [1,] 0.5088 0.4911 0.0000 0.0000 #> [2,] 0.1048 0.0306 0.8646 0.0000 #> [3,] 0.0000 0.0001 0.0417 0.9582 #>  #>  #> $subject_emiss[[2]] #> $subject_emiss[[2]][[1]] #>        [,1]   [,2]   [,3]   [,4] #> [1,] 0.3201 0.6799 0.0000 0.0000 #> [2,] 0.2236 0.5420 0.2344 0.0000 #> [3,] 0.0000 0.0000 0.0248 0.9751 #>  #>  #> $subject_emiss[[3]] #> $subject_emiss[[3]][[1]] #>        [,1]   [,2]   [,3]   [,4] #> [1,] 0.8711 0.1288 0.0000 0.0000 #> [2,] 0.1932 0.3019 0.5048 0.0000 #> [3,] 0.0000 0.0000 0.1272 0.8728 #>  #>  #> $subject_emiss[[4]] #> $subject_emiss[[4]][[1]] #>        [,1]   [,2]   [,3]   [,4] #> [1,] 0.3527 0.6473 0.0000 0.0000 #> [2,] 0.1126 0.1275 0.7598 0.0000 #> [3,] 0.0000 0.0000 0.0544 0.9456 #>  #>  #> $subject_emiss[[5]] #> $subject_emiss[[5]][[1]] #>        [,1]   [,2]   [,3]   [,4] #> [1,] 0.4178 0.5821 0.0000 0.0000 #> [2,] 0.4577 0.2915 0.2508 0.0000 #> [3,] 0.0000 0.0000 0.0355 0.9645"},{"path":[]},{"path":"/articles/online_only/tutorial-mhmm.html","id":"introduction","dir":"Articles > Online_only","previous_headings":"","what":"Introduction","title":"Multilevel HMM tutorial","text":"Hidden Markov models [HMMs; Rabiner (1989)] machine learning method used many different scientific fields describe sequence observations several decades. example, translating fragment spoken words text (.e., speech recognition, see e.g. Rabiner 1989; Woodland Povey 2002), identification regions DNA encode genes (.e., gene tagging, see e.g., Krogh, Mian, Haussler 1994; Henderson, Salzberg, Fasman 1997; Burge Karlin 1998). development package , however, motivated area social sciences. Due technological advancements, becomes increasingly easy collect long sequences data behavior. , can monitor behavior unfolds real time. example interaction therapist patient, different types nonverbal communication registered every second period 15 minutes. applying HMMs behavioral data, can used extract latent behavioral states time, model dynamics behavior time. quite recent development HMMs extension multilevel HMMs (see e.g., Altman 2007; Shirley et al. 2010; Rueda, Rueda, Diaz-Uriarte 2013; Zhang Berhane 2014; Haan-Rietdijk et al. 2017). Using multilevel framework, can model several sequences (e.g., sequences different persons) simultaneously, accommodating heterogeneity persons. result, can quantify amount variation persons dynamics behavior, easily perform group comparisons model parameters, investigate model parameters change result covariate. example, dynamics patient therapist different patients good therapeutic outcome patients less favorable therapeutic outcome? package mHMMbayes, one can estimate multilevel hidden Markov models. tutorial starts brief description HMM multilevel HMM. elaborate gentle introduction HMMs, refer Zucchini, MacDonald, Langrock (2016). Next, show use package mHMMbayes extensive example, also touching issues determining number hidden states checking model convergence. Information used estimation methods algorithms package given vignette Estimation multilevel hidden Markov model.","code":""},{"path":"/articles/online_only/tutorial-mhmm.html","id":"hidden-markov-models","dir":"Articles > Online_only","previous_headings":"","what":"Hidden Markov models","title":"Multilevel HMM tutorial","text":"Hidden Markov Models used data 1) believe distribution generating observation depends state underlying, hidden state, 2) hidden states follow Markov process, .e., states time independent one another, current state depends previous state (earlier states) (see e.g., Rabiner 1989; Ephraim Merhav 2002; Cappé 2005; Zucchini, MacDonald, Langrock 2016). HMM discrete time model: point time \\(t\\), one hidden state generates one observed event time point \\(t\\). Hence, probability observing current outcome \\(O_t\\) exclusively determined current latent state \\(S_t\\): \\[\\begin{equation} Pr(O_{t} \\mid \\ O_{t-1}, O_{t-2}, \\ldots, O_{1}, \\ S_{t}, S_{t-1}, \\ldots, S_{1}) = Pr(O_{t} \\mid S_{t}). \\end{equation}\\] probability observing \\(O_t\\) given \\(S_t\\) can distribution, e.g., discrete continuous. current version package mHMMbayes, categorical emission distribution implemented. hidden states sequence take values countable finite set states \\(S_t = , \\\\{1, 2, \\ldots, m\\}\\), \\(m\\) denotes number distinct states, form Markov chain, Markov property: \\[\\begin{equation} Pr(S_{t+1} \\mid \\ S_{t}, S_{t-1}, \\ldots, S_{1}) = Pr(S_{t+1} \\mid S_{t}). \\end{equation}\\] , probability switching next state \\(S_{t+1}\\) depends current state \\(S_t\\). HMM discrete time model, duration state represented self-transition probabilities \\(\\gamma_{ii}\\), probability certain time t spent state \\(S\\) given geometric distribution: \\(\\gamma_{ii}^{t-1}(1-\\gamma_{ii})\\). HMM includes three sets parameters: initial probabilities states \\(\\pi_i\\), matrix \\(\\mathbf{\\Gamma}\\) including transition probabilities \\(\\gamma_{ij}\\) states, state-dependent probability distribution observing \\(O_t\\) given \\(S_t\\) parameter set \\(\\boldsymbol{\\theta}_i\\). initial probabilities \\(\\pi_i\\) denote probability first state hidden state sequence, \\(S_1\\), \\(\\): \\[\\begin{equation} \\pi_i = Pr(S_1 = ) \\quad \\text{} \\sum_i \\pi_i = 1.  \\end{equation}\\] Often, initial probabilities states \\(\\pi_i\\) assumed stationary distribution implied transition probability matrix \\(\\mathbf{\\Gamma}\\), , long term steady-state probabilities obtained \\(\\lim_{T \\rightarrow \\infty} \\mathbf{\\Gamma}^T\\). transition probability matrix \\(\\mathbf{\\Gamma}\\) transition probabilities \\(\\gamma_{ij}\\) denote probability switching state \\(\\) time \\(t\\) state \\(j\\) time \\(t+1\\): \\[\\begin{equation} \\gamma_{ij} = Pr(S_{t+1} = j \\mid S_{t} = ) \\quad \\text{} \\sum_j \\gamma_{ij} = 1. \\end{equation}\\] , transition probabilities \\(\\gamma_{ij}\\) HMM represent probability switch hidden states rather observed acts, MC CTMC model. state-dependent probability distribution denotes probability observing \\(O_t\\) given \\(S_t\\) parameter set \\(\\boldsymbol{\\theta}_i\\). case package, state-dependent probability distribution given categorical distribution, parameter set \\(\\boldsymbol{\\theta}_i\\) set state-dependent probabilities observing categorical outcomes. , \\[\\begin{equation} Pr(O_t = o \\mid S_t = ) \\sim \\text{Cat} (\\boldsymbol{\\theta}_i), \\end{equation}\\] observed outcomes \\(o = 1, 2, \\ldots, q\\) \\(\\boldsymbol{\\theta}_i = (\\theta_{i1}, \\theta_{i2}, \\ldots, \\theta_{iq})\\) vector probabilities state \\(S = , \\ldots, m\\) \\(\\sum \\theta_i = 1\\), .e., within state, probabilities possible outcomes sum 1. assume parameters HMM independent \\(t\\), .e., assume time-homogeneous model. vignette Estimation multilevel hidden Markov model discuss three methods (.e., Maximum likelihood, Expectation Maximization Baum-Welch algorithm, Bayesian estimation) estimate parameters HMM. package mHMMbayes, chose use Bayesian estimation flexibility, require multilevel framework model.","code":""},{"path":"/articles/online_only/tutorial-mhmm.html","id":"multilevel-hidden-markov-models","dir":"Articles > Online_only","previous_headings":"","what":"Multilevel hidden Markov models","title":"Multilevel HMM tutorial","text":"Given data multiple subjects, one may fit HMM data subject separately, fit one HMM model data subject, strong (generally untenable) assumption subjects differ respect parameters HMM. Fitting different model behavioral sequence parsimonious, computationally intensive, results large number parameters estimates. Neither approach lends well formal comparison (e.g., comparing parameters experimental conditions). facilitate analysis multiple subjects, HMM extended putting multilevel framework. multilevel models, model parameters specified pertain different levels data. example, subject-specific model parameters describe data collected within subject, group level parameters describe typically observed within group subjects, variation observed subjects. implemented multilevel HMM, allow subject unique parameter values within HMM model (.e., identical number similar composition hidden states). Rather estimating subject-specific parameters individually, assume parameters HMM random, .e., follow given group level distribution. Within multilevel structure, mean variance group level distribution given parameter thus expresses overall mean parameter value group subjects parameter variability subjects group. Multilevel HMMs received attention literature. frequentist context, Altman (2007) presented general framework HMMs multiple processes defining class Mixed Hidden Markov Models (MHMMs). models however, computationally intensive due slow convergence suited modeling limited number random effects. approach Altman translated Bayesian framework, proved much faster time reach convergence decreased Zhang Berhane (2014). addition, HMM Bayesian context easier adapt multilevel model, need numerical integration eliminated. Examples application multilevel HMM (within Bayesian framework) : Rueda, Rueda, Diaz-Uriarte (2013) applied model analysis DNA copy number data, Zhang Berhane (2014) identify risk factors asthma, Shirley et al. (2010) clinical trial data treatment alcoholism Haan-Rietdijk et al. (2017) longitudinal data sets psychology. tutorial, use following notation parameters multilevel HMM. subject specific parameters supplemented prefix \\(k\\), denoting subject \\(k \\\\{1,2,\\ldots,K\\}\\). Hence, multilevel (Bayesian) HMM, subject specific parameters : subject-specific transition probability matrix \\(\\boldsymbol{\\Gamma}_k\\) transition probabilities \\(\\gamma_{k,ij}\\), subject-specific emission distributions denoting subject-specific probabilities \\(\\boldsymbol{\\theta}_{k,}\\) categorical outcomes within hidden state \\(\\). initial probabilities states \\(\\pi_{k,j}\\) estimated \\(\\pi_{k}\\) assumed stationary distribution \\(\\boldsymbol{\\Gamma}_k\\). group level parameters : group level state transition probability matrix \\(\\boldsymbol{\\Gamma}\\) transition probabilities \\(\\gamma_{ij}\\), group level state-dependent probabilities \\(\\boldsymbol{\\theta}_{}\\). fit model using Bayesian estimation (.e., hybrid Metropolis Gibbs sampler utilizes forward-backward recursion sample hidden state sequence subject, see vignette Estimation multilevel hidden Markov model).","code":""},{"path":"/articles/online_only/tutorial-mhmm.html","id":"using-the-package-mhmmbayes","dir":"Articles > Online_only","previous_headings":"","what":"Using the package mHMMbayes","title":"Multilevel HMM tutorial","text":"illustrate using package using embedded example data nonverbal. data contains nonverbal communication 10 patient-therapist couples, recorded 15 minutes frequency 1 observation per second (= 900 observations per couple). following variables contained dataset: id: id variable patient - therapist couple distinguish observation belongs couple. p_verbalizing: verbalizing behavior patient, consisting 1 = verbalizing, 2 = verbalizing, 3 = back channeling. p_looking: looking behavior patient, consisting 1 = looking therapist, 2 = looking therapist. t_verbalizing: verbalizing behavior therapist, consisting 1 = verbalizing, 2 = verbalizing, 3 = back channeling. t_looking: looking behavior therapist, consisting 1 = looking patient, 2 = looking patient. top 6 rows dataset provided . plot data first 5 minutes (= first 300 observations) first couple, get following:  can, example, observe patient therapist mainly looking observed 5 minutes. first minute, patient primarily speaking. second minute, therapists starts, patient takes therapist back channeling.","code":"#> Warning: package 'RColorBrewer' was built under R version 4.1.3"},{"path":"/articles/online_only/tutorial-mhmm.html","id":"a-simple-model","dir":"Articles > Online_only","previous_headings":"Using the package mHMMbayes Introduction","what":"A simple model","title":"Multilevel HMM tutorial","text":"fit simple 2 state multilevel model function mHMM, one first specify general model properties starting values: first line code loads mHMMbayes package nonverbal data. Next specify general model properties: number states used set m <- 2, number dependent variables dataset used infer hidden states specified n_dep <- 4, number categorical outcomes dependent variables specified q_emiss <- c(3, 2, 3, 2).","code":"library(mHMMbayes) # specifying general model properties: m <- 2 n_dep <- 4 q_emiss <- c(3, 2, 3, 2)  # specifying starting values start_TM <- diag(.8, m) start_TM[lower.tri(start_TM) | upper.tri(start_TM)] <- .2 start_EM <- list(matrix(c(0.05, 0.90, 0.05,                            0.90, 0.05, 0.05), byrow = TRUE,                          nrow = m, ncol = q_emiss[1]), # vocalizing patient                   matrix(c(0.1, 0.9,                             0.1, 0.9), byrow = TRUE, nrow = m,                          ncol = q_emiss[2]), # looking patient                   matrix(c(0.90, 0.05, 0.05,                             0.05, 0.90, 0.05), byrow = TRUE,                          nrow = m, ncol = q_emiss[3]), # vocalizing therapist                   matrix(c(0.1, 0.9,                             0.1, 0.9), byrow = TRUE, nrow = m,                          ncol = q_emiss[4])) # looking therapist"},{"path":"/articles/online_only/tutorial-mhmm.html","id":"starting-values","dir":"Articles > Online_only","previous_headings":"Using the package mHMMbayes Introduction > A simple model","what":"Starting values","title":"Multilevel HMM tutorial","text":"subsequent lines code specify starting values transition probability matrix emission distribution(s), given model argument start_val (see next piece code). starting values used first run forward backward algorithm. Although hidden states observed, one often idea probable compositions states. example, expect state patient mostly speaks, therapist silent, state patient silent therapists speaks. addition, expect states, therapist patient mainly looking instead looking away. One usually also (vague) idea likely unlikely switches states, size self-transition probabilities. example, think state usually last quite seconds, thus expect rather high self-transition probability. ideas can used construct set sensible starting values. Using sensible starting values increases convergence speed, often prevents problem called ‘label switching.’ Hence, using random uniform starting values recommended, default option included package. Note strongly advised check model convergence label switching. , one check algorithm reaches solution set different (often conceptually similar) starting values used, label switching problem. See section Checking model convergence label switching example. See vignette Estimation multilevel hidden Markov model) information forward backward algorithm problem label switching.","code":""},{"path":"/articles/online_only/tutorial-mhmm.html","id":"prior-distributions","dir":"Articles > Online_only","previous_headings":"Using the package mHMMbayes Introduction > A simple model","what":"Prior distributions","title":"Multilevel HMM tutorial","text":"estimation proceeds within Bayesian context, (hyper-)prior distribution defined group level parameters, .e., group level emission transition probabilities. Default, non-informative priors used unless specified otherwise user. , present information . elaborate explanation used (hyper-)prior distributions parameters, see vignette Estimation multilevel hidden Markov model. First , note prior distributions emission distribution transition probabilities probabilities directly, intercepts (regression coefficients given covariates used) Multinomial regression model used accommodate multilevel framework data. Second, parameters independent prior distribution. row emission distribution matrix transition probability matrix sum one, individual parameters rows connected. Hence, row seen set parameters estimated jointly, set parameters multivariate prior distribution. sets intercepts Multinomial regression model multivariate normal distribution. (hyper-) prior intercepts thus consists prior distribution vector means, prior distribution covariance matrix. hyper-prior mean intercepts multivariate Normal distribution, , default, vector means equal 0, parameter \\(K_0\\) covariance matrix multiplied. , \\(K_0\\) denotes number observations (.e., number hypothetical prior subjects) prior mean vector zero’s based. default, \\(K_0\\) set 1. hyper-prior covariance matrix set (state specific) intercepts Inverse Wishart distribution, variance default setting equals 3 + \\(m\\) - 1 transition probabilities 3 + \\(q\\) - 1 emission probabilities, covariance equals 0. degrees freedom Inverse Wishart distribution set 3 + \\(m\\) - 1 transition probabilities 3 + \\(q\\) - 1 emission probabilities. specify user specific prior distributions, one uses input option emiss_hyp_prior emission distribution gamma_hyp_prior transition probabilities function mHMM. input arguments take object class mHMM_prior_emiss mHMM_prior_gamma created functions prior_emiss_cat prior_gamma, respectively. objects list, containing following key elements: mu0, lists containing hypothesized hyper-prior mean values intercepts Multinomial logit model. K0, number hypothetical prior subjects set hyper-prior mean intercepts specified mu0 based. nu, degrees freedom hyper-prior Inverse Wishart distribution covariance Multinomial logit intercepts. V, variance-covariance hyper-prior Inverse Wishart distribution covariance Multinomial logit intercepts. Note K0, nu V assumed equal states. mean values intercepts (regression coefficients covariates) denoted mu0 allowed vary states. elements list either prefix gamma_ emiss_, depending list belong . specifying prior distributions, note first element row probability domain intercept, serves baseline category Multinomial logit regression model. means, example, specify model 3 states, mu0 vector 2 elements, K0 nu contain 1 element V 2 2 matrix.","code":""},{"path":"/articles/online_only/tutorial-mhmm.html","id":"fitting-the-model","dir":"Articles > Online_only","previous_headings":"Using the package mHMMbayes Introduction > A simple model","what":"Fitting the model","title":"Multilevel HMM tutorial","text":"multilevel HMM fitted using function mHMM: call mHMM specifies model several arguments. s_data argument specifies input data used infer hidden states time. gen start_val argument specify general model properties starting values, discussed . arguments needed MCMC algorithm given mcmc: J specifies number iterations used hybrid metropolis within Gibbs algorithm burn_in specifies number iterations discard obtaining model parameter summary statistics. function mHMM returns object class mHMM, print summary methods see results. print method provides basic information model fitted. , number subjects dataset analyzed, number iterations burn-period, average log likelihood subjects model fit indices AIC, number states specified, number dependent variables states based : summary method provides information estimated parameters. , point estimates posterior distribution transition probability matrix emission distribution dependent variables group level: resulting model indicates 2 well separated states: one patient speaking one therapist speaking. Looking behavior quite similar patient therapist 2 states. Information estimated parameters can also obtained using function obtain_gamma obtain_emiss. functions allow user inspect estimated parameters group level, subject individually well, specifying input variable level = \"subject\": additional option functions obtain_gamma obtain_emiss offer changing burn-period used obtaining summary statistics, using input variable burn_in.","code":"# Run a model without covariate(s) and default priors: set.seed(14532) out_2st <- mHMM(s_data = nonverbal,                      gen = list(m = m, n_dep = n_dep, q_emiss = q_emiss),                      start_val = c(list(start_TM), start_EM),                     mcmc = list(J = 1000, burn_in = 200)) out_2st #> Number of subjects: 10  #>  #> 1000 iterations used in the MCMC algorithm with a burn in of 200  #> Average Log likelihood over all subjects: -1624.389  #> Average AIC over all subjects: 3276.777  #>  #> Number of states used: 2  #>  #> Number of dependent variables used: 4 summary(out_2st) #> State transition probability matrix  #>  (at the group level):  #>   #>              To state 1 To state 2 #> From state 1      0.929      0.071 #> From state 2      0.074      0.926 #>  #>   #> Emission distribution for each of the dependent variables  #>  (at the group level):  #>   #> $p_vocalizing #>         Category 1 Category 2 Category 3 #> State 1      0.018      0.957      0.024 #> State 2      0.795      0.052      0.153 #>  #> $p_looking #>         Category 1 Category 2 #> State 1      0.248      0.752 #> State 2      0.096      0.904 #>  #> $t_vocalizing #>         Category 1 Category 2 Category 3 #> State 1      0.806      0.075      0.119 #> State 2      0.034      0.945      0.020 #>  #> $t_looking #>         Category 1 Category 2 #> State 1      0.047      0.953 #> State 2      0.277      0.723 # When not specified, level defaults to \"group\" gamma_pop <- obtain_gamma(out_2st) gamma_pop #>              To state 1 To state 2 #> From state 1      0.929      0.071 #> From state 2      0.074      0.926  # To obtain the subject specific parameter estimates: gamma_subj <- obtain_gamma(out_2st, level = \"subject\") gamma_subj #> $`Subject 1` #>              To state 1 To state 2 #> From state 1      0.942      0.058 #> From state 2      0.048      0.952 #>  #> $`Subject 2` #>              To state 1 To state 2 #> From state 1      0.936      0.064 #> From state 2      0.060      0.940 #>  #> $`Subject 3` #>              To state 1 To state 2 #> From state 1      0.969      0.031 #> From state 2      0.054      0.946 #>  #> $`Subject 4` #>              To state 1 To state 2 #> From state 1      0.934      0.066 #> From state 2      0.046      0.954 #>  #> $`Subject 5` #>              To state 1 To state 2 #> From state 1      0.942      0.058 #> From state 2      0.058      0.942 #>  #> $`Subject 6` #>              To state 1 To state 2 #> From state 1      0.942      0.058 #> From state 2      0.087      0.913 #>  #> $`Subject 7` #>              To state 1 To state 2 #> From state 1      0.929      0.071 #> From state 2      0.043      0.958 #>  #> $`Subject 8` #>              To state 1 To state 2 #> From state 1       0.93      0.070 #> From state 2       0.08      0.919 #>  #> $`Subject 9` #>              To state 1 To state 2 #> From state 1      0.948      0.052 #> From state 2      0.058      0.942 #>  #> $`Subject 10` #>              To state 1 To state 2 #> From state 1      0.960      0.040 #> From state 2      0.068      0.932"},{"path":"/articles/online_only/tutorial-mhmm.html","id":"graphically-displaying-outcomes","dir":"Articles > Online_only","previous_headings":"Using the package mHMMbayes Introduction","what":"Graphically displaying outcomes","title":"Multilevel HMM tutorial","text":"package includes several plot functions display fitted model parameters. First, one can plot posterior densities fitted model, transition probability matrix gamma emission distribution probabilities. posterior densities plotted group level subject level simultaneously. example, emission distribution variable p_vocalizing: , component specifies whether want visualize posterior densities transition probability matrix gamma (component = \"gamma\") emission distribution probabilities (component = \"emiss\"), using component = \"emiss\" input variable dep specifies dependent variable want inspect (variable p_vocolizing first variable set, set dep = 1), col specifies colors used plotting lines, dep_lab denotes label dependent variable plotting, cat_lab denotes labels categorical outcomes dependent variable. plot, solid line visualizes posterior density group level, dotted lines visualizes posterior density one subject. Second, one can plot transition probabilities obtained function obtain_gamma riverplot:  Note graphically displaying transition probabilities becomes informative number states increase.","code":"library(RColorBrewer) Voc_col <- c(brewer.pal(3,\"PuBuGn\")[c(1,3,2)]) Voc_lab <- c(\"Not Speaking\", \"Speaking\", \"Back channeling\")  plot(out_2st, component = \"emiss\", dep = 1, col = Voc_col,       dep_lab = c(\"Patient vocalizing\"), cat_lab = Voc_lab) # Transition probabilities at the group level and for subject number 1, respectively: plot(gamma_pop, col = rep(rev(brewer.pal(3,\"PiYG\"))[-2], each = m)) plot(gamma_subj, subj_nr = 1, col = rep(rev(brewer.pal(3,\"PiYG\"))[-2], each = m))"},{"path":"/articles/online_only/tutorial-mhmm.html","id":"determining-the-number-of-hidden-states","dir":"Articles > Online_only","previous_headings":"Using the package mHMMbayes Introduction","what":"Determining the number of hidden states","title":"Multilevel HMM tutorial","text":"first step developing HMM determine number states \\(m\\) best describes observed data, model selection problem. modelling, example, behavior, task define states clusters observed behavioral outcomes provide reasonable, theoretically interpretable, description data. suggest using combination Akaike Information Criterion (AIC) theoretical interpretability estimated states choose models1. example dataset, 2, 3 4 state model result AIC 3279, 3087, 2959, respectively. According model fit indices, 4 state model clearly best model2. Let’s inspect composition states 4 state model, transition probabilities:  can see state patient speaks therapist silent (state 1), state patient silent therapist speaks (state 2), state patient therapist speak (state 3) state therapist speaks look patient (contrast looking behavior states), patient silent. addition, states quite stable probability remaining state .6 states.","code":"summary(out_4st) #> State transition probability matrix  #>  (at the group level):  #>   #>              To state 1 To state 2 To state 3 To state 4 #> From state 1      0.910      0.031      0.040      0.019 #> From state 2      0.044      0.815      0.056      0.084 #> From state 3      0.183      0.097      0.666      0.054 #> From state 4      0.024      0.220      0.019      0.738 #>  #>   #> Emission distribution for each of the dependent variables  #>  (at the group level):  #>   #> $p_vocalizing #>         Category 1 Category 2 Category 3 #> State 1      0.011      0.976      0.013 #> State 2      0.729      0.046      0.225 #> State 3      0.184      0.665      0.150 #> State 4      0.876      0.065      0.059 #>  #> $p_looking #>         Category 1 Category 2 #> State 1      0.237      0.763 #> State 2      0.061      0.939 #> State 3      0.439      0.561 #> State 4      0.094      0.906 #>  #> $t_vocalizing #>         Category 1 Category 2 Category 3 #> State 1      0.889      0.013      0.097 #> State 2      0.019      0.971      0.011 #> State 3      0.332      0.593      0.074 #> State 4      0.087      0.845      0.068 #>  #> $t_looking #>         Category 1 Category 2 #> State 1      0.030      0.970 #> State 2      0.045      0.955 #> State 3      0.074      0.926 #> State 4      0.949      0.051  m <- 4 plot(obtain_gamma(out_4st), cex = .5, col = rep(rev(brewer.pal(5,\"PiYG\"))[-3], each = m))"},{"path":"/articles/online_only/tutorial-mhmm.html","id":"determining-the-most-likely-state-sequence","dir":"Articles > Online_only","previous_headings":"Using the package mHMMbayes Introduction","what":"Determining the most likely state sequence","title":"Multilevel HMM tutorial","text":"Given well-fitting HMM, may interest determine actual sequence, order succession, hidden states likely given rise sequence outcomes observed subject. One can either use local decoding, probabilities hidden state sequence obtained simultaneously model parameters estimates, well-known Viterbi algorithm (Viterbi 1967; Forney Jr 1973). local decoding, likely state determined separately time point \\(t\\), contrast Viterbi algorithm one determines joint probability complete sequence observations \\(O_{1:T}\\) complete sequence hidden states \\(S_{1:T}\\). package, local decoding can achieved saving sampled hidden state sequence iteration Gibbs sampler, setting input variable return_path = TRUE function mHMM. result large output files, however. Global decoding can performed using function vit_mHMM: function returns hidden state sequence subject matrix, row represents point time column represents subject. can inspect obtained hidden state sequence example plotting together observed data. , first 5 minutes first couple plotted , addition estimated state sequence:","code":"state_seq <- vit_mHMM(out_2st, s_data = nonverbal)  head(state_seq) #>      Subj_1 Subj_2 Subj_3 Subj_4 Subj_5 Subj_6 Subj_7 Subj_8 Subj_9 Subj_10 #> [1,]      1      2      2      2      1      2      1      1      1       1 #> [2,]      1      2      2      2      1      2      1      1      1       1 #> [3,]      1      2      2      2      2      2      1      1      1       1 #> [4,]      1      2      2      2      2      2      1      1      1       1 #> [5,]      1      2      2      2      2      2      2      1      1       1 #> [6,]      1      2      2      2      2      1      2      1      1       1"},{"path":"/articles/online_only/tutorial-mhmm.html","id":"checking-model-convergence-and-label-switching","dir":"Articles > Online_only","previous_headings":"Using the package mHMMbayes Introduction","what":"Checking model convergence and label switching","title":"Multilevel HMM tutorial","text":"using Bayesian estimation procedures, strongly advised check model convergence label switching. , one check algorithm reaches solution set different (often conceptually similar) starting values used, label switching problem. label switching, label (.e., state represents ) ordering states switches iterations estimation algorithm. example, started state 1, now becomes state 2. One can check model convergence label switching visually inspecting trace plots parameters set identical models used varying starting values. Trace plots plots sampled parameter values iterations. First, fit model 2 states , different starting values: group level parameter estimates emission probabilities transition probability matrix iteration estimation algorithm stored objects emiss_prob_bar gamma_prob_bar, respectively. subject level parameter estimates stored object PD_subj, PD abbreviation posterior density. , example, want inspect trace plots emission probabilities looking behavior patient group level, use following code:  can observed parameter estimates converge parameter space, chains mix well. Also, evidence label switching.","code":"# specifying general model properties m <-2 n_dep <- 4 q_emiss <- c(3, 2, 3, 2)  # specifying different starting values start_TM <- diag(.8, m) start_TM[lower.tri(start_TM) | upper.tri(start_TM)] <- .2 start_EM_b <- list(matrix(c(0.2, 0.6, 0.2,                             0.6, 0.2, 0.2), byrow = TRUE,                         nrow = m, ncol = q_emiss[1]), # vocalizing patient                  matrix(c(0.4, 0.6,                           0.4, 0.6), byrow = TRUE, nrow = m,                         ncol = q_emiss[2]), # looking patient                  matrix(c(0.6, 0.2, 0.2,                           0.2, 0.6, 0.2), byrow = TRUE,                         nrow = m, ncol = q_emiss[3]), # vocalizing therapist                  matrix(c(0.4, 0.6,                           0.4, 0.6), byrow = TRUE, nrow = m,                         ncol = q_emiss[4])) # looking therapist  # Run a model identical to out_2st, but with different starting values: set.seed(9843) out_2st_b <- mHMM(s_data = nonverbal,                        gen = list(m = m, n_dep = n_dep, q_emiss = q_emiss),                        start_val = c(list(start_TM), start_EM),                       mcmc = list(J = 1000, burn_in = 200)) par(mfrow = c(m,q_emiss[2])) for(i in 1:m){   for(q in 1:q_emiss[2]){      plot(x = 1:1000, y = out_2st$emiss_prob_bar[[2]][,(i-1) * q_emiss[2] + q],            ylim = c(0,1.4), yaxt = 'n', type = \"l\", ylab = \"Transition probability\",           xlab = \"Iteration\", main = paste(\"Patient\", Look_lab[q], \"in state\", i), col = \"#8da0cb\")      axis(2, at = seq(0,1, .2), las = 2)     lines(x = 1:1000, y = out_2st_b$emiss_prob_bar[[2]][,(i-1) * q_emiss[2] + q], col = \"#e78ac3\")     legend(\"topright\", col = c(\"#8da0cb\", \"#e78ac3\"), lwd = 2,             legend = c(\"Starting value set 1\", \"Starting value set 2\"), bty = \"n\")   } }"},{"path":[]},{"path":"/articles/tutorial-mhmm.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Multilevel HMM tutorial","text":"Hidden Markov models [HMMs; Rabiner (1989)] machine learning method used many different scientific fields describe sequence observations several decades. example, translating fragment spoken words text (.e., speech recognition, see e.g. Rabiner 1989; Woodland Povey 2002), identification regions DNA encode genes (.e., gene tagging, see e.g., Krogh, Mian, Haussler 1994; Henderson, Salzberg, Fasman 1997; Burge Karlin 1998). development package , however, motivated area social sciences. Due technological advancements, becomes increasingly easy collect long sequences data behavior. , can monitor behavior unfolds real time. example interaction therapist patient, different types nonverbal communication registered every second period 15 minutes. applying HMMs behavioral data, can used extract latent behavioral states time, model dynamics behavior time. quite recent development HMMs extension multilevel HMMs (see e.g., Altman 2007; Shirley et al. 2010; Rueda, Rueda, Diaz-Uriarte 2013; Zhang Berhane 2014; Haan-Rietdijk et al. 2017). Using multilevel framework, can model several sequences (e.g., sequences different persons) simultaneously, accommodating heterogeneity persons. result, can quantify amount variation persons dynamics behavior, easily perform group comparisons model parameters, investigate model parameters change result covariate. example, dynamics patient therapist different patients good therapeutic outcome patients less favorable therapeutic outcome? package mHMMbayes, one can estimate multilevel hidden Markov models. tutorial starts brief description HMM multilevel HMM. elaborate gentle introduction HMMs, refer Zucchini, MacDonald, Langrock (2016). Next, show use package mHMMbayes extensive example, also touching issues determining number hidden states checking model convergence. Information used estimation methods algorithms package given vignette Estimation multilevel hidden Markov model.","code":""},{"path":"/articles/tutorial-mhmm.html","id":"hidden-markov-models","dir":"Articles","previous_headings":"","what":"Hidden Markov models","title":"Multilevel HMM tutorial","text":"Hidden Markov Models used data 1) believe distribution generating observation depends state underlying, hidden state, 2) hidden states follow Markov process, .e., states time independent one another, current state depends previous state (earlier states) (see e.g., Rabiner 1989; Ephraim Merhav 2002; Cappé 2005; Zucchini, MacDonald, Langrock 2016). HMM discrete time model: point time \\(t\\), one hidden state generates one observed event time point \\(t\\). Hence, probability observing current outcome \\(O_t\\) exclusively determined current latent state \\(S_t\\): \\[\\begin{equation} Pr(O_{t} \\mid \\ O_{t-1}, O_{t-2}, \\ldots, O_{1}, \\ S_{t}, S_{t-1}, \\ldots, S_{1}) = Pr(O_{t} \\mid S_{t}). \\end{equation}\\] probability observing \\(O_t\\) given \\(S_t\\) can distribution, e.g., discrete continuous. current version package mHMMbayes, categorical emission distribution implemented. hidden states sequence take values countable finite set states \\(S_t = , \\\\{1, 2, \\ldots, m\\}\\), \\(m\\) denotes number distinct states, form Markov chain, Markov property: \\[\\begin{equation} Pr(S_{t+1} \\mid \\ S_{t}, S_{t-1}, \\ldots, S_{1}) = Pr(S_{t+1} \\mid S_{t}). \\end{equation}\\] , probability switching next state \\(S_{t+1}\\) depends current state \\(S_t\\). HMM discrete time model, duration state represented self-transition probabilities \\(\\gamma_{ii}\\), probability certain time t spent state \\(S\\) given geometric distribution: \\(\\gamma_{ii}^{t-1}(1-\\gamma_{ii})\\). HMM includes three sets parameters: initial probabilities states \\(\\pi_i\\), matrix \\(\\mathbf{\\Gamma}\\) including transition probabilities \\(\\gamma_{ij}\\) states, state-dependent probability distribution observing \\(O_t\\) given \\(S_t\\) parameter set \\(\\boldsymbol{\\theta}_i\\). initial probabilities \\(\\pi_i\\) denote probability first state hidden state sequence, \\(S_1\\), \\(\\): \\[\\begin{equation} \\pi_i = Pr(S_1 = ) \\quad \\text{} \\sum_i \\pi_i = 1.  \\end{equation}\\] Often, initial probabilities states \\(\\pi_i\\) assumed stationary distribution implied transition probability matrix \\(\\mathbf{\\Gamma}\\), , long term steady-state probabilities obtained \\(\\lim_{T \\rightarrow \\infty} \\mathbf{\\Gamma}^T\\). transition probability matrix \\(\\mathbf{\\Gamma}\\) transition probabilities \\(\\gamma_{ij}\\) denote probability switching state \\(\\) time \\(t\\) state \\(j\\) time \\(t+1\\): \\[\\begin{equation} \\gamma_{ij} = Pr(S_{t+1} = j \\mid S_{t} = ) \\quad \\text{} \\sum_j \\gamma_{ij} = 1. \\end{equation}\\] , transition probabilities \\(\\gamma_{ij}\\) HMM represent probability switch hidden states rather observed acts, MC CTMC model. state-dependent probability distribution denotes probability observing \\(O_t\\) given \\(S_t\\) parameter set \\(\\boldsymbol{\\theta}_i\\). case package, state-dependent probability distribution given categorical distribution, parameter set \\(\\boldsymbol{\\theta}_i\\) set state-dependent probabilities observing categorical outcomes. , \\[\\begin{equation} Pr(O_t = o \\mid S_t = ) \\sim \\text{Cat} (\\boldsymbol{\\theta}_i), \\end{equation}\\] observed outcomes \\(o = 1, 2, \\ldots, q\\) \\(\\boldsymbol{\\theta}_i = (\\theta_{i1}, \\theta_{i2}, \\ldots, \\theta_{iq})\\) vector probabilities state \\(S = , \\ldots, m\\) \\(\\sum \\theta_i = 1\\), .e., within state, probabilities possible outcomes sum 1. assume parameters HMM independent \\(t\\), .e., assume time-homogeneous model. vignette Estimation multilevel hidden Markov model discuss three methods (.e., Maximum likelihood, Expectation Maximization Baum-Welch algorithm, Bayesian estimation) estimate parameters HMM. package mHMMbayes, chose use Bayesian estimation flexibility, require multilevel framework model.","code":""},{"path":"/articles/tutorial-mhmm.html","id":"multilevel-hidden-markov-models","dir":"Articles","previous_headings":"","what":"Multilevel hidden Markov models","title":"Multilevel HMM tutorial","text":"Given data multiple subjects, one may fit HMM data subject separately, fit one HMM model data subject, strong (generally untenable) assumption subjects differ respect parameters HMM. Fitting different model behavioral sequence parsimonious, computationally intensive, results large number parameters estimates. Neither approach lends well formal comparison (e.g., comparing parameters experimental conditions). facilitate analysis multiple subjects, HMM extended putting multilevel framework. multilevel models, model parameters specified pertain different levels data. example, subject-specific model parameters describe data collected within subject, group level parameters describe typically observed within group subjects, variation observed subjects. implemented multilevel HMM, allow subject unique parameter values within HMM model (.e., identical number similar composition hidden states). Rather estimating subject-specific parameters individually, assume parameters HMM random, .e., follow given group level distribution. Within multilevel structure, mean variance group level distribution given parameter thus expresses overall mean parameter value group subjects parameter variability subjects group. Multilevel HMMs received attention literature. frequentist context, Altman (2007) presented general framework HMMs multiple processes defining class Mixed Hidden Markov Models (MHMMs). models however, computationally intensive due slow convergence suited modeling limited number random effects. approach Altman translated Bayesian framework, proved much faster time reach convergence decreased Zhang Berhane (2014). addition, HMM Bayesian context easier adapt multilevel model, need numerical integration eliminated. Examples application multilevel HMM (within Bayesian framework) : Rueda, Rueda, Diaz-Uriarte (2013) applied model analysis DNA copy number data, Zhang Berhane (2014) identify risk factors asthma, Shirley et al. (2010) clinical trial data treatment alcoholism Haan-Rietdijk et al. (2017) longitudinal data sets psychology. tutorial, use following notation parameters multilevel HMM. subject specific parameters supplemented prefix \\(k\\), denoting subject \\(k \\\\{1,2,\\ldots,K\\}\\). Hence, multilevel (Bayesian) HMM, subject specific parameters : subject-specific transition probability matrix \\(\\boldsymbol{\\Gamma}_k\\) transition probabilities \\(\\gamma_{k,ij}\\), subject-specific emission distributions denoting subject-specific probabilities \\(\\boldsymbol{\\theta}_{k,}\\) categorical outcomes within hidden state \\(\\). initial probabilities states \\(\\pi_{k,j}\\) estimated \\(\\pi_{k}\\) assumed stationary distribution \\(\\boldsymbol{\\Gamma}_k\\). group level parameters : group level state transition probability matrix \\(\\boldsymbol{\\Gamma}\\) transition probabilities \\(\\gamma_{ij}\\), group level state-dependent probabilities \\(\\boldsymbol{\\theta}_{}\\). fit model using Bayesian estimation (.e., hybrid Metropolis Gibbs sampler utilizes forward-backward recursion sample hidden state sequence subject, see vignette Estimation multilevel hidden Markov model).","code":""},{"path":"/articles/tutorial-mhmm.html","id":"using-the-package-mhmmbayes","dir":"Articles","previous_headings":"","what":"Using the package mHMMbayes","title":"Multilevel HMM tutorial","text":"illustrate using package using embedded example data nonverbal. data contains nonverbal communication 10 patient-therapist couples, recorded 15 minutes frequency 1 observation per second (= 900 observations per couple). following variables contained dataset: id: id variable patient - therapist couple distinguish observation belongs couple. p_verbalizing: verbalizing behavior patient, consisting 1 = verbalizing, 2 = verbalizing, 3 = back channeling. p_looking: looking behavior patient, consisting 1 = looking therapist, 2 = looking therapist. t_verbalizing: verbalizing behavior therapist, consisting 1 = verbalizing, 2 = verbalizing, 3 = back channeling. t_looking: looking behavior therapist, consisting 1 = looking patient, 2 = looking patient. top 6 rows dataset provided . plot data first 5 minutes (= first 300 observations) first couple, get following:  can, example, observe patient therapist mainly looking observed 5 minutes. first minute, patient primarily speaking. second minute, therapists starts, patient takes therapist back channeling.","code":""},{"path":"/articles/tutorial-mhmm.html","id":"a-simple-model","dir":"Articles","previous_headings":"Using the package mHMMbayes Introduction","what":"A simple model","title":"Multilevel HMM tutorial","text":"fit simple 2 state multilevel model function mHMM, one first specify general model properties starting values: first line code loads mHMMbayes package nonverbal data. Next specify general model properties: number states used set m <- 2, number dependent variables dataset used infer hidden states specified n_dep <- 4, number categorical outcomes dependent variables specified q_emiss <- c(3, 2, 3, 2).","code":"library(mHMMbayes) # specifying general model properties: m <- 2 n_dep <- 4 q_emiss <- c(3, 2, 3, 2)  # specifying starting values start_TM <- diag(.8, m) start_TM[lower.tri(start_TM) | upper.tri(start_TM)] <- .2 start_EM <- list(matrix(c(0.05, 0.90, 0.05,                            0.90, 0.05, 0.05), byrow = TRUE,                          nrow = m, ncol = q_emiss[1]), # vocalizing patient                   matrix(c(0.1, 0.9,                             0.1, 0.9), byrow = TRUE, nrow = m,                          ncol = q_emiss[2]), # looking patient                   matrix(c(0.90, 0.05, 0.05,                             0.05, 0.90, 0.05), byrow = TRUE,                          nrow = m, ncol = q_emiss[3]), # vocalizing therapist                   matrix(c(0.1, 0.9,                             0.1, 0.9), byrow = TRUE, nrow = m,                          ncol = q_emiss[4])) # looking therapist"},{"path":"/articles/tutorial-mhmm.html","id":"starting-values","dir":"Articles","previous_headings":"Using the package mHMMbayes Introduction > A simple model","what":"Starting values","title":"Multilevel HMM tutorial","text":"subsequent lines code specify starting values transition probability matrix emission distribution(s), given model argument start_val (see next piece code). starting values used first run forward backward algorithm. Although hidden states observed, one often idea probable compositions states. example, expect state patient mostly speaks, therapist silent, state patient silent therapists speaks. addition, expect states, therapist patient mainly looking instead looking away. One usually also (vague) idea likely unlikely switches states, size self-transition probabilities. example, think state usually last quite seconds, thus expect rather high self-transition probability. ideas can used construct set sensible starting values. Using sensible starting values increases convergence speed, often prevents problem called ‘label switching.’ Hence, using random uniform starting values recommended, default option included package. Note strongly advised check model convergence label switching. , one check algorithm reaches solution set different (often conceptually similar) starting values used, label switching problem. See section Checking model convergence label switching example. See vignette Estimation multilevel hidden Markov model) information forward backward algorithm problem label switching.","code":""},{"path":"/articles/tutorial-mhmm.html","id":"prior-distributions","dir":"Articles","previous_headings":"Using the package mHMMbayes Introduction > A simple model","what":"Prior distributions","title":"Multilevel HMM tutorial","text":"estimation proceeds within Bayesian context, (hyper-)prior distribution defined group level parameters, .e., group level emission transition probabilities. Default, non-informative priors used unless specified otherwise user. , present information . elaborate explanation used (hyper-)prior distributions parameters, see vignette Estimation multilevel hidden Markov model. First , note prior distributions emission distribution transition probabilities probabilities directly, intercepts (regression coefficients given covariates used) Multinomial regression model used accommodate multilevel framework data. Second, parameters independent prior distribution. row emission distribution matrix transition probability matrix sum one, individual parameters rows connected. Hence, row seen set parameters estimated jointly, set parameters multivariate prior distribution. sets intercepts Multinomial regression model multivariate normal distribution. (hyper-) prior intercepts thus consists prior distribution vector means, prior distribution covariance matrix. hyper-prior mean intercepts multivariate Normal distribution, , default, vector means equal 0, parameter \\(K_0\\) covariance matrix multiplied. , \\(K_0\\) denotes number observations (.e., number hypothetical prior subjects) prior mean vector zero’s based. default, \\(K_0\\) set 1. hyper-prior covariance matrix set (state specific) intercepts Inverse Wishart distribution, variance default setting equals 3 + \\(m\\) - 1 transition probabilities 3 + \\(q\\) - 1 emission probabilities, covariance equals 0. degrees freedom Inverse Wishart distribution set 3 + \\(m\\) - 1 transition probabilities 3 + \\(q\\) - 1 emission probabilities. specify user specific prior distributions, one uses input option emiss_hyp_prior emission distribution gamma_hyp_prior transition probabilities function mHMM. input arguments take object class mHMM_prior_emiss mHMM_prior_gamma created functions prior_emiss_cat prior_gamma, respectively. objects list, containing following key elements: mu0, lists containing hypothesized hyper-prior mean values intercepts Multinomial logit model. K0, number hypothetical prior subjects set hyper-prior mean intercepts specified mu0 based. nu, degrees freedom hyper-prior Inverse Wishart distribution covariance Multinomial logit intercepts. V, variance-covariance hyper-prior Inverse Wishart distribution covariance Multinomial logit intercepts. Note K0, nu V assumed equal states. mean values intercepts (regression coefficients covariates) denoted mu0 allowed vary states. elements list either prefix gamma_ emiss_, depending list belong . specifying prior distributions, note first element row probability domain intercept, serves baseline category Multinomial logit regression model. means, example, specify model 3 states, mu0 vector 2 elements, K0 nu contain 1 element V 2 2 matrix.","code":""},{"path":"/articles/tutorial-mhmm.html","id":"fitting-the-model","dir":"Articles","previous_headings":"Using the package mHMMbayes Introduction > A simple model","what":"Fitting the model","title":"Multilevel HMM tutorial","text":"multilevel HMM fitted using function mHMM: call mHMM specifies model several arguments. s_data argument specifies input data used infer hidden states time. gen start_val argument specify general model properties starting values, discussed . arguments needed MCMC algorithm given mcmc: J specifies number iterations used hybrid metropolis within Gibbs algorithm burn_in specifies number iterations discard obtaining model parameter summary statistics. function mHMM returns object class mHMM, print summary methods see results. print method provides basic information model fitted. , number subjects dataset analyzed, number iterations burn-period, average log likelihood subjects model fit indices AIC, number states specified, number dependent variables states based : summary method provides information estimated parameters. , point estimates posterior distribution transition probability matrix emission distribution dependent variables group level: resulting model indicates 2 well separated states: one patient speaking one therapist speaking. Looking behavior quite similar patient therapist 2 states. Information estimated parameters can also obtained using function obtain_gamma obtain_emiss. functions allow user inspect estimated parameters group level, subject individually well, specifying input variable level = \"subject\": additional option functions obtain_gamma obtain_emiss offer changing burn-period used obtaining summary statistics, using input variable burn_in.","code":"# Run a model without covariate(s) and default priors: set.seed(14532) out_2st <- mHMM(s_data = nonverbal,                      gen = list(m = m, n_dep = n_dep, q_emiss = q_emiss),                      start_val = c(list(start_TM), start_EM),                     mcmc = list(J = 1000, burn_in = 200)) out_2st #> Number of subjects: 10  #>  #> 1000 iterations used in the MCMC algorithm with a burn in of 200  #> Average Log likelihood over all subjects: -1624.389  #> Average AIC over all subjects: 3276.777  #>  #> Number of states used: 2  #>  #> Number of dependent variables used: 4 summary(out_2st) #> State transition probability matrix  #>  (at the group level):  #>   #>              To state 1 To state 2 #> From state 1      0.929      0.071 #> From state 2      0.074      0.926 #>  #>   #> Emission distribution for each of the dependent variables  #>  (at the group level):  #>   #> $p_vocalizing #>         Category 1 Category 2 Category 3 #> State 1      0.018      0.957      0.024 #> State 2      0.795      0.052      0.153 #>  #> $p_looking #>         Category 1 Category 2 #> State 1      0.248      0.752 #> State 2      0.096      0.904 #>  #> $t_vocalizing #>         Category 1 Category 2 Category 3 #> State 1      0.806      0.075      0.119 #> State 2      0.034      0.945      0.020 #>  #> $t_looking #>         Category 1 Category 2 #> State 1      0.047      0.953 #> State 2      0.277      0.723 # When not specified, level defaults to \"group\" gamma_pop <- obtain_gamma(out_2st) gamma_pop #>              To state 1 To state 2 #> From state 1      0.929      0.071 #> From state 2      0.074      0.926  # To obtain the subject specific parameter estimates: gamma_subj <- obtain_gamma(out_2st, level = \"subject\") gamma_subj #> $`Subject 1` #>              To state 1 To state 2 #> From state 1      0.942      0.058 #> From state 2      0.048      0.952 #>  #> $`Subject 2` #>              To state 1 To state 2 #> From state 1      0.936      0.064 #> From state 2      0.060      0.940 #>  #> $`Subject 3` #>              To state 1 To state 2 #> From state 1      0.969      0.031 #> From state 2      0.054      0.946 #>  #> $`Subject 4` #>              To state 1 To state 2 #> From state 1      0.934      0.066 #> From state 2      0.046      0.954 #>  #> $`Subject 5` #>              To state 1 To state 2 #> From state 1      0.942      0.058 #> From state 2      0.058      0.942 #>  #> $`Subject 6` #>              To state 1 To state 2 #> From state 1      0.942      0.058 #> From state 2      0.087      0.913 #>  #> $`Subject 7` #>              To state 1 To state 2 #> From state 1      0.929      0.071 #> From state 2      0.043      0.958 #>  #> $`Subject 8` #>              To state 1 To state 2 #> From state 1       0.93      0.070 #> From state 2       0.08      0.919 #>  #> $`Subject 9` #>              To state 1 To state 2 #> From state 1      0.948      0.052 #> From state 2      0.058      0.942 #>  #> $`Subject 10` #>              To state 1 To state 2 #> From state 1      0.960      0.040 #> From state 2      0.068      0.932"},{"path":"/articles/tutorial-mhmm.html","id":"graphically-displaying-outcomes","dir":"Articles","previous_headings":"Using the package mHMMbayes Introduction","what":"Graphically displaying outcomes","title":"Multilevel HMM tutorial","text":"package includes several plot functions display fitted model parameters. First, one can plot posterior densities fitted model, transition probability matrix gamma emission distribution probabilities. posterior densities plotted group level subject level simultaneously. example, emission distribution variable p_vocalizing: , component specifies whether want visualize posterior densities transition probability matrix gamma (component = \"gamma\") emission distribution probabilities (component = \"emiss\"), using component = \"emiss\" input variable dep specifies dependent variable want inspect (variable p_vocolizing first variable set, set dep = 1), col specifies colors used plotting lines, dep_lab denotes label dependent variable plotting, cat_lab denotes labels categorical outcomes dependent variable. plot, solid line visualizes posterior density group level, dotted lines visualizes posterior density one subject. Second, one can plot transition probabilities obtained function obtain_gamma riverplot:  Note graphically displaying transition probabilities becomes informative number states increase.","code":"library(RColorBrewer) Voc_col <- c(brewer.pal(3,\"PuBuGn\")[c(1,3,2)]) Voc_lab <- c(\"Not Speaking\", \"Speaking\", \"Back channeling\")  plot(out_2st, component = \"emiss\", dep = 1, col = Voc_col,       dep_lab = c(\"Patient vocalizing\"), cat_lab = Voc_lab) # Transition probabilities at the group level and for subject number 1, respectively: plot(gamma_pop, col = rep(rev(brewer.pal(3,\"PiYG\"))[-2], each = m)) plot(gamma_subj, subj_nr = 1, col = rep(rev(brewer.pal(3,\"PiYG\"))[-2], each = m))"},{"path":"/articles/tutorial-mhmm.html","id":"determining-the-number-of-hidden-states","dir":"Articles","previous_headings":"Using the package mHMMbayes Introduction","what":"Determining the number of hidden states","title":"Multilevel HMM tutorial","text":"first step developing HMM determine number states \\(m\\) best describes observed data, model selection problem. modelling, example, behavior, task define states clusters observed behavioral outcomes provide reasonable, theoretically interpretable, description data. suggest using combination Akaike Information Criterion (AIC) theoretical interpretability estimated states choose models1. example dataset, 2, 3 4 state model result AIC 3279, 3087, 2959, respectively. According model fit indices, 4 state model clearly best model2. Let’s inspect composition states 4 state model, transition probabilities:  can see state patient speaks therapist silent (state 1), state patient silent therapist speaks (state 2), state patient therapist speak (state 3) state therapist speaks look patient (contrast looking behavior states), patient silent. addition, states quite stable probability remaining state .6 states.","code":"summary(out_4st) #> State transition probability matrix  #>  (at the group level):  #>   #>              To state 1 To state 2 To state 3 To state 4 #> From state 1      0.910      0.031      0.040      0.019 #> From state 2      0.044      0.815      0.056      0.084 #> From state 3      0.183      0.097      0.666      0.054 #> From state 4      0.024      0.220      0.019      0.738 #>  #>   #> Emission distribution for each of the dependent variables  #>  (at the group level):  #>   #> $p_vocalizing #>         Category 1 Category 2 Category 3 #> State 1      0.011      0.976      0.013 #> State 2      0.729      0.046      0.225 #> State 3      0.184      0.665      0.150 #> State 4      0.876      0.065      0.059 #>  #> $p_looking #>         Category 1 Category 2 #> State 1      0.237      0.763 #> State 2      0.061      0.939 #> State 3      0.439      0.561 #> State 4      0.094      0.906 #>  #> $t_vocalizing #>         Category 1 Category 2 Category 3 #> State 1      0.889      0.013      0.097 #> State 2      0.019      0.971      0.011 #> State 3      0.332      0.593      0.074 #> State 4      0.087      0.845      0.068 #>  #> $t_looking #>         Category 1 Category 2 #> State 1      0.030      0.970 #> State 2      0.045      0.955 #> State 3      0.074      0.926 #> State 4      0.949      0.051  m <- 4 plot(obtain_gamma(out_4st), cex = .5, col = rep(rev(brewer.pal(5,\"PiYG\"))[-3], each = m))"},{"path":"/articles/tutorial-mhmm.html","id":"determining-the-most-likely-state-sequence","dir":"Articles","previous_headings":"Using the package mHMMbayes Introduction","what":"Determining the most likely state sequence","title":"Multilevel HMM tutorial","text":"Given well-fitting HMM, may interest determine actual sequence, order succession, hidden states likely given rise sequence outcomes observed subject. One can either use local decoding, probabilities hidden state sequence obtained simultaneously model parameters estimates, well-known Viterbi algorithm (Viterbi 1967; Forney Jr 1973). local decoding, likely state determined separately time point \\(t\\), contrast Viterbi algorithm one determines joint probability complete sequence observations \\(O_{1:T}\\) complete sequence hidden states \\(S_{1:T}\\). package, local decoding can achieved saving sampled hidden state sequence iteration Gibbs sampler, setting input variable return_path = TRUE function mHMM. result large output files, however. Global decoding can performed using function vit_mHMM: function returns hidden state sequence subject matrix, row represents point time column represents subject. can inspect obtained hidden state sequence example plotting together observed data. , first 5 minutes first couple plotted , addition estimated state sequence:","code":"state_seq <- vit_mHMM(out_2st, s_data = nonverbal)  head(state_seq) #>      Subj_1 Subj_2 Subj_3 Subj_4 Subj_5 Subj_6 Subj_7 Subj_8 Subj_9 Subj_10 #> [1,]      1      2      2      2      1      2      1      1      1       1 #> [2,]      1      2      2      2      1      2      1      1      1       1 #> [3,]      1      2      2      2      2      2      1      1      1       1 #> [4,]      1      2      2      2      2      2      1      1      1       1 #> [5,]      1      2      2      2      2      2      2      1      1       1 #> [6,]      1      2      2      2      2      1      2      1      1       1"},{"path":"/articles/tutorial-mhmm.html","id":"checking-model-convergence-and-label-switching","dir":"Articles","previous_headings":"Using the package mHMMbayes Introduction","what":"Checking model convergence and label switching","title":"Multilevel HMM tutorial","text":"using Bayesian estimation procedures, strongly advised check model convergence label switching. , one check algorithm reaches solution set different (often conceptually similar) starting values used, label switching problem. label switching, label (.e., state represents ) ordering states switches iterations estimation algorithm. example, started state 1, now becomes state 2. One can check model convergence label switching visually inspecting trace plots parameters set identical models used varying starting values. Trace plots plots sampled parameter values iterations. First, fit model 2 states , different starting values: group level parameter estimates emission probabilities transition probability matrix iteration estimation algorithm stored objects emiss_prob_bar gamma_prob_bar, respectively. subject level parameter estimates stored object PD_subj, PD abbreviation posterior density. , example, want inspect trace plots emission probabilities looking behavior patient group level, use following code:  can observed parameter estimates converge parameter space, chains mix well. Also, evidence label switching.","code":"# specifying general model properties m <-2 n_dep <- 4 q_emiss <- c(3, 2, 3, 2)  # specifying different starting values start_TM <- diag(.8, m) start_TM[lower.tri(start_TM) | upper.tri(start_TM)] <- .2 start_EM_b <- list(matrix(c(0.2, 0.6, 0.2,                             0.6, 0.2, 0.2), byrow = TRUE,                         nrow = m, ncol = q_emiss[1]), # vocalizing patient                  matrix(c(0.4, 0.6,                           0.4, 0.6), byrow = TRUE, nrow = m,                         ncol = q_emiss[2]), # looking patient                  matrix(c(0.6, 0.2, 0.2,                           0.2, 0.6, 0.2), byrow = TRUE,                         nrow = m, ncol = q_emiss[3]), # vocalizing therapist                  matrix(c(0.4, 0.6,                           0.4, 0.6), byrow = TRUE, nrow = m,                         ncol = q_emiss[4])) # looking therapist  # Run a model identical to out_2st, but with different starting values: set.seed(9843) out_2st_b <- mHMM(s_data = nonverbal,                        gen = list(m = m, n_dep = n_dep, q_emiss = q_emiss),                        start_val = c(list(start_TM), start_EM),                       mcmc = list(J = 1000, burn_in = 200)) par(mfrow = c(m,q_emiss[2])) for(i in 1:m){   for(q in 1:q_emiss[2]){      plot(x = 1:1000, y = out_2st$emiss_prob_bar[[2]][,(i-1) * q_emiss[2] + q],            ylim = c(0,1.4), yaxt = 'n', type = \"l\", ylab = \"Transition probability\",           xlab = \"Iteration\", main = paste(\"Patient\", Look_lab[q], \"in state\", i), col = \"#8da0cb\")      axis(2, at = seq(0,1, .2), las = 2)     lines(x = 1:1000, y = out_2st_b$emiss_prob_bar[[2]][,(i-1) * q_emiss[2] + q], col = \"#e78ac3\")     legend(\"topright\", col = c(\"#8da0cb\", \"#e78ac3\"), lwd = 2,             legend = c(\"Starting value set 1\", \"Starting value set 2\"), bty = \"n\")   } }"},{"path":[]},{"path":"/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Emmeke Aarts. Author, maintainer. Sebastian Mildiner Moraga. Contributor.","code":""},{"path":"/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Aarts E (2023). mHMMbayes: Multilevel Hidden Markov Models Using Bayesian Estimation. R package version 0.2.0, https://github.com/emmekeaarts/mHMMbayes.","code":"@Manual{,   title = {mHMMbayes: Multilevel Hidden Markov Models Using Bayesian Estimation},   author = {Emmeke Aarts},   year = {2023},   note = {R package version 0.2.0},   url = {https://github.com/emmekeaarts/mHMMbayes}, }"},{"path":"/index.html","id":"mhmmbayes---","dir":"","previous_headings":"","what":"Multilevel Hidden Markov Models Using Bayesian Estimation","title":"Multilevel Hidden Markov Models Using Bayesian Estimation","text":"package mHMMbayes can fit multilevel hidden Markov models. multilevel hidden Markov model (HMM) generalization well-known hidden Markov model, tailored accommodate (intense) longitudinal data multiple individuals simultaneously. Using multilevel framework, allow heterogeneity model parameters (transition probability matrix conditional distribution), estimating one overall HMM. model great potential application many fields, social sciences medicine. model can fitted multivariate data categorical distribution, include individual level covariates (allowing e.g. group comparisons model parameters). Parameters estimated using Bayesian estimation utilizing forward-backward recursion within hybrid Metropolis within Gibbs sampler. package also includes various options model visualization, function simulate data function obtain likely hidden state sequence individual using Viterbi algorithm. Please hesitate contact questions regarding package.","code":""},{"path":"/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Multilevel Hidden Markov Models Using Bayesian Estimation","text":"can install mHMMbayes github :","code":"# install.packages(\"devtools\") devtools::install_github(\"emmekeaarts/mHMMbayes\")"},{"path":"/index.html","id":"usage","dir":"","previous_headings":"","what":"Usage","title":"Multilevel Hidden Markov Models Using Bayesian Estimation","text":"basic example shows run model using example data included package, simulate data. elaborate introduction, see vignette(\"tutorial-mhmm\") accompanying package.","code":"library(mHMMbayes)  ##### Simple 2 state model # specifying general model properties m <- 2 n_dep <- 4 q_emiss <- c(3, 2, 3, 2)  # specifying starting values start_TM <- diag(.8, m) start_TM[lower.tri(start_TM) | upper.tri(start_TM)] <- .2 start_EM <- list(matrix(c(0.05, 0.90, 0.05,                           0.90, 0.05, 0.05), byrow = TRUE,                          nrow = m, ncol = q_emiss[1]), # vocalizing patient                   matrix(c(0.1, 0.9,                            0.1, 0.9), byrow = TRUE, nrow = m,                          ncol = q_emiss[2]), # looking patient                   matrix(c(0.90, 0.05, 0.05,                            0.05, 0.90, 0.05), byrow = TRUE,                          nrow = m, ncol = q_emiss[3]), # vocalizing therapist                   matrix(c(0.1, 0.9,                            0.1, 0.9), byrow = TRUE, nrow = m,                          ncol = q_emiss[4])) # looking therapist   # Run a model without covariates.   # Note that normally, a much higher number of iterations J would be used  set.seed(23245)  out_2st <- mHMM(s_data = nonverbal,                gen = list(m = m, n_dep = n_dep, q_emiss = q_emiss),                start_val = c(list(start_TM), start_EM),               mcmc = list(J = 11, burn_in = 5)) #> Progress of the Bayesian mHMM algorithm:  #>   |                                                                              |                                                                      |   0%  |                                                                              |========                                                              |  11%  |                                                                              |================                                                      |  22%  |                                                                              |=======================                                               |  33%  |                                                                              |===============================                                       |  44%  |                                                                              |=======================================                               |  56%  |                                                                              |===============================================                       |  67%  |                                                                              |======================================================                |  78%  |                                                                              |==============================================================        |  89%  |                                                                              |======================================================================| 100% #> Total time elapsed (hh:mm:ss): 00:00:02   out_2st #> Number of subjects: 10  #>  #> 11 iterations used in the MCMC algorithm with a burn in of 5  #> Average Log likelihood over all subjects: -1640.002  #> Average AIC over all subjects: 3308.004  #>  #> Number of states used: 2  #>  #> Number of dependent variables used: 4 summary(out_2st) #> State transition probability matrix  #>  (at the group level):  #>   #>              To state 1 To state 2 #> From state 1      0.946      0.054 #> From state 2      0.080      0.920 #>  #>   #> Emission distribution for each of the dependent variables  #>  (at the group level):  #>   #> $p_vocalizing #>         Category 1 Category 2 Category 3 #> State 1      0.034      0.937      0.029 #> State 2      0.738      0.139      0.122 #>  #> $p_looking #>         Category 1 Category 2 #> State 1      0.285      0.715 #> State 2      0.090      0.910 #>  #> $t_vocalizing #>         Category 1 Category 2 Category 3 #> State 1      0.764      0.126      0.111 #> State 2      0.026      0.960      0.014 #>  #> $t_looking #>         Category 1 Category 2 #> State 1      0.052      0.948 #> State 2      0.282      0.718  # Run a model including a covariate  # Here, the covariate (standardized CDI change) predicts the emission  # distribution for each of the 4 dependent variables: n_subj <- 10 xx <- rep(list(matrix(1, ncol = 1, nrow = n_subj)), (n_dep + 1)) for(i in 2:(n_dep + 1)){  xx[[i]] <- cbind(xx[[i]], nonverbal_cov$std_CDI_change) } out_2st_c <- mHMM(s_data = nonverbal, xx = xx,                   gen = list(m = m, n_dep = n_dep, q_emiss = q_emiss),                   start_val = c(list(start_TM), start_EM),                  mcmc = list(J = 11, burn_in = 5)) #> Progress of the Bayesian mHMM algorithm:  #>   |                                                                              |                                                                      |   0%  |                                                                              |========                                                              |  11%  |                                                                              |================                                                      |  22%  |                                                                              |=======================                                               |  33%  |                                                                              |===============================                                       |  44%  |                                                                              |=======================================                               |  56%  |                                                                              |===============================================                       |  67%  |                                                                              |======================================================                |  78%  |                                                                              |==============================================================        |  89%  |                                                                              |======================================================================| 100% #> Total time elapsed (hh:mm:ss): 00:00:02     ### Simulating data # simulating data for 10 subjects with each 100 observations n_t     <- 100 n       <- 10 m       <- 3 n_dep   <- 1 q_emiss <- 4 gamma   <- matrix(c(0.8, 0.1, 0.1,                     0.2, 0.7, 0.1,                     0.2, 0.2, 0.6), ncol = m, byrow = TRUE) emiss_distr <- list(matrix(c(0.5, 0.5, 0.0, 0.0,                              0.1, 0.1, 0.8, 0.0,                              0.0, 0.0, 0.1, 0.9),                             nrow = m, ncol = q_emiss, byrow = TRUE))  data1 <- sim_mHMM(n_t = n_t, n = n,                    gen = list(m = m, n_dep = n_dep, q_emiss = q_emiss),                   gamma = gamma, emiss_distr = emiss_distr,                    var_gamma = 1, var_emiss = 1) head(data1$obs) #>      subj observation 1 #> [1,]    1             1 #> [2,]    1             1 #> [3,]    1             1 #> [4,]    1             1 #> [5,]    1             2 #> [6,]    1             1 head(data1$states) #>      subj state #> [1,]    1     1 #> [2,]    1     1 #> [3,]    1     1 #> [4,]    1     1 #> [5,]    1     1 #> [6,]    1     1    # simulating subject specific transition probability matrices # and emission distributions only n_t <- 0 n <- 5 m <- 3 n_dep   <- 1 q_emiss <- 4 gamma <- matrix(c(0.8, 0.1, 0.1,                   0.2, 0.7, 0.1,                   0.2, 0.2, 0.6), ncol = m, byrow = TRUE) emiss_distr <- list(matrix(c(0.5, 0.5, 0.0, 0.0,                              0.1, 0.1, 0.8, 0.0,                              0.0, 0.0, 0.1, 0.9),                             nrow = m, ncol = q_emiss, byrow = TRUE)) set.seed(549801)  data2 <- sim_mHMM(n_t = n_t, n = n,                    gen = list(m = m, n_dep = n_dep, q_emiss = q_emiss),                   gamma = gamma, emiss_distr = emiss_distr,                    var_gamma = 1, var_emiss = 1) data2 #> $subject_gamma #> $subject_gamma[[1]] #>        [,1]   [,2]   [,3] #> [1,] 0.6302 0.2849 0.0849 #> [2,] 0.1817 0.7714 0.0469 #> [3,] 0.2164 0.1738 0.6098 #>  #> $subject_gamma[[2]] #>        [,1]   [,2]   [,3] #> [1,] 0.7819 0.1235 0.0945 #> [2,] 0.0747 0.9015 0.0238 #> [3,] 0.3285 0.4705 0.2011 #>  #> $subject_gamma[[3]] #>        [,1]   [,2]   [,3] #> [1,] 0.6228 0.1443 0.2329 #> [2,] 0.5242 0.3106 0.1652 #> [3,] 0.5215 0.1167 0.3618 #>  #> $subject_gamma[[4]] #>        [,1]   [,2]   [,3] #> [1,] 0.5726 0.1054 0.3220 #> [2,] 0.1751 0.5438 0.2811 #> [3,] 0.2109 0.1686 0.6204 #>  #> $subject_gamma[[5]] #>        [,1]   [,2]   [,3] #> [1,] 0.8227 0.1212 0.0561 #> [2,] 0.2029 0.5990 0.1982 #> [3,] 0.0902 0.3200 0.5898 #>  #>  #> $subject_emiss #> $subject_emiss[[1]] #> $subject_emiss[[1]][[1]] #>        [,1]   [,2]   [,3]   [,4] #> [1,] 0.4916 0.5083 0.0001 0.0000 #> [2,] 0.0572 0.1810 0.7618 0.0000 #> [3,] 0.0000 0.0000 0.0629 0.9371 #>  #>  #> $subject_emiss[[2]] #> $subject_emiss[[2]][[1]] #>        [,1]   [,2]   [,3]   [,4] #> [1,] 0.1451 0.8549 0.0000 0.0000 #> [2,] 0.0510 0.1518 0.7972 0.0000 #> [3,] 0.0000 0.0000 0.0514 0.9486 #>  #>  #> $subject_emiss[[3]] #> $subject_emiss[[3]][[1]] #>        [,1]   [,2]   [,3]   [,4] #> [1,] 0.2158 0.7842 0.0000 0.0000 #> [2,] 0.1865 0.1831 0.6304 0.0000 #> [3,] 0.0001 0.0002 0.5793 0.4204 #>  #>  #> $subject_emiss[[4]] #> $subject_emiss[[4]][[1]] #>        [,1]   [,2]   [,3]   [,4] #> [1,] 0.3268 0.6732 0.0000 0.0000 #> [2,] 0.0501 0.0867 0.8631 0.0000 #> [3,] 0.0000 0.0000 0.1194 0.8806 #>  #>  #> $subject_emiss[[5]] #> $subject_emiss[[5]][[1]] #>        [,1]   [,2]   [,3]   [,4] #> [1,] 0.7268 0.2731 0.0000 0.0001 #> [2,] 0.1303 0.2549 0.6148 0.0000 #> [3,] 0.0000 0.0000 0.1085 0.8915  set.seed(10893) data3 <- sim_mHMM(n_t = n_t, n = n,                    gen = list(m = m, n_dep = n_dep, q_emiss = q_emiss),                   gamma = gamma, emiss_distr = emiss_distr,                    var_gamma = .5, var_emiss = .5) data3 #> $subject_gamma #> $subject_gamma[[1]] #>        [,1]   [,2]   [,3] #> [1,] 0.7958 0.0461 0.1581 #> [2,] 0.3042 0.4663 0.2295 #> [3,] 0.1396 0.6520 0.2084 #>  #> $subject_gamma[[2]] #>        [,1]   [,2]   [,3] #> [1,] 0.6221 0.0834 0.2946 #> [2,] 0.1143 0.8430 0.0427 #> [3,] 0.1414 0.3805 0.4780 #>  #> $subject_gamma[[3]] #>        [,1]   [,2]  [,3] #> [1,] 0.7416 0.0554 0.203 #> [2,] 0.1403 0.7937 0.066 #> [3,] 0.1915 0.0975 0.711 #>  #> $subject_gamma[[4]] #>        [,1]   [,2]   [,3] #> [1,] 0.6333 0.0932 0.2736 #> [2,] 0.1127 0.6909 0.1964 #> [3,] 0.1058 0.3872 0.5070 #>  #> $subject_gamma[[5]] #>        [,1]   [,2]   [,3] #> [1,] 0.7610 0.1833 0.0557 #> [2,] 0.0781 0.8920 0.0300 #> [3,] 0.2269 0.1116 0.6615 #>  #>  #> $subject_emiss #> $subject_emiss[[1]] #> $subject_emiss[[1]][[1]] #>        [,1]   [,2]   [,3]   [,4] #> [1,] 0.6359 0.3641 0.0000 0.0000 #> [2,] 0.2302 0.2625 0.5073 0.0000 #> [3,] 0.0000 0.0000 0.0326 0.9674 #>  #>  #> $subject_emiss[[2]] #> $subject_emiss[[2]][[1]] #>        [,1]   [,2]   [,3]   [,4] #> [1,] 0.6471 0.3528 0.0000 0.0000 #> [2,] 0.1977 0.2782 0.5240 0.0001 #> [3,] 0.0000 0.0000 0.2446 0.7554 #>  #>  #> $subject_emiss[[3]] #> $subject_emiss[[3]][[1]] #>        [,1]   [,2]   [,3]   [,4] #> [1,] 0.7053 0.2946 0.0000 0.0000 #> [2,] 0.1433 0.0626 0.7940 0.0001 #> [3,] 0.0000 0.0000 0.0274 0.9726 #>  #>  #> $subject_emiss[[4]] #> $subject_emiss[[4]][[1]] #>        [,1]   [,2]   [,3]   [,4] #> [1,] 0.8119 0.1880 0.0000 0.0000 #> [2,] 0.0704 0.0669 0.8627 0.0000 #> [3,] 0.0000 0.0000 0.1557 0.8443 #>  #>  #> $subject_emiss[[5]] #> $subject_emiss[[5]][[1]] #>        [,1]   [,2]   [,3]   [,4] #> [1,] 0.5968 0.4032 0.0000 0.0000 #> [2,] 0.1023 0.1158 0.7819 0.0000 #> [3,] 0.0000 0.0000 0.1358 0.8642"},{"path":"/reference/int_to_prob.html","id":null,"dir":"Reference","previous_headings":"","what":"Transforming a set of Multinomial logit regression intercepts to probabilities — int_to_prob","title":"Transforming a set of Multinomial logit regression intercepts to probabilities — int_to_prob","text":"int_to_prob transforms set Multinomial logit regression intercepts corresponding state transition categorical emission observation probabilities. Note first state category assumed reference category, hence intercept specified first state category.","code":""},{"path":"/reference/int_to_prob.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Transforming a set of Multinomial logit regression intercepts to probabilities — int_to_prob","text":"","code":"int_to_prob(int_matrix)"},{"path":"/reference/int_to_prob.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Transforming a set of Multinomial logit regression intercepts to probabilities — int_to_prob","text":"int_matrix matrix (number states categories - 1) columns number rows determined user. obtaining set probabilities complete transition probability matrix gamma categorical emission distribution matrix, number rows equals number states m. first state / category assumed reference category, intercept specified first category.","code":""},{"path":"/reference/int_to_prob.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Transforming a set of Multinomial logit regression intercepts to probabilities — int_to_prob","text":"int_to_prob returns matrix containing probabilities   row summing one, number columns equal number   states / categories number rows equal number rows   specified input matrix.","code":""},{"path":"/reference/int_to_prob.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Transforming a set of Multinomial logit regression intercepts to probabilities — int_to_prob","text":"Designed ease specification informative hyper-prior values mean intercepts transition probability matrix gamma categorical emission distribution(s) multilevel hidden Markov model functions prior_gamma prior_emiss_cat. check performed correct specifications dimensions.","code":""},{"path":[]},{"path":"/reference/int_to_prob.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Transforming a set of Multinomial logit regression intercepts to probabilities — int_to_prob","text":"","code":"# example for transition probability matrix gamma with 3 states m <- 3 gamma_int <- matrix(c(-1, -1,                        3,  0,                        0,  2), ncol = m-1, nrow = m, byrow = TRUE) gamma_prob <- int_to_prob(gamma_int) gamma_prob #>        [,1]   [,2]   [,3] #> [1,] 0.5761 0.2119 0.2119 #> [2,] 0.0453 0.9094 0.0453 #> [3,] 0.1065 0.1065 0.7870"},{"path":"/reference/mHMM.html","id":null,"dir":"Reference","previous_headings":"","what":"Multilevel hidden  Markov model using Bayesian estimation — mHMM","title":"Multilevel hidden  Markov model using Bayesian estimation — mHMM","text":"mHMM fits multilevel (also known mixed random effects) hidden Markov model (HMM) intense longitudinal data categorical observations multiple subjects using Bayesian estimation, creates object class mHMM. using multilevel framework, allow heterogeneity model parameters subjects, estimating one overall HMM. function includes possibility add covariates level 2 (.e., subject level) varying observation lengths subjects. short description package see mHMMbayes. See vignette(\"tutorial-mhmm\") introduction multilevel hidden Markov models package, see vignette(\"estimation-mhmm\") overview used estimation algorithms.","code":""},{"path":"/reference/mHMM.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Multilevel hidden  Markov model using Bayesian estimation — mHMM","text":"","code":"mHMM(   s_data,   gen,   xx = NULL,   start_val,   mcmc,   return_path = FALSE,   print_iter,   show_progress = TRUE,   gamma_hyp_prior = NULL,   emiss_hyp_prior = NULL,   gamma_sampler = NULL,   emiss_sampler = NULL )"},{"path":"/reference/mHMM.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Multilevel hidden  Markov model using Bayesian estimation — mHMM","text":"s_data matrix containing observations modeled, rows represent observations time. s_data, first column indicates subject id number. Hence, id number repeated rows equal number observations subject. subsequent columns contain dependent variable(s). Note dependent variables numeric, .e., (set ) factor variable(s). total number rows equal sum number observations subject, number columns equal number dependent variables (n_dep) + 1. number observations can vary subjects. gen List containing following elements denoting general model properties: n_dep: numeric vector length 1 denoting number dependent variables q_emiss: numeric vector length n_dep denoting number observed categories categorical emission distribution dependent variables. xx optional list (level 2) covariates predict transition   matrix /emission probabilities. Level 2 covariate(s) means   one observation per subject covariate. first element   list xx used predict transition matrix. Subsequent   elements list used predict emission distribution (  ) dependent variable(s). element list matrix,   number rows equal number subjects. first column   matrix represents intercept, , column consisting   ones. Subsequent columns correspond covariates used predict   transition matrix / emission distribution. See Details   information use covariates. xx omitted completely, xx defaults NULL,   resembling covariates. Specific elements list can also left   empty (.e., set NULL) signify either transition   probability matrix specific emission distribution predicted   covariates. start_val List containing start values transition probability matrix gamma emission distribution(s). first element list contains m m matrix start values gamma. subsequent elements contain m q_emiss[k] matrices start values k n_dep emission distribution(s). Note start_val contain nested lists (.e., lists within lists). mcmc List Markov chain Monte Carlo (MCMC) arguments, containing following elements: burn_in: numeric vector length 1 denoting burn-period MCMC algorithm. return_path logical scalar. sampled state sequence obtained iteration subject returned function (sample_path = TRUE) (sample_path = FALSE). Note sampled state sequence quite large object, hence default setting sample_path = FALSE. Can used local decoding purposes. print_iter argument print_iter deprecated; please use show_progress instead show progress algorithm. show_progress logical scaler. function show text progress bar R console represent progress algorithm (show_progress = TRUE) (show_progress = FALSE). Defaults show_progress = TRUE. gamma_hyp_prior optional object class mHMM_prior_gamma containing user specified parameter values hyper-prior distribution transition probability matrix gamma, generated function prior_gamma. emiss_hyp_prior optional object class mHMM_prior_emiss containing user specified parameter values hyper-prior distribution categorical emission distribution, generated function prior_emiss_cat. gamma_sampler optional object class mHMM_pdRW_gamma containing user specified settings proposal distribution random walk (RW) Metropolis sampler subject level transition probability matrix parameters, generated function pd_RW_gamma. emiss_sampler optional object class mHMM_pdRW_emiss containing user specified settings proposal distribution random walk (RW) Metropolis sampler subject level emission distribution(s) parameters, generated function pd_RW_emiss_cat.","code":""},{"path":"/reference/mHMM.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Multilevel hidden  Markov model using Bayesian estimation — mHMM","text":"mHMM returns object class mHMM,  print summary methods see results.   object contains following components:  PD_subj list containing one matrix per subject   subject level parameter estimates log likelihood   iterations hybrid Metropolis within Gibbs sampler. iterations   sampler contained rows, columns contain subject   level (parameter) estimates subsequently emission probabilities,   transition probabilities log likelihood. gamma_prob_bar matrix containing group level parameter   estimates transition probabilities iterations hybrid   Metropolis within Gibbs sampler. iterations sampler   contained rows, columns contain group level parameter   estimates. covariates included analysis, group level   probabilities represent predicted probability given covariate   average value continuous covariates, given   covariate equals zero dichotomous covariates. gamma_int_bar matrix containing group level intercepts   Multinomial logistic regression modeling transition   probabilities iterations hybrid Metropolis within Gibbs   sampler. iterations sampler contained rows,   columns contain group level intercepts. gamma_cov_bar matrix containing group level regression   coefficients Multinomial logistic regression predicting   transition probabilities iterations hybrid Metropolis within   Gibbs sampler. iterations sampler contained rows,   columns contain group level regression coefficients. gamma_int_subj list containing one matrix per subject   denoting subject level intercepts Multinomial logistic   regression modeling transition probabilities iterations   hybrid Metropolis within Gibbs sampler. iterations sampler   contained rows, columns contain subject level   intercepts. gamma_naccept matrix containing number accepted   draws subject level RW Metropolis step set parameters   transition probabilities. subjects contained rows,   columns contain sets parameters. emiss_prob_bar list containing one matrix per dependent   variable, denoting group level emission probabilities dependent   variable iterations hybrid Metropolis within Gibbs sampler.   iterations sampler contained rows matrix,   columns contain group level emission probabilities. covariates   included analysis, group level probabilities represent   predicted probability given covariate average value   continuous covariates, given covariate equals zero   dichotomous covariates. emiss_int_bar list containing one matrix per dependent   variable, denoting group level intercepts dependent variable   Multinomial logistic regression modeling probabilities   emission distribution iterations hybrid Metropolis within   Gibbs sampler. iterations sampler contained rows   matrix, columns contain group level intercepts. emiss_cov_bar list containing one matrix per dependent   variable, denoting group level regression coefficients   Multinomial logistic regression predicting emission probabilities within   dependent variables iterations hybrid   Metropolis within Gibbs sampler. iterations sampler   contained rows  matrix, columns contain group   level regression coefficients. emiss_int_subj list containing one list per subject denoting   subject level intercepts dependent variable Multinomial   logistic regression modeling probabilities emission distribution   iterations hybrid Metropolis within Gibbs sampler.   lower level list contains one matrix per dependent variable,   iterations sampler contained rows, columns   contain subject level intercepts. emiss_naccept list containing one matrix per dependent   variable number accepted draws subject level RW   Metropolis step set parameters emission distribution.   subjects contained rows, columns matrix   contain sets parameters. input Overview used input specifications: number   states m, number used dependent variables n_dep,   number output categories dependent variables   q_emiss, number iterations J specified burn   period burn_in hybrid Metropolis within Gibbs sampler,   number subjects n_subj, observation length subject   n_vary, column names dependent variables   dep_labels. sample_path list containing one matrix per subject   sampled hidden state sequence hybrid Metropolis within Gibbs   sampler. time points dataset contained rows,   sampled paths iterations contained columns.   returned return_path = TRUE.","code":""},{"path":"/reference/mHMM.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Multilevel hidden  Markov model using Bayesian estimation — mHMM","text":"Covariates specified xx can either dichotomous continuous variables. Dichotomous variables coded 0/1 variables. Categorical factor variables can yet used predictor covariates. user can however break categorical variable multiple dummy variables (.e., dichotomous variables), can used simultaneously analysis. Continuous predictors automatically centered. , mean value covariate subtracted values covariate new mean equals zero. done presented probabilities output (.e., population transition probability matrix population emission probabilities) correspond predicted probabilities average value covariate(s).","code":""},{"path":"/reference/mHMM.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Multilevel hidden  Markov model using Bayesian estimation — mHMM","text":"Rabiner LR (1989). “tutorial hidden Markov models selected applications speech recognition.” Proceedings IEEE, 77(2), 257--286. Scott SL (2002). “Bayesian methods hidden Markov models: Recursive computing 21st century.” Journal American Statistical Association, 97(457), 337--351. Altman RM (2007). “Mixed hidden Markov models: extension hidden Markov model longitudinal data setting.” Journal American Statistical Association, 102(477), 201--210. Rossi PE, Allenby GM, McCulloch R (2012). Bayesian statistics marketing. John Wiley & Sons. Zucchini W, MacDonald IL, Langrock R (2017). Hidden Markov models time series: introduction using R. Chapman Hall/CRC.","code":""},{"path":[]},{"path":"/reference/mHMM.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Multilevel hidden  Markov model using Bayesian estimation — mHMM","text":"","code":"###### Example on package example data, see ?nonverbal # \\donttest{ # specifying general model properties: m <- 2 n_dep <- 4 q_emiss <- c(3, 2, 3, 2)  # specifying starting values start_TM <- diag(.8, m) start_TM[lower.tri(start_TM) | upper.tri(start_TM)] <- .2 start_EM <- list(matrix(c(0.05, 0.90, 0.05,                           0.90, 0.05, 0.05), byrow = TRUE,                         nrow = m, ncol = q_emiss[1]), # vocalizing patient                  matrix(c(0.1, 0.9,                           0.1, 0.9), byrow = TRUE, nrow = m,                         ncol = q_emiss[2]), # looking patient                  matrix(c(0.90, 0.05, 0.05,                           0.05, 0.90, 0.05), byrow = TRUE,                         nrow = m, ncol = q_emiss[3]), # vocalizing therapist                  matrix(c(0.1, 0.9,                           0.1, 0.9), byrow = TRUE, nrow = m,                         ncol = q_emiss[4])) # looking therapist  # Run a model without covariate(s): # Note that for reasons of running time, J is set at a ridiculous low value. # One would typically use a number of iterations J of at least 1000, # and a burn_in of 200. out_2st <- mHMM(s_data = nonverbal,                 gen = list(m = m, n_dep = n_dep, q_emiss = q_emiss),                 start_val = c(list(start_TM), start_EM),                 mcmc = list(J = 11, burn_in = 5)) #> Progress of the Bayesian mHMM algorithm:  #>    |                                                                               |                                                                      |   0%   |                                                                               |========                                                              |  11%   |                                                                               |================                                                      |  22%   |                                                                               |=======================                                               |  33%   |                                                                               |===============================                                       |  44%   |                                                                               |=======================================                               |  56%   |                                                                               |===============================================                       |  67%   |                                                                               |======================================================                |  78%   |                                                                               |==============================================================        |  89%   |                                                                               |======================================================================| 100% #> Total time elapsed (hh:mm:ss): 00:00:07  out_2st #> Number of subjects: 10  #>  #> 11 iterations used in the MCMC algorithm with a burn in of 5  #> Average Log likelihood over all subjects: -1638.898  #> Average AIC over all subjects: 3305.797  #>  #> Number of states used: 2  #>  #> Number of dependent variables used: 4  #>  summary(out_2st) #> State transition probability matrix  #>  (at the group level):  #>   #>              To state 1 To state 2 #> From state 1      0.957      0.043 #> From state 2      0.094      0.905 #>  #>   #> Emission distribution for each of the dependent variables  #>  (at the group level):  #>   #> $p_vocalizing #>         Category 1 Category 2 Category 3 #> State 1      0.034      0.925      0.041 #> State 2      0.759      0.109      0.132 #>  #> $p_looking #>         Category 1 Category 2 #> State 1       0.24       0.76 #> State 2       0.10       0.90 #>  #> $t_vocalizing #>         Category 1 Category 2 Category 3 #> State 1      0.775      0.128      0.098 #> State 2      0.038      0.935      0.027 #>  #> $t_looking #>         Category 1 Category 2 #> State 1      0.058      0.942 #> State 2      0.280      0.720 #>  #>   # plot the posterior densities for the transition and emission probabilities plot(out_2st, component = \"gamma\", col =c(\"darkslategray3\", \"goldenrod\"))   # Run a model including a covariate (see ?nonverbal_cov) to predict the # emission distribution for each of the 4 dependent variables:  n_subj <- 10 xx_emiss <- rep(list(matrix(c(rep(1, n_subj),nonverbal_cov$std_CDI_change),                             ncol = 2, nrow = n_subj)), n_dep) xx <- c(list(matrix(1, ncol = 1, nrow = n_subj)), xx_emiss) out_2st_c <- mHMM(s_data = nonverbal, xx = xx,                   gen = list(m = m, n_dep = n_dep, q_emiss = q_emiss),                   start_val = c(list(start_TM), start_EM),                   mcmc = list(J = 11, burn_in = 5)) #> Progress of the Bayesian mHMM algorithm:  #>    |                                                                               |                                                                      |   0%   |                                                                               |========                                                              |  11%   |                                                                               |================                                                      |  22%   |                                                                               |=======================                                               |  33%   |                                                                               |===============================                                       |  44%   |                                                                               |=======================================                               |  56%   |                                                                               |===============================================                       |  67%   |                                                                               |======================================================                |  78%   |                                                                               |==============================================================        |  89%   |                                                                               |======================================================================| 100% #> Total time elapsed (hh:mm:ss): 00:00:05  # } ###### Example on simulated data # Simulate data for 10 subjects with each 100 observations: n_t <- 100 n <- 10 m <- 2 n_dep <- 1 q_emiss <- 3 gamma <- matrix(c(0.8, 0.2,                   0.3, 0.7), ncol = m, byrow = TRUE) emiss_distr <- list(matrix(c(0.5, 0.5, 0.0,                         0.1, 0.1, 0.8), nrow = m, ncol = q_emiss, byrow = TRUE)) data1 <- sim_mHMM(n_t = n_t, n = n, gen = list(m = m, n_dep = n_dep, q_emiss = q_emiss),                   gamma = gamma, emiss_distr = emiss_distr, var_gamma = .5, var_emiss = .5)  # Specify remaining required analysis input (for the example, we use simulation # input as starting values): n_dep <- 1 q_emiss <- 3  # Run the model on the simulated data: out_2st_sim <- mHMM(s_data = data1$obs,                  gen = list(m = m, n_dep = n_dep, q_emiss = q_emiss),                  start_val = c(list(gamma), emiss_distr),                  mcmc = list(J = 11, burn_in = 5)) #> Progress of the Bayesian mHMM algorithm:  #>    |                                                                               |                                                                      |   0%   |                                                                               |========                                                              |  11%   |                                                                               |================                                                      |  22%   |                                                                               |=======================                                               |  33%   |                                                                               |===============================                                       |  44%   |                                                                               |=======================================                               |  56%   |                                                                               |===============================================                       |  67%   |                                                                               |======================================================                |  78%   |                                                                               |==============================================================        |  89%   |                                                                               |======================================================================| 100% #> Total time elapsed (hh:mm:ss): 00:00:01"},{"path":"/reference/mHMMbayes.html","id":null,"dir":"Reference","previous_headings":"","what":"mHMMbayes: multilevel hidden Markov models using Bayesian estimation. — mHMMbayes","title":"mHMMbayes: multilevel hidden Markov models using Bayesian estimation. — mHMMbayes","text":"R package mHMMbayes can fit multilevel hidden   Markov models. multilevel hidden Markov model (HMM) generalization   well-known hidden Markov model, tailored accommodate (intense)   longitudinal data multiple individuals simultaneously. Using   multilevel framework, allow heterogeneity model parameters   (transition probability matrix conditional distribution),   estimating one overall HMM. model great potential application    many fields, social sciences medicine. model can   fitted multivariate data categorical  distribution, include   individual level covariates (allowing e.g., group comparisons model   parameters). Parameters estimated using Bayesian estimation utilizing   forward-backward recursion within hybrid Metropolis within Gibbs   sampler.","code":""},{"path":"/reference/mHMMbayes.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"mHMMbayes: multilevel hidden Markov models using Bayesian estimation. — mHMMbayes","text":"mHMMbayes package provides three main functions: mHMM   , sim_mHMM vit_mHMM, described .   elaborate guide package mHMMbayes, see tutorial-mhmm   vignette: vignette(\"tutorial-mhmm\", package = \"mHMMbayes\") . extensive   information estimation parameters package, see estimation-mhmm   vignette: vignette(\"estimation-mhmm\", package = \"mHMMbayes\").","code":""},{"path":"/reference/mHMMbayes.html","id":"mhmm","dir":"Reference","previous_headings":"","what":"mHMM","title":"mHMMbayes: multilevel hidden Markov models using Bayesian estimation. — mHMMbayes","text":"function mHMM fits multilevel hidden Markov model (intense longitudinal) data multiple subjects using Bayesian estimation. using multilevel framework, one general 'population' HMM estimated, heterogeneity subjects accommodated. function can handle covariates subject level varying observation lengths subjects. Estimation performed using hybrid Metropolis within Gibbs sampler, completes forward backward algorithm subjects sequential manner.","code":""},{"path":"/reference/mHMMbayes.html","id":"sim-mhmm","dir":"Reference","previous_headings":"","what":"sim_mHMM","title":"mHMMbayes: multilevel hidden Markov models using Bayesian estimation. — mHMMbayes","text":"function sim_mHMM simulates data multiple subjects, data categorical observations follow hidden Markov model (HMM) multilevel structure. multilevel structure implies subject allowed set parameters, parameters subject level (level 1) tied together population distribution level 2 corresponding parameters. shape population distribution parameters normal (.e., Gaussian) distribution. addition (natural /unexplained) heterogeneity subjects, subjects parameters can also depend (set ) covariate(s).","code":""},{"path":"/reference/mHMMbayes.html","id":"vit-mhmm","dir":"Reference","previous_headings":"","what":"vit_mHMM","title":"mHMMbayes: multilevel hidden Markov models using Bayesian estimation. — mHMMbayes","text":"function vit_mHMM obtains likely hidden state sequence subject, given data subject specific parameter estimates. function utilizing Viterbi algorithm.","code":""},{"path":"/reference/nonverbal.html","id":null,"dir":"Reference","previous_headings":"","what":"Nonverbal communication of patients and therapist — nonverbal","title":"Nonverbal communication of patients and therapist — nonverbal","text":"dataset containing nonverbal communication 10 patient-therapist couples, recorded 15 minutes frequency 1 observation per second (= 900 observations per couple).","code":""},{"path":"/reference/nonverbal.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Nonverbal communication of patients and therapist — nonverbal","text":"","code":"nonverbal"},{"path":"/reference/nonverbal.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Nonverbal communication of patients and therapist — nonverbal","text":"matrix 10 * 900 rows 5 variables: id id variable patient - therapist couple distinguish observation belongs couple p_verbalizing verbalizing behavior patient, consisting 1 = verbalizing, 2 = verbalizing, 3 = back channeling p_looking looking behavior patient, consisting 1 = looking therapist, 2 = looking therapist t_verbalizing verbalizing behavior therapist, consisting 1 = verbalizing, 2 = verbalizing, 3 = back channeling t_looking looking behavior therapist, consisting 1 = looking patient, 2 = looking patient","code":""},{"path":"/reference/nonverbal_cov.html","id":null,"dir":"Reference","previous_headings":"","what":"Predictors of nonverbal communication — nonverbal_cov","title":"Predictors of nonverbal communication — nonverbal_cov","text":"dataset containing predictors nonverbal communication 10 patient-therapist couples.","code":""},{"path":"/reference/nonverbal_cov.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Predictors of nonverbal communication — nonverbal_cov","text":"","code":"nonverbal_cov"},{"path":"/reference/nonverbal_cov.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Predictors of nonverbal communication — nonverbal_cov","text":"matrix 10 rows 3 variables: diagnosis Diagnosis patient, consisting 0 = depression, 1 = anxiety std_CDI_change Change measure depression (CDI) therapy, standardized scale std_SCA_change Change measure anxiety (SCARED) therapy, standardized scale","code":""},{"path":"/reference/obtain_emiss.html","id":null,"dir":"Reference","previous_headings":"","what":"Obtain the emission distribution probabilities for a fitted multilevel HMM — obtain_emiss","title":"Obtain the emission distribution probabilities for a fitted multilevel HMM — obtain_emiss","text":"obtain_emiss obtains emission distribution fitted multilevel hidden Markov model, either group level, .e., representing average emission distribution subjects, subject level, returning emission distribution subject.","code":""},{"path":"/reference/obtain_emiss.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Obtain the emission distribution probabilities for a fitted multilevel HMM — obtain_emiss","text":"","code":"obtain_emiss(object, level = \"group\", burn_in = NULL)"},{"path":"/reference/obtain_emiss.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Obtain the emission distribution probabilities for a fitted multilevel HMM — obtain_emiss","text":"object object class mHMM, generated function mHMM. level String specifying returned transition probability matrix gamma group level (level = \"group\"), .e., representing average transition probability matrix subjects, subject level (level = \"subject\"). burn_in integer specifies number iterations discard obtaining model parameter summary statistics. left unspecified (burn_in = NULL), burn period specified creating mHMM object used.","code":""},{"path":"/reference/obtain_emiss.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Obtain the emission distribution probabilities for a fitted multilevel HMM — obtain_emiss","text":"obtain_emiss returns object est_emiss. Depending   specification input variable level, est_emiss   either list matrices emission distribution group level   (level = \"group\") dependent variable, list   lists, dependent variable list returned number   elements equal number subjects analyzed, level =   'subject'). latter scenario, matrix lower level list   represents subject specific emission distribution specific   dependent variable.","code":""},{"path":[]},{"path":"/reference/obtain_emiss.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Obtain the emission distribution probabilities for a fitted multilevel HMM — obtain_emiss","text":"","code":"###### Example on package data, see ?nonverbal # \\donttest{ # specifying general model properties: m <- 2 n_dep <- 4 q_emiss <- c(3, 2, 3, 2)  # specifying starting values start_TM <- diag(.8, m) start_TM[lower.tri(start_TM) | upper.tri(start_TM)] <- .2 start_EM <- list(matrix(c(0.05, 0.90, 0.05,                           0.90, 0.05, 0.05), byrow = TRUE,                         nrow = m, ncol = q_emiss[1]), # vocalizing patient                  matrix(c(0.1, 0.9,                           0.1, 0.9), byrow = TRUE, nrow = m,                         ncol = q_emiss[2]), # looking patient                  matrix(c(0.90, 0.05, 0.05,                           0.05, 0.90, 0.05), byrow = TRUE,                         nrow = m, ncol = q_emiss[3]), # vocalizing therapist                  matrix(c(0.1, 0.9,                           0.1, 0.9), byrow = TRUE, nrow = m,                         ncol = q_emiss[4])) # looking therapist  # Run a model without covariate(s): out_2st <- mHMM(s_data = nonverbal,                 gen = list(m = m, n_dep = n_dep, q_emiss = q_emiss),                 start_val = c(list(start_TM), start_EM),                 mcmc = list(J = 11, burn_in = 5)) #> Progress of the Bayesian mHMM algorithm:  #>    |                                                                               |                                                                      |   0%   |                                                                               |========                                                              |  11%   |                                                                               |================                                                      |  22%   |                                                                               |=======================                                               |  33%   |                                                                               |===============================                                       |  44%   |                                                                               |=======================================                               |  56%   |                                                                               |===============================================                       |  67%   |                                                                               |======================================================                |  78%   |                                                                               |==============================================================        |  89%   |                                                                               |======================================================================| 100% #> Total time elapsed (hh:mm:ss): 00:00:05  out_2st #> Number of subjects: 10  #>  #> 11 iterations used in the MCMC algorithm with a burn in of 5  #> Average Log likelihood over all subjects: -1637.137  #> Average AIC over all subjects: 3302.273  #>  #> Number of states used: 2  #>  #> Number of dependent variables used: 4  #>  summary(out_2st) #> State transition probability matrix  #>  (at the group level):  #>   #>              To state 1 To state 2 #> From state 1      0.934      0.066 #> From state 2      0.072      0.928 #>  #>   #> Emission distribution for each of the dependent variables  #>  (at the group level):  #>   #> $p_vocalizing #>         Category 1 Category 2 Category 3 #> State 1      0.030      0.930       0.04 #> State 2      0.757      0.103       0.14 #>  #> $p_looking #>         Category 1 Category 2 #> State 1      0.261      0.739 #> State 2      0.166      0.834 #>  #> $t_vocalizing #>         Category 1 Category 2 Category 3 #> State 1      0.801      0.100      0.098 #> State 2      0.034      0.942      0.024 #>  #> $t_looking #>         Category 1 Category 2 #> State 1      0.039      0.961 #> State 2      0.275      0.725 #>  #>   # obtaining the emission probabilities at the group and subject level obtain_emiss(out_2st, level = \"group\") #> $p_vocalizing #>         Category 1 Category 2 Category 3 #> State 1      0.030      0.930       0.04 #> State 2      0.757      0.103       0.14 #>  #> $p_looking #>         Category 1 Category 2 #> State 1      0.261      0.739 #> State 2      0.166      0.834 #>  #> $t_vocalizing #>         Category 1 Category 2 Category 3 #> State 1      0.801      0.100      0.098 #> State 2      0.034      0.942      0.024 #>  #> $t_looking #>         Category 1 Category 2 #> State 1      0.039      0.961 #> State 2      0.275      0.725 #>  obtain_emiss(out_2st, level = \"subject\") #> $p_vocalizing #> $p_vocalizing$`Subject 1` #>         Category 1 Category 2 Category 3 #> State 1      0.008      0.987      0.005 #> State 2      0.843      0.045      0.113 #>  #> $p_vocalizing$`Subject 2` #>         Category 1 Category 2 Category 3 #> State 1      0.028      0.827      0.146 #> State 2      0.849      0.048      0.103 #>  #> $p_vocalizing$`Subject 3` #>         Category 1 Category 2 Category 3 #> State 1      0.043      0.938      0.020 #> State 2      0.836      0.045      0.119 #>  #> $p_vocalizing$`Subject 4` #>         Category 1 Category 2 Category 3 #> State 1      0.019      0.954      0.027 #> State 2      0.831      0.068      0.102 #>  #> $p_vocalizing$`Subject 5` #>         Category 1 Category 2 Category 3 #> State 1      0.002      0.989      0.009 #> State 2      0.754      0.168      0.077 #>  #> $p_vocalizing$`Subject 6` #>         Category 1 Category 2 Category 3 #> State 1      0.017      0.951      0.032 #> State 2      0.694      0.108      0.198 #>  #> $p_vocalizing$`Subject 7` #>         Category 1 Category 2 Category 3 #> State 1      0.162      0.784      0.054 #> State 2      0.827      0.048      0.125 #>  #> $p_vocalizing$`Subject 8` #>         Category 1 Category 2 Category 3 #> State 1      0.012      0.963      0.026 #> State 2      0.436      0.408      0.155 #>  #> $p_vocalizing$`Subject 9` #>         Category 1 Category 2 Category 3 #> State 1      0.102      0.837      0.061 #> State 2      0.747      0.129      0.124 #>  #> $p_vocalizing$`Subject 10` #>         Category 1 Category 2 Category 3 #> State 1      0.009      0.966      0.025 #> State 2      0.783      0.107      0.110 #>  #>  #> $p_looking #> $p_looking$`Subject 1` #>         Category 1 Category 2 #> State 1      0.121      0.879 #> State 2      0.037      0.963 #>  #> $p_looking$`Subject 2` #>         Category 1 Category 2 #> State 1      0.300      0.700 #> State 2      0.068      0.932 #>  #> $p_looking$`Subject 3` #>         Category 1 Category 2 #> State 1      0.455      0.545 #> State 2      0.181      0.819 #>  #> $p_looking$`Subject 4` #>         Category 1 Category 2 #> State 1      0.069      0.931 #> State 2      0.015      0.986 #>  #> $p_looking$`Subject 5` #>         Category 1 Category 2 #> State 1      0.220      0.780 #> State 2      0.104      0.896 #>  #> $p_looking$`Subject 6` #>         Category 1 Category 2 #> State 1      0.081      0.919 #> State 2      0.016      0.984 #>  #> $p_looking$`Subject 7` #>         Category 1 Category 2 #> State 1      0.444      0.556 #> State 2      0.210      0.790 #>  #> $p_looking$`Subject 8` #>         Category 1 Category 2 #> State 1      0.355      0.645 #> State 2      0.183      0.817 #>  #> $p_looking$`Subject 9` #>         Category 1 Category 2 #> State 1      0.233      0.767 #> State 2      0.102      0.898 #>  #> $p_looking$`Subject 10` #>         Category 1 Category 2 #> State 1      0.446      0.554 #> State 2      0.168      0.832 #>  #>  #> $t_vocalizing #> $t_vocalizing$`Subject 1` #>         Category 1 Category 2 Category 3 #> State 1      0.822      0.124      0.054 #> State 2      0.020      0.973      0.007 #>  #> $t_vocalizing$`Subject 2` #>         Category 1 Category 2 Category 3 #> State 1      0.822      0.134      0.044 #> State 2      0.060      0.874      0.066 #>  #> $t_vocalizing$`Subject 3` #>         Category 1 Category 2 Category 3 #> State 1      0.855      0.043      0.102 #> State 2      0.035      0.927      0.038 #>  #> $t_vocalizing$`Subject 4` #>         Category 1 Category 2 Category 3 #> State 1      0.755      0.107      0.138 #> State 2      0.014      0.977      0.010 #>  #> $t_vocalizing$`Subject 5` #>         Category 1 Category 2 Category 3 #> State 1      0.814      0.083      0.104 #> State 2      0.012      0.970      0.018 #>  #> $t_vocalizing$`Subject 6` #>         Category 1 Category 2 Category 3 #> State 1      0.808      0.113      0.078 #> State 2      0.023      0.926      0.051 #>  #> $t_vocalizing$`Subject 7` #>         Category 1 Category 2 Category 3 #> State 1      0.736      0.192      0.072 #> State 2      0.038      0.957      0.005 #>  #> $t_vocalizing$`Subject 8` #>         Category 1 Category 2 Category 3 #> State 1      0.629      0.088      0.283 #> State 2      0.026      0.959      0.014 #>  #> $t_vocalizing$`Subject 9` #>         Category 1 Category 2 Category 3 #> State 1       0.75      0.059      0.191 #> State 2       0.03      0.964      0.006 #>  #> $t_vocalizing$`Subject 10` #>         Category 1 Category 2 Category 3 #> State 1      0.813      0.103      0.084 #> State 2      0.008      0.972      0.020 #>  #>  #> $t_looking #> $t_looking$`Subject 1` #>         Category 1 Category 2 #> State 1      0.016      0.984 #> State 2      0.194      0.806 #>  #> $t_looking$`Subject 2` #>         Category 1 Category 2 #> State 1      0.037      0.963 #> State 2      0.295      0.705 #>  #> $t_looking$`Subject 3` #>         Category 1 Category 2 #> State 1      0.010      0.990 #> State 2      0.246      0.754 #>  #> $t_looking$`Subject 4` #>         Category 1 Category 2 #> State 1      0.015      0.985 #> State 2      0.261      0.739 #>  #> $t_looking$`Subject 5` #>         Category 1 Category 2 #> State 1      0.011      0.989 #> State 2      0.202      0.797 #>  #> $t_looking$`Subject 6` #>         Category 1 Category 2 #> State 1      0.017      0.983 #> State 2      0.304      0.696 #>  #> $t_looking$`Subject 7` #>         Category 1 Category 2 #> State 1      0.092      0.908 #> State 2      0.218      0.782 #>  #> $t_looking$`Subject 8` #>         Category 1 Category 2 #> State 1      0.184      0.816 #> State 2      0.191      0.809 #>  #> $t_looking$`Subject 9` #>         Category 1 Category 2 #> State 1      0.336      0.664 #> State 2      0.230      0.770 #>  #> $t_looking$`Subject 10` #>         Category 1 Category 2 #> State 1      0.041      0.959 #> State 2      0.422      0.578 #>  #>   # }"},{"path":"/reference/obtain_gamma.html","id":null,"dir":"Reference","previous_headings":"","what":"Obtain the transition probabilities gamma for a fitted multilevel HMM — obtain_gamma","title":"Obtain the transition probabilities gamma for a fitted multilevel HMM — obtain_gamma","text":"obtain_gamma obtains transition probability matrix fitted multilevel hidden Markov model, either group level, .e.,   representing average transition probability matrix subjects,   subject level, returning transition probability matrices   subject.","code":""},{"path":"/reference/obtain_gamma.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Obtain the transition probabilities gamma for a fitted multilevel HMM — obtain_gamma","text":"","code":"obtain_gamma(object, level = \"group\", burn_in = NULL)"},{"path":"/reference/obtain_gamma.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Obtain the transition probabilities gamma for a fitted multilevel HMM — obtain_gamma","text":"object object class mHMM, generated function mHMM. level String specifying returned transition probability matrix gamma group level (level = \"group\"), .e., representing average transition probability matrix subjects, subject level (level = \"subject\"). burn_in integer specifies number iterations discard obtaining model parameter summary statistics. left unspecified (burn_in = NULL), burn period specified creating mHMM object used.","code":""},{"path":"/reference/obtain_gamma.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Obtain the transition probabilities gamma for a fitted multilevel HMM — obtain_gamma","text":"obtain_gamma returns object est_gamma class  mHMM_gamma. object can directly plotted using function  plot.mHMM_gamma(), simply plot(). Depending   specification input variable level, est_gamma   either matrix transition probabilities group level ( level = \"group\"), list matrices (number elements   equal number subjects analyzed, level = 'subject'),   matrix list represents subject specific transition   probability matrix.","code":""},{"path":[]},{"path":"/reference/obtain_gamma.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Obtain the transition probabilities gamma for a fitted multilevel HMM — obtain_gamma","text":"","code":"###### Example on package data, see ?nonverbal # \\donttest{ # specifying general model properties: m <- 2 n_dep <- 4 q_emiss <- c(3, 2, 3, 2)  # specifying starting values start_TM <- diag(.8, m) start_TM[lower.tri(start_TM) | upper.tri(start_TM)] <- .2 start_EM <- list(matrix(c(0.05, 0.90, 0.05,                           0.90, 0.05, 0.05), byrow = TRUE,                         nrow = m, ncol = q_emiss[1]), # vocalizing patient                  matrix(c(0.1, 0.9,                           0.1, 0.9), byrow = TRUE, nrow = m,                         ncol = q_emiss[2]), # looking patient                  matrix(c(0.90, 0.05, 0.05,                           0.05, 0.90, 0.05), byrow = TRUE,                         nrow = m, ncol = q_emiss[3]), # vocalizing therapist                  matrix(c(0.1, 0.9,                           0.1, 0.9), byrow = TRUE, nrow = m,                         ncol = q_emiss[4])) # looking therapist  # Run a model without covariate(s): out_2st <- mHMM(s_data = nonverbal,                 gen = list(m = m, n_dep = n_dep, q_emiss = q_emiss),                 start_val = c(list(start_TM), start_EM),                 mcmc = list(J = 11, burn_in = 5)) #> Progress of the Bayesian mHMM algorithm:  #>    |                                                                               |                                                                      |   0%   |                                                                               |========                                                              |  11%   |                                                                               |================                                                      |  22%   |                                                                               |=======================                                               |  33%   |                                                                               |===============================                                       |  44%   |                                                                               |=======================================                               |  56%   |                                                                               |===============================================                       |  67%   |                                                                               |======================================================                |  78%   |                                                                               |==============================================================        |  89%   |                                                                               |======================================================================| 100% #> Total time elapsed (hh:mm:ss): 00:00:05  out_2st #> Number of subjects: 10  #>  #> 11 iterations used in the MCMC algorithm with a burn in of 5  #> Average Log likelihood over all subjects: -1640.904  #> Average AIC over all subjects: 3309.808  #>  #> Number of states used: 2  #>  #> Number of dependent variables used: 4  #>  summary(out_2st) #> State transition probability matrix  #>  (at the group level):  #>   #>              To state 1 To state 2 #> From state 1      0.933      0.067 #> From state 2      0.073      0.927 #>  #>   #> Emission distribution for each of the dependent variables  #>  (at the group level):  #>   #> $p_vocalizing #>         Category 1 Category 2 Category 3 #> State 1      0.025      0.952      0.023 #> State 2      0.753      0.088      0.159 #>  #> $p_looking #>         Category 1 Category 2 #> State 1      0.261      0.739 #> State 2      0.112      0.888 #>  #> $t_vocalizing #>         Category 1 Category 2 Category 3 #> State 1      0.784      0.109      0.107 #> State 2      0.051      0.913      0.036 #>  #> $t_looking #>         Category 1 Category 2 #> State 1      0.049      0.951 #> State 2      0.292      0.708 #>  #>   # obtaining the transition probabilities at the group and subject level obtain_gamma(out_2st, level = \"group\") #>              To state 1 To state 2 #> From state 1      0.933      0.067 #> From state 2      0.073      0.927 obtain_gamma(out_2st, level = \"subject\") #> $`Subject 1` #>              To state 1 To state 2 #> From state 1      0.956      0.044 #> From state 2      0.051      0.949 #>  #> $`Subject 2` #>              To state 1 To state 2 #> From state 1      0.937      0.063 #> From state 2      0.050      0.950 #>  #> $`Subject 3` #>              To state 1 To state 2 #> From state 1      0.961      0.039 #> From state 2      0.045      0.955 #>  #> $`Subject 4` #>              To state 1 To state 2 #> From state 1      0.938      0.062 #> From state 2      0.034      0.966 #>  #> $`Subject 5` #>              To state 1 To state 2 #> From state 1      0.953      0.047 #> From state 2      0.057      0.943 #>  #> $`Subject 6` #>              To state 1 To state 2 #> From state 1      0.949      0.052 #> From state 2      0.095      0.905 #>  #> $`Subject 7` #>              To state 1 To state 2 #> From state 1      0.925      0.075 #> From state 2      0.050      0.950 #>  #> $`Subject 8` #>              To state 1 To state 2 #> From state 1      0.957      0.043 #> From state 2      0.059      0.941 #>  #> $`Subject 9` #>              To state 1 To state 2 #> From state 1      0.961      0.039 #> From state 2      0.046      0.954 #>  #> $`Subject 10` #>              To state 1 To state 2 #> From state 1      0.968      0.032 #> From state 2      0.060      0.940 #>   # }"},{"path":"/reference/pd_RW_emiss_cat.html","id":null,"dir":"Reference","previous_headings":"","what":"Proposal distribution settings RW Metropolis sampler for mHMM categorical emission distribution(s) — pd_RW_emiss_cat","title":"Proposal distribution settings RW Metropolis sampler for mHMM categorical emission distribution(s) — pd_RW_emiss_cat","text":"pd_RW_emiss_cat provides framework manually specify settings proposal distribution random walk (RW) Metropolis sampler emission distribution(s) multilevel hidden Markov model, creates object class mHMM_pdRW_emiss. RW metropolis sampler used sampling subject level parameter estimates relating emission distributions dependent variables k, , Multinomial logistic regression intercepts.","code":""},{"path":"/reference/pd_RW_emiss_cat.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Proposal distribution settings RW Metropolis sampler for mHMM categorical emission distribution(s) — pd_RW_emiss_cat","text":"","code":"pd_RW_emiss_cat(gen, emiss_int_mle0, emiss_scalar, emiss_w)"},{"path":"/reference/pd_RW_emiss_cat.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Proposal distribution settings RW Metropolis sampler for mHMM categorical emission distribution(s) — pd_RW_emiss_cat","text":"gen List containing following elements denoting general model properties: n_dep: numeric vector length 1 denoting number dependent variables q_emiss: numeric vector length n_dep denoting number observed categories categorical emission distribution dependent variables. emiss_int_mle0 list containing n_dep elements corresponding dependent variables k, element matrix m rows q_emiss[k] - 1 columns denoting starting values maximum likelihood (ML) estimates Multinomial logit regression intercepts emission distribution(s). ML parameters estimated based pooled data (data subjects). emiss_scalar list containing n_dep elements corresponding dependent variables, element numeric vector length 1 denoting scale factor s. , scale proposal distribution composed covariance matrix Sigma, tuned multiplying scaling factor s^2. emiss_w list containing n_dep elements corresponding dependent variables, element numeric vector length 1 denoting weight overall log likelihood (.e., log likelihood based pooled data subjects) fractional likelihood.","code":""},{"path":"/reference/pd_RW_emiss_cat.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Proposal distribution settings RW Metropolis sampler for mHMM categorical emission distribution(s) — pd_RW_emiss_cat","text":"pd_RW_emiss_cat returns object class  mHMM_pdRW_emiss, containing settings proposal distribution   random walk (RW) Metropolis sampler categorical emission   distribution(s) multilevel hidden Markov model. object   specifically created formatted use function mHMM,   checked correct input dimensions. object contains following   components:  gen list containing elements m, n_dep,   q_emiss, used checking equivalent general model properties   specified pd_RW_emiss_cat mHMM. emiss_int_mle0 list containing n_dep elements,   element matrix  containing starting values maximum   likelihood (ML) estimates Multinomial logit regression intercepts   emission distribution(s). emiss_scalar list containing n_dep elements denoting   scale factor s proposal distribution. emiss_w list containing n_dep elements denoting  denoting weight overall log likelihood fractional  likelihood.","code":""},{"path":"/reference/pd_RW_emiss_cat.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Proposal distribution settings RW Metropolis sampler for mHMM categorical emission distribution(s) — pd_RW_emiss_cat","text":"manual values settings proposal distribution random walk (RW) Metropolis sampler specified (, function pd_RW_emiss_cat used), elements emiss_int_mle0 set 0, emiss_scalar set 2.93 / sqrt(q_emiss[k] - 1), emiss_w set 0.1. See section Scaling proposal distribution RW Metropolis sampler vignette(\"estimation-mhmm\") details. Within function mHMM, acceptance rate RW metropolis sampler relating emission distribution(s) can tracked using output parameter emiss_naccept. acceptance rate 23% considered optimal many parameters updated (Gelman, Carlin, Stern & Rubin, 2014).","code":""},{"path":"/reference/pd_RW_emiss_cat.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Proposal distribution settings RW Metropolis sampler for mHMM categorical emission distribution(s) — pd_RW_emiss_cat","text":"Gelman , Carlin JB, Stern HS, Rubin DB (2014). Bayesian Data Analysis vol. 2. Taylor & Francis. Rossi PE, Allenby GM, McCulloch R (2012). Bayesian statistics marketing. John Wiley & Sons.","code":""},{"path":"/reference/pd_RW_emiss_cat.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Proposal distribution settings RW Metropolis sampler for mHMM categorical emission distribution(s) — pd_RW_emiss_cat","text":"","code":"###### Example using package example data, see ?nonverbal # specifying general model properties: m <- 3 n_dep <- 4 q_emiss <- c(3, 2, 3, 2)  # specifying manual values for RW metropolis sampler on emission distribtutions emiss_int_mle0 <- list(matrix(c( 2,  0,                                 -2, -2,                                  0, -1), byrow = TRUE, nrow = m, ncol = q_emiss[1] - 1),                        matrix(c( 2,                                  2,                                  2), byrow = TRUE, nrow = m, ncol = q_emiss[2] - 1),                        matrix(c(-2, -2,                                  2,  0,                                  0, -1), byrow = TRUE, nrow = m, ncol = q_emiss[3] - 1),                        matrix(c( 2,                                  2,                                  2), byrow = TRUE, nrow = m, ncol = q_emiss[4] - 1)) emiss_scalar <- list(c(2), c(3), c(2), c(3)) emiss_w <- rep(list(c(0.2)), n_dep) manual_emiss_sampler <- pd_RW_emiss_cat(gen = list(m = m, n_dep = n_dep, q_emiss = q_emiss),                                         emiss_int_mle0 = emiss_int_mle0,                                         emiss_scalar = emiss_scalar,                                         emiss_w = emiss_w)  # specifying starting values start_TM <- diag(.7, m) start_TM[lower.tri(start_TM) | upper.tri(start_TM)] <- .1 start_EM <- list(matrix(c(0.05, 0.90, 0.05,                           0.90, 0.05, 0.05,                           0.55, 0.45, 0.05), byrow = TRUE,                         nrow = m, ncol = q_emiss[1]), # vocalizing patient                  matrix(c(0.1, 0.9,                           0.1, 0.9,                           0.1, 0.9), byrow = TRUE, nrow = m,                         ncol = q_emiss[2]), # looking patient                  matrix(c(0.90, 0.05, 0.05,                           0.05, 0.90, 0.05,                           0.55, 0.45, 0.05), byrow = TRUE,                         nrow = m, ncol = q_emiss[3]), # vocalizing therapist                  matrix(c(0.1, 0.9,                           0.1, 0.9,                           0.1, 0.9), byrow = TRUE, nrow = m,                         ncol = q_emiss[4])) # looking therapist  # Note that for reasons of running time, J is set at a ridiculous low value. # One would typically use a number of iterations J of at least 1000, # and a burn_in of 200. # \\donttest{ out_3st_RWemiss <- mHMM(s_data = nonverbal,                          gen = list(m = m, n_dep = n_dep, q_emiss = q_emiss),                          start_val = c(list(start_TM), start_EM),                          emiss_sampler = manual_emiss_sampler,                          mcmc = list(J = 11, burn_in = 5)) #> Progress of the Bayesian mHMM algorithm:  #>    |                                                                               |                                                                      |   0%   |                                                                               |========                                                              |  11%   |                                                                               |================                                                      |  22%   |                                                                               |=======================                                               |  33%   |                                                                               |===============================                                       |  44%   |                                                                               |=======================================                               |  56%   |                                                                               |===============================================                       |  67%   |                                                                               |======================================================                |  78%   |                                                                               |==============================================================        |  89%   |                                                                               |======================================================================| 100% #> Total time elapsed (hh:mm:ss): 00:00:06 # } # \\dontshow{ out_3st_RWemiss <- mHMM(s_data = nonverbal,                          gen = list(m = m, n_dep = n_dep, q_emiss = q_emiss),                          start_val = c(list(start_TM), start_EM),                          emiss_sampler = manual_emiss_sampler,                          mcmc = list(J = 6, burn_in = 3)) #> Progress of the Bayesian mHMM algorithm:  #>    |                                                                               |                                                                      |   0%   |                                                                               |==================                                                    |  25%   |                                                                               |===================================                                   |  50%   |                                                                               |====================================================                  |  75%   |                                                                               |======================================================================| 100% #> Total time elapsed (hh:mm:ss): 00:00:03 # }  out_3st_RWemiss #> Number of subjects: 10  #>  #> 6 iterations used in the MCMC algorithm with a burn in of 3  #> Average Log likelihood over all subjects: -1554.725  #> Average AIC over all subjects: 3157.449  #>  #> Number of states used: 3  #>  #> Number of dependent variables used: 4  #>  summary(out_3st_RWemiss) #> State transition probability matrix  #>  (at the group level):  #>   #>              To state 1 To state 2 To state 3 #> From state 1      0.870      0.040      0.090 #> From state 2      0.033      0.898      0.069 #> From state 3      0.176      0.157      0.667 #>  #>   #> Emission distribution for each of the dependent variables  #>  (at the group level):  #>   #> $p_vocalizing #>         Category 1 Category 2 Category 3 #> State 1      0.006      0.971      0.023 #> State 2      0.818      0.032      0.150 #> State 3      0.258      0.669      0.072 #>  #> $p_looking #>         Category 1 Category 2 #> State 1      0.247      0.753 #> State 2      0.104      0.895 #> State 3      0.238      0.762 #>  #> $t_vocalizing #>         Category 1 Category 2 Category 3 #> State 1      0.813      0.053      0.133 #> State 2      0.024      0.950      0.026 #> State 3      0.265      0.663      0.072 #>  #> $t_looking #>         Category 1 Category 2 #> State 1      0.042      0.958 #> State 2      0.238      0.762 #> State 3      0.178      0.823 #>  #>   # checking acceptance rate (for illustrative purposes, in the example, # J is too low for getting a fair indication) div_J <- function(x, J) x / J J_it <- 11 - 1 # accept/reject starts at iteration 2 of MCMC algorithm RW_emiss_accept <- sapply(out_3st_RWemiss$emiss_naccept, div_J, J_it, simplify = FALSE)  # average acceptance rate over all subjects per parameter # rows represent each of the n_dep dependent variables, columns represent the m states t(sapply(RW_emiss_accept, apply, MARGIN = 2, mean, simplyfy = FALSE)) #>      [,1] [,2] [,3] #> [1,] 0.34 0.19 0.18 #> [2,] 0.20 0.34 0.26 #> [3,] 0.24 0.35 0.21 #> [4,] 0.37 0.19 0.28"},{"path":"/reference/pd_RW_gamma.html","id":null,"dir":"Reference","previous_headings":"","what":"Proposal distribution settings RW Metropolis sampler for mHMM transition probability matrix gamma — pd_RW_gamma","title":"Proposal distribution settings RW Metropolis sampler for mHMM transition probability matrix gamma — pd_RW_gamma","text":"pd_RW_gamma provides framework manually specify settings proposal distribution random walk (RW) Metropolis sampler transition probability matrix gamma multilevel hidden Markov model, creates object class mHMM_pdRW_gamma. RW metropolis sampler used sampling subject level parameter estimates relating transition probability matrix gamma, , Multinomial logistic regression intercepts.","code":""},{"path":"/reference/pd_RW_gamma.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Proposal distribution settings RW Metropolis sampler for mHMM transition probability matrix gamma — pd_RW_gamma","text":"","code":"pd_RW_gamma(m, gamma_int_mle0, gamma_scalar, gamma_w)"},{"path":"/reference/pd_RW_gamma.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Proposal distribution settings RW Metropolis sampler for mHMM transition probability matrix gamma — pd_RW_gamma","text":"m Numeric vector length 1 denoting number hidden states. gamma_int_mle0 matrix m rows m - 1 columns denoting starting values maximum likelihood (ML) estimates Multinomial logit regression intercepts transition probability matrix gamma. ML parameters estimated based pooled data (data subjects). gamma_scalar numeric vector length 1 denoting scale factor s. , scale proposal distribution composed covariance matrix Sigma, tuned multiplying scaling factor s^2. gamma_w numeric vector length 1 denoting weight overall log likelihood (.e., log likelihood based pooled data subjects) fractional likelihood.","code":""},{"path":"/reference/pd_RW_gamma.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Proposal distribution settings RW Metropolis sampler for mHMM transition probability matrix gamma — pd_RW_gamma","text":"pd_RW_gamma returns object class  mHMM_pdRW_gamma, containing settings proposal distribution   random walk (RW) Metropolis sampler transition probability   matrix gamma multilevel hidden Markov model. object   specifically created formatted use function mHMM,   checked correct input dimensions. object contains following   components:  m Numeric vector denoting number hidden states, used   checking equivalent general model properties specified   pd_RW_gamma mHMM. gamma_int_mle0 matrix containing starting values   maximum likelihood (ML) estimates Multinomial logit regression   intercepts transition probability matrix gamma. gamma_scalar numeric vector length 1 denoting   scale factor s proposal distribution. gamma_w numeric vector length 1  denoting  denoting weight overall log likelihood fractional  likelihood.","code":""},{"path":"/reference/pd_RW_gamma.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Proposal distribution settings RW Metropolis sampler for mHMM transition probability matrix gamma — pd_RW_gamma","text":"manual values settings proposal distribution random walk (RW) Metropolis sampler specified (, function pd_RW_gamma used), elements gamma_int_mle0 set 0, gamma_scalar set 2.93 / sqrt(m - 1), gamma_w set 0.1. See section Scaling proposal distribution RW Metropolis sampler vignette(\"estimation-mhmm\") details. Within function mHMM, acceptance rate RW metropolis sampler relating transition probability matrix gamma can tracked using output parameter gamma_naccept. acceptance rate 23% considered optimal many parameters updated (Gelman, Carlin, Stern & Rubin, 2014).","code":""},{"path":"/reference/pd_RW_gamma.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Proposal distribution settings RW Metropolis sampler for mHMM transition probability matrix gamma — pd_RW_gamma","text":"Gelman , Carlin JB, Stern HS, Rubin DB (2014). Bayesian Data Analysis vol. 2. Taylor & Francis. Rossi PE, Allenby GM, McCulloch R (2012). Bayesian statistics marketing. John Wiley & Sons.","code":""},{"path":"/reference/pd_RW_gamma.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Proposal distribution settings RW Metropolis sampler for mHMM transition probability matrix gamma — pd_RW_gamma","text":"","code":"###### Example using package example data, see ?nonverbal # specifying general model properties: m <- 3  # specifying manual values for RW metropolis sampler on gamma gamma_int_mle0 <- matrix(c( -2, -2,                              2,  0,                              0,  3), byrow = TRUE, nrow = m, ncol = m - 1) gamma_scalar <- c(2) gamma_w <- c(0.2) manual_gamma_sampler <- pd_RW_gamma(m = m, gamma_int_mle0 = gamma_int_mle0,                                     gamma_scalar = gamma_scalar,                                     gamma_w = gamma_w)  # specifying starting values n_dep <- 4 q_emiss <- c(3, 2, 3, 2)  start_TM <- diag(.7, m) start_TM[lower.tri(start_TM) | upper.tri(start_TM)] <- .1 start_EM <- list(matrix(c(0.05, 0.90, 0.05,                           0.90, 0.05, 0.05,                           0.55, 0.45, 0.05), byrow = TRUE,                         nrow = m, ncol = q_emiss[1]), # vocalizing patient                  matrix(c(0.1, 0.9,                           0.1, 0.9,                           0.1, 0.9), byrow = TRUE, nrow = m,                         ncol = q_emiss[2]), # looking patient                  matrix(c(0.90, 0.05, 0.05,                           0.05, 0.90, 0.05,                           0.55, 0.45, 0.05), byrow = TRUE,                         nrow = m, ncol = q_emiss[3]), # vocalizing therapist                  matrix(c(0.1, 0.9,                           0.1, 0.9,                           0.1, 0.9), byrow = TRUE, nrow = m,                         ncol = q_emiss[4])) # looking therapist  # Note that for reasons of running time, J is set at a ridiculous low value. # One would typically use a number of iterations J of at least 1000, # and a burn_in of 200. # \\donttest{ out_3st_RWgamma <- mHMM(s_data = nonverbal,                          gen = list(m = m, n_dep = n_dep, q_emiss = q_emiss),                          start_val = c(list(start_TM), start_EM),                          gamma_sampler = manual_gamma_sampler,                          mcmc = list(J = 11, burn_in = 5)) #> Progress of the Bayesian mHMM algorithm:  #>    |                                                                               |                                                                      |   0%   |                                                                               |========                                                              |  11%   |                                                                               |================                                                      |  22%   |                                                                               |=======================                                               |  33%   |                                                                               |===============================                                       |  44%   |                                                                               |=======================================                               |  56%   |                                                                               |===============================================                       |  67%   |                                                                               |======================================================                |  78%   |                                                                               |==============================================================        |  89%   |                                                                               |======================================================================| 100% #> Total time elapsed (hh:mm:ss): 00:00:07 # } # \\dontshow{ out_3st_RWgamma <- mHMM(s_data = nonverbal,                          gen = list(m = m, n_dep = n_dep, q_emiss = q_emiss),                          start_val = c(list(start_TM), start_EM),                          gamma_sampler = manual_gamma_sampler,                          mcmc = list(J = 6, burn_in = 3)) #> Progress of the Bayesian mHMM algorithm:  #>    |                                                                               |                                                                      |   0%   |                                                                               |==================                                                    |  25%   |                                                                               |===================================                                   |  50%   |                                                                               |====================================================                  |  75%   |                                                                               |======================================================================| 100% #> Total time elapsed (hh:mm:ss): 00:00:03 # }  out_3st_RWgamma #> Number of subjects: 10  #>  #> 6 iterations used in the MCMC algorithm with a burn in of 3  #> Average Log likelihood over all subjects: -1552.502  #> Average AIC over all subjects: 3153.004  #>  #> Number of states used: 3  #>  #> Number of dependent variables used: 4  #>  summary(out_3st_RWgamma) #> State transition probability matrix  #>  (at the group level):  #>   #>              To state 1 To state 2 To state 3 #> From state 1      0.882      0.030      0.088 #> From state 2      0.023      0.932      0.045 #> From state 3      0.164      0.154      0.682 #>  #>   #> Emission distribution for each of the dependent variables  #>  (at the group level):  #>   #> $p_vocalizing #>         Category 1 Category 2 Category 3 #> State 1      0.009      0.956      0.035 #> State 2      0.831      0.030      0.138 #> State 3      0.262      0.646      0.091 #>  #> $p_looking #>         Category 1 Category 2 #> State 1      0.235      0.765 #> State 2      0.088      0.912 #> State 3      0.218      0.782 #>  #> $t_vocalizing #>         Category 1 Category 2 Category 3 #> State 1      0.844      0.035      0.121 #> State 2      0.028      0.948      0.025 #> State 3      0.195      0.752      0.053 #>  #> $t_looking #>         Category 1 Category 2 #> State 1      0.048      0.952 #> State 2      0.270      0.730 #> State 3      0.149      0.851 #>  #>   # checking acceptance rate (for illustrative purposes, in the example, # J is too low for getting a fair indication) J_it <- 11 - 1 # accept/reject starts at iteration 2 of MCMC algorithm out_3st_RWgamma$gamma_naccept / J_it #>       [,1] [,2] [,3] #>  [1,]  0.0  0.3  0.4 #>  [2,]  0.4  0.1  0.2 #>  [3,]  0.0  0.2  0.1 #>  [4,]  0.1  0.2  0.2 #>  [5,]  0.2  0.0  0.1 #>  [6,]  0.2  0.1  0.1 #>  [7,]  0.1  0.1  0.0 #>  [8,]  0.1  0.3  0.2 #>  [9,]  0.2  0.2  0.2 #> [10,]  0.1  0.1  0.3 # average acceptance rate over all subjects per parameter apply(out_3st_RWgamma$gamma_naccept / J_it, 2, mean) #> [1] 0.14 0.16 0.18"},{"path":"/reference/plot.mHMM.html","id":null,"dir":"Reference","previous_headings":"","what":"Plotting the posterior densities for a fitted multilevel HMM — plot.mHMM","title":"Plotting the posterior densities for a fitted multilevel HMM — plot.mHMM","text":"plot.mHMM plots posterior densities fitted multilevel hidden Markov model group subject level parameters simultaneously. plotted posterior densities either transition probability matrix gamma, emission distribution probabilities.","code":""},{"path":"/reference/plot.mHMM.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plotting the posterior densities for a fitted multilevel HMM — plot.mHMM","text":"","code":"# S3 method for mHMM plot(   x,   component = \"gamma\",   dep = 1,   col,   cat_lab,   dep_lab,   lwd1 = 2,   lwd2 = 1,   lty1 = 1,   lty2 = 3,   legend_cex,   burn_in,   ... )"},{"path":"/reference/plot.mHMM.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plotting the posterior densities for a fitted multilevel HMM — plot.mHMM","text":"x Object class mHMM, generated function mHMM. component String specifying displayed posterior densities transition probability matrix gamma (component = \"gamma\"), emission distribution probabilities (component = \"emiss\"). case latter model based multiple dependent variables, user indicate dependent variable posterior densities plotted, see dep. dep Integer specifying dependent variable posterior densities plotted. required one wishes plot emission distribution probabilities model based multiple dependent variables. col Vector colors posterior density lines. one plotting posterior densities gamma, vector length m (.e., number hidden states). one plotting posterior densities emission probabilities, vector length q_emiss[k] (.e., number outcome categories dependent variable k). cat_lab Optional vector strings plotting posterior densities emission probabilities, denoting labels categorical outcome values. Automatically generated provided. dep_lab Optional string plotting posterior densities emission probabilities length 1, denoting label dependent variable plotted. Automatically obtained input object x specified. lwd1 Positive number indicating line width posterior density group level. lwd2 Positive number indicating line width posterior density subject level. lty1 Positive number indicating line type posterior density group level. lty2 Positive number indicating line type posterior density subject level. legend_cex numerical value giving amount plotting text symbols legend magnified relative default. burn_in integer specifies number iterations discard obtaining model parameter summary statistics. left unspecified, burn period specified creating mHMM object function mHMM used. ... Arguments passed methods (see par)","code":""},{"path":"/reference/plot.mHMM.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plotting the posterior densities for a fitted multilevel HMM — plot.mHMM","text":"plot.mHMM returns plot posterior densities. Depending   whether (component = \"gamma\") (component = \"emiss\"),   plotted posterior densities either transition probability   matrix gamma emission distribution probabilities, respectively.","code":""},{"path":[]},{"path":"/reference/plot.mHMM.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plotting the posterior densities for a fitted multilevel HMM — plot.mHMM","text":"","code":"###### Example on package example data, see ?nonverbal # First run the function mHMM on example data # \\donttest{ # specifying general model properties: m <- 2 n_dep <- 4 q_emiss <- c(3, 2, 3, 2)  # specifying starting values start_TM <- diag(.8, m) start_TM[lower.tri(start_TM) | upper.tri(start_TM)] <- .2 start_EM <- list(matrix(c(0.05, 0.90, 0.05, 0.90, 0.05, 0.05), byrow = TRUE,                         nrow = m, ncol = q_emiss[1]), # vocalizing patient                  matrix(c(0.1, 0.9, 0.1, 0.9), byrow = TRUE, nrow = m,                         ncol = q_emiss[2]), # looking patient                  matrix(c(0.90, 0.05, 0.05, 0.05, 0.90, 0.05), byrow = TRUE,                         nrow = m, ncol = q_emiss[3]), # vocalizing therapist                  matrix(c(0.1, 0.9, 0.1, 0.9), byrow = TRUE, nrow = m,                         ncol = q_emiss[4])) # looking therapist  # Run a model without covariate(s): out_2st <- mHMM(s_data = nonverbal, gen = list(m = m, n_dep = n_dep,                 q_emiss = q_emiss), start_val = c(list(start_TM), start_EM),                 mcmc = list(J = 11, burn_in = 5)) #> Progress of the Bayesian mHMM algorithm:  #>    |                                                                               |                                                                      |   0%   |                                                                               |========                                                              |  11%   |                                                                               |================                                                      |  22%   |                                                                               |=======================                                               |  33%   |                                                                               |===============================                                       |  44%   |                                                                               |=======================================                               |  56%   |                                                                               |===============================================                       |  67%   |                                                                               |======================================================                |  78%   |                                                                               |==============================================================        |  89%   |                                                                               |======================================================================| 100% #> Total time elapsed (hh:mm:ss): 00:00:05  ## plot the posterior densities for gamma plot(out_2st, component = \"gamma\")  # }"},{"path":"/reference/plot.mHMM_gamma.html","id":null,"dir":"Reference","previous_headings":"","what":"Plotting the transition probabilities gamma for a fitted multilevel HMM — plot.mHMM_gamma","title":"Plotting the transition probabilities gamma for a fitted multilevel HMM — plot.mHMM_gamma","text":"plot.mHMM_gamma plots transition probability matrix fitted multilevel hidden Markov model, means alluvial plot (also known Sankey diagram riverplot) using R package alluvial. plotted transition probability matrix either represents probabilities group level, .e., representing average transition probability matrix subjects, subject level. case latter, user specify subject transition probability matrix plotted.","code":""},{"path":"/reference/plot.mHMM_gamma.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plotting the transition probabilities gamma for a fitted multilevel HMM — plot.mHMM_gamma","text":"","code":"# S3 method for mHMM_gamma plot(x, subj_nr = NULL, cex = 0.8, col, hide, ...)"},{"path":"/reference/plot.mHMM_gamma.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plotting the transition probabilities gamma for a fitted multilevel HMM — plot.mHMM_gamma","text":"x object class mHMM_gamma, generated function obtain_gamma. subj_nr integer specifying specific subject transition probability matrix plotted. required input object represents subject specific transition probability matrices. cex integer specifying scaling fonts category labels. specified, defaults cex = 0.8. col optional vector length m * m (.e., m denotes number hidden states) specifying used colors alluvial plot. hide optional logical vector  length m * m (.e., m denotes number hidden states) specifying whether particular stripes plotted. specified, omits lines representing value exactly zero. ... Arguments passed alluvial (see alluvial)","code":""},{"path":"/reference/plot.mHMM_gamma.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plotting the transition probabilities gamma for a fitted multilevel HMM — plot.mHMM_gamma","text":"plot.mHMM_gamma returns plot transition probability   matrix. Depending whether input object represents transition   probabilities group level subject specific transition   probability matrices, returned plot represents either group   transition probability matrix, transition probability matrix   given subject, specified subject_nr.","code":""},{"path":[]},{"path":"/reference/plot.mHMM_gamma.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plotting the transition probabilities gamma for a fitted multilevel HMM — plot.mHMM_gamma","text":"","code":"# \\donttest{ #' ###### Example on package data, see ?nonverbal # specifying general model properties: m <- 2 n_dep <- 4 q_emiss <- c(3, 2, 3, 2)  # specifying starting values start_TM <- diag(.8, m) start_TM[lower.tri(start_TM) | upper.tri(start_TM)] <- .2 start_EM <- list(matrix(c(0.05, 0.90, 0.05,                           0.90, 0.05, 0.05), byrow = TRUE,                         nrow = m, ncol = q_emiss[1]), # vocalizing patient                  matrix(c(0.1, 0.9,                           0.1, 0.9), byrow = TRUE, nrow = m,                         ncol = q_emiss[2]), # looking patient                  matrix(c(0.90, 0.05, 0.05,                           0.05, 0.90, 0.05), byrow = TRUE,                         nrow = m, ncol = q_emiss[3]), # vocalizing therapist                  matrix(c(0.1, 0.9,                           0.1, 0.9), byrow = TRUE, nrow = m,                         ncol = q_emiss[4])) # looking therapist  # Run a model without covariate(s): out_2st <- mHMM(s_data = nonverbal,                 gen = list(m = m, n_dep = n_dep, q_emiss = q_emiss),                 start_val = c(list(start_TM), start_EM),                 mcmc = list(J = 11, burn_in = 5)) #> Progress of the Bayesian mHMM algorithm:  #>    |                                                                               |                                                                      |   0%   |                                                                               |========                                                              |  11%   |                                                                               |================                                                      |  22%   |                                                                               |=======================                                               |  33%   |                                                                               |===============================                                       |  44%   |                                                                               |=======================================                               |  56%   |                                                                               |===============================================                       |  67%   |                                                                               |======================================================                |  78%   |                                                                               |==============================================================        |  89%   |                                                                               |======================================================================| 100% #> Total time elapsed (hh:mm:ss): 00:00:05  out_2st #> Number of subjects: 10  #>  #> 11 iterations used in the MCMC algorithm with a burn in of 5  #> Average Log likelihood over all subjects: -1640.614  #> Average AIC over all subjects: 3309.228  #>  #> Number of states used: 2  #>  #> Number of dependent variables used: 4  #>  summary(out_2st) #> State transition probability matrix  #>  (at the group level):  #>   #>              To state 1 To state 2 #> From state 1      0.942      0.058 #> From state 2      0.095      0.905 #>  #>   #> Emission distribution for each of the dependent variables  #>  (at the group level):  #>   #> $p_vocalizing #>         Category 1 Category 2 Category 3 #> State 1      0.036      0.919      0.045 #> State 2      0.738      0.117      0.145 #>  #> $p_looking #>         Category 1 Category 2 #> State 1      0.285      0.715 #> State 2      0.084      0.916 #>  #> $t_vocalizing #>         Category 1 Category 2 Category 3 #> State 1      0.804      0.092      0.104 #> State 2      0.036      0.947      0.017 #>  #> $t_looking #>         Category 1 Category 2 #> State 1      0.064      0.936 #> State 2      0.271      0.729 #>  #>   # obtaining the transition probabilities at the group and subject level est_gamma_group <- obtain_gamma(out_2st, level = \"group\")  # plot the obtained transition probabilities plot(est_gamma_group, col = rep(c(\"green\", \"blue\"), each = m))   # } # \\dontshow{ ###### Example on simulated data # Simulate data for 10 subjects with each 100 observations: n_t <- 100 n <- 10 m <- 2 n_dep <- 1 q_emiss <- 3 gamma <- matrix(c(0.8, 0.2,                   0.3, 0.7), ncol = m, byrow = TRUE) emiss_distr <- list(matrix(c(0.5, 0.5, 0.0,                         0.1, 0.1, 0.8), nrow = m, ncol = q_emiss, byrow = TRUE)) data1 <- sim_mHMM(n_t = n_t, n = n, gen = list(m = m, n_dep = n_dep, q_emiss = q_emiss), gamma = gamma,                   emiss_distr = emiss_distr, var_gamma = .5, var_emiss = .5)  # Specify remaining required analysis input (for the example, we use simulation # input as starting values): n_dep <- 1 q_emiss <- 3  # Run the model on the simulated data: out_2st_sim <- mHMM(s_data = data1$obs,                  gen = list(m = m, n_dep = n_dep, q_emiss = q_emiss),                  start_val = list(gamma, emiss_distr),                  mcmc = list(J = 11, burn_in = 5)) #> Progress of the Bayesian mHMM algorithm:  #>    |                                                                               |                                                                      |   0%   |                                                                               |========                                                              |  11%   |                                                                               |================                                                      |  22%   |                                                                               |=======================                                               |  33%   |                                                                               |===============================                                       |  44%   |                                                                               |=======================================                               |  56%   |                                                                               |===============================================                       |  67%   |                                                                               |======================================================                |  78%   |                                                                               |==============================================================        |  89%   |                                                                               |======================================================================| 100% #> Total time elapsed (hh:mm:ss): 00:00:01  # obtaining the transition probabilities at the group and subject level est_gamma_group_sim <- obtain_gamma(out_2st_sim, level = \"group\")  # plot the obtained transition probabilities plot(est_gamma_group_sim, col = rep(c(\"green\", \"blue\"), each = m))   # }"},{"path":"/reference/prior_emiss_cat.html","id":null,"dir":"Reference","previous_headings":"","what":"Specifying informative hyper-prior on the categorical emission distribution(s)  of the multilevel hidden Markov model — prior_emiss_cat","title":"Specifying informative hyper-prior on the categorical emission distribution(s)  of the multilevel hidden Markov model — prior_emiss_cat","text":"prior_emiss_cat provides framework manually specify informative hyper-prior categorical emission distribution(s), creates object class mHMM_prior_emiss used function mHMM. Note hyper-prior distribution categorical emission probabilities intercepts (, subject level covariates used, regression coefficients) Multinomial logit model used accommodate multilevel framework data, instead probabilities directly. set hyper-prior distributions consists multivariate Normal hyper-prior distribution vector means (.e., intercepts regression coefficients), Inverse Wishart hyper-prior distribution covariance matrix.","code":""},{"path":"/reference/prior_emiss_cat.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Specifying informative hyper-prior on the categorical emission distribution(s)  of the multilevel hidden Markov model — prior_emiss_cat","text":"","code":"prior_emiss_cat(   gen,   emiss_mu0,   emiss_K0 = NULL,   emiss_nu = NULL,   emiss_V = NULL,   n_xx_emiss = NULL )"},{"path":"/reference/prior_emiss_cat.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Specifying informative hyper-prior on the categorical emission distribution(s)  of the multilevel hidden Markov model — prior_emiss_cat","text":"gen List containing following elements denoting general model properties: n_dep: numeric vector length 1 denoting number dependent variables q_emiss: numeric vector length n_dep denoting number observed categories categorical emission distribution dependent variables. emiss_mu0 list lists: emiss_mu0 contains n_dep lists, .e., one list dependent variable k. list k contains m matrices; one matrix set emission probabilities within state. matrices contain hypothesized hyper-prior mean values intercepts Multinomial logit model categorical emission probabilities. Hence, matrix consists one row (including covariates model) q_emiss[k] - 1 columns. covariates used, number rows matrix list equal 1 + n_xx (.e., first row corresponds hyper-prior mean values intercepts, subsequent rows correspond hyper-prior mean values regression coefficients connected covariates). emiss_K0 Optional list containing n_dep elements corresponding dependent variable k. element k numeric vector length 1 (covariates used) denoting number hypothetical prior subjects set hyper-prior mean intercepts specified emiss_mu0 based. covariates used: element numeric vector length 1 + n_xx denoting number hypothetical prior subjects set intercepts (first value) set regression coefficients (subsequent values) based. emiss_nu Optional list containing n_dep elements corresponding dependent variable k. element k numeric vector length 1 denoting degrees freedom hyper-prior Inverse Wishart distribution covariance Multinomial logit intercepts. emiss_V Optional list containing n_dep elements corresponding dependent variable k, element k matrix q_emiss[k] - 1 q_emiss[k] - 1 containing variance-covariance hyper-prior Inverse Wishart distribution covariance Multinomial logit intercepts. n_xx_emiss Optional numeric vector length n_dep denoting number (level 2) covariates used predict emission distribution dependent variables k. omitted, model assumes covariates used predict emission distribution(s).","code":""},{"path":"/reference/prior_emiss_cat.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Specifying informative hyper-prior on the categorical emission distribution(s)  of the multilevel hidden Markov model — prior_emiss_cat","text":"prior_emiss_cat returns object class mHMM_prior_emiss,   containing informative hyper-prior values categorical emission   distribution(s) multilevel hidden Markov model. object   specifically created formatted use function mHMM,   thoroughly checked correct input dimensions.   object contains following components:  gen list containing elements m, n_dep,   q_emiss, used checking equivalent general model properties   specified prior_emiss_cat mHMM. emiss_mu0 list lists containing hypothesized   hyper-prior mean values intercepts Multinomial logit model   categorical emission probabilities. emiss_K0 list containing n_dep elements denoting   number hypothetical prior subjects set hyper-prior mean   intercepts specified emiss_mu0 based. emiss_nu list containing n_dep elements denoting   degrees freedom hyper-prior Inverse Wishart distribution   covariance Multinomial logit intercepts. emiss_V list containing n_dep elements containing   variance-covariance hyper-prior Inverse Wishart distribution   covariance Multinomial logit intercepts. n_xx_emiss numeric vector denoting number (level 2)   covariates used predict emission distribution   dependent variables. covariates used, n_xx_emiss equals   NULL.","code":""},{"path":"/reference/prior_emiss_cat.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Specifying informative hyper-prior on the categorical emission distribution(s)  of the multilevel hidden Markov model — prior_emiss_cat","text":"Estimation mHMM proceeds within Bayesian context, hence hyper-prior distribution defined group level parameters. Default, non-informative priors used unless specified otherwise user. dependent variable, row categorical emission probability matrix (.e., probability observe category (columns) within states (rows)) set Multinomial logit intercepts, assumed follow multivariate normal distribution. Hence, hyper-prior distributions intercepts consists multivariate Normal hyper-prior distribution vector means, Inverse Wishart hyper-prior distribution covariance matrix. Note general model properties (number states m, number dependent variables n_dep, number observed categories dependent variable q_emiss) values hypothesized hyper-prior mean values Multinomial logit intercepts specified user, default values available hyper-prior distribution parameters. Given hyper-priors specified intercepts Multinomial logit model intercepts instead categorical emission probabilities directly, specifying hyper-prior can seem rather daunting. However, see function prob_to_int int_to_prob translating probabilities set Multinomial logit intercepts vice versa. Note emiss_K0, emiss_nu emiss_V assumed equal states.  hyper-prior values emiss_K0, emiss_nu emiss_V manually specified, default values follows. emiss_K0 set 1, emiss_nu set 3 + q_emiss[k] - 1, diagonal gamma_V (.e., variance) set 3 + q_emiss[k] - 1 -diagonal elements (.e., covariance) set 0. addition, manual values hyper-prior categorical emission distribution specified (, function prior_emiss_cat used), elements matrices contained emiss_mu0 set 0 function mHMM. Note case covariates specified, hyper-prior parameter values inverse Wishart distribution covariance matrix remain unchanged, estimates regression coefficients covariates fixed subjects.","code":""},{"path":[]},{"path":"/reference/prior_emiss_cat.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Specifying informative hyper-prior on the categorical emission distribution(s)  of the multilevel hidden Markov model — prior_emiss_cat","text":"","code":"###### Example using package example data, see ?nonverbal # specifying general model properties: m <- 3 n_dep <- 4 q_emiss <- c(3, 2, 3, 2)  # hypothesized mean emission probabilities prior_prob_emiss_cat <- list(matrix(c(0.10, 0.80, 0.10,                                       0.80, 0.10, 0.10,                                       0.40, 0.40, 0.20), byrow = TRUE,                                     nrow = m, ncol = q_emiss[1]), # vocalizing patient,                                     # prior belief: state 1 - much talking, state 2 -                                     # no talking, state 3 - mixed                              matrix(c(0.30, 0.70,                                       0.30, 0.70,                                       0.30, 0.70), byrow = TRUE, nrow = m,                                     ncol = q_emiss[2]), # looking patient                                     # prior belief: all 3 states show frequent looking                                     # behavior                              matrix(c(0.80, 0.10, 0.10,                                       0.10, 0.80, 0.10,                                       0.40, 0.40, 0.20), byrow = TRUE,                                     nrow = m, ncol = q_emiss[3]), # vocalizing therapist                                     # prior belief: state 1 - no talking, state 2 -                                     # frequent talking, state 3 - mixed                              matrix(c(0.30, 0.70,                                       0.30, 0.70,                                       0.30, 0.70), byrow = TRUE, nrow = m,                                     ncol = q_emiss[4])) # looking therapist                                     # prior belief: all 3 states show frequent looking                                     # behavior  # using the function prob_to_int to obtain intercept values for the above specified # categorical emission distributions prior_int_emiss <- sapply(prior_prob_emiss_cat, prob_to_int) emiss_mu0 <- rep(list(vector(mode = \"list\", length = m)), n_dep) for(k in 1:n_dep){   for(i in 1:m){   emiss_mu0[[k]][[i]] <- matrix(prior_int_emiss[[k]][i,], nrow = 1)   } }  emiss_K0 <- rep(list(c(1)), n_dep) emiss_nu <- list(c(5), c(4), c(5), c(4)) emiss_V <- list(diag(5, q_emiss[1] - 1),                 diag(4, q_emiss[2] - 1),                 diag(5, q_emiss[3] - 1),                 diag(4, q_emiss[4] - 1))  manual_prior_emiss <- prior_emiss_cat(gen = list(m = m, n_dep = n_dep, q_emiss = q_emiss),                                   emiss_mu0 = emiss_mu0, emiss_K0 = emiss_K0,                                   emiss_nu = emiss_nu, emiss_V = emiss_V)   # using the informative hyper-prior in a model  # specifying starting values start_TM <- diag(.7, m) start_TM[lower.tri(start_TM) | upper.tri(start_TM)] <- .1 start_EM <- list(matrix(c(0.05, 0.90, 0.05,                           0.90, 0.05, 0.05,                           0.55, 0.45, 0.05), byrow = TRUE,                         nrow = m, ncol = q_emiss[1]), # vocalizing patient                  matrix(c(0.1, 0.9,                           0.1, 0.9,                           0.1, 0.9), byrow = TRUE, nrow = m,                         ncol = q_emiss[2]), # looking patient                  matrix(c(0.90, 0.05, 0.05,                           0.05, 0.90, 0.05,                           0.55, 0.45, 0.05), byrow = TRUE,                         nrow = m, ncol = q_emiss[3]), # vocalizing therapist                  matrix(c(0.1, 0.9,                           0.1, 0.9,                           0.1, 0.9), byrow = TRUE, nrow = m,                         ncol = q_emiss[4])) # looking therapist  # Note that for reasons of running time, J is set at a ridiculous low value. # One would typically use a number of iterations J of at least 1000, # and a burn_in of 200. # \\donttest{ out_3st_infemiss <- mHMM(s_data = nonverbal,                     gen = list(m = m, n_dep = n_dep, q_emiss = q_emiss),                     start_val = c(list(start_TM), start_EM),                     emiss_hyp_prior = manual_prior_emiss,                     mcmc = list(J = 11, burn_in = 5)) #> Progress of the Bayesian mHMM algorithm:  #>    |                                                                               |                                                                      |   0%   |                                                                               |========                                                              |  11%   |                                                                               |================                                                      |  22%   |                                                                               |=======================                                               |  33%   |                                                                               |===============================                                       |  44%   |                                                                               |=======================================                               |  56%   |                                                                               |===============================================                       |  67%   |                                                                               |======================================================                |  78%   |                                                                               |==============================================================        |  89%   |                                                                               |======================================================================| 100% #> Total time elapsed (hh:mm:ss): 00:00:06 # } # \\dontshow{ # executable in < 5 sec together with the examples above out_3st_infemiss <- mHMM(s_data = nonverbal,                     gen = list(m = m, n_dep = n_dep, q_emiss = q_emiss),                     start_val = c(list(start_TM), start_EM),                     emiss_hyp_prior = manual_prior_emiss,                     mcmc = list(J = 6, burn_in = 3)) #> Progress of the Bayesian mHMM algorithm:  #>    |                                                                               |                                                                      |   0%   |                                                                               |==================                                                    |  25%   |                                                                               |===================================                                   |  50%   |                                                                               |====================================================                  |  75%   |                                                                               |======================================================================| 100% #> Total time elapsed (hh:mm:ss): 00:00:03 # }  out_3st_infemiss #> Number of subjects: 10  #>  #> 6 iterations used in the MCMC algorithm with a burn in of 3  #> Average Log likelihood over all subjects: -1552.213  #> Average AIC over all subjects: 3152.426  #>  #> Number of states used: 3  #>  #> Number of dependent variables used: 4  #>  summary(out_3st_infemiss) #> State transition probability matrix  #>  (at the group level):  #>   #>              To state 1 To state 2 To state 3 #> From state 1      0.901      0.028      0.071 #> From state 2      0.018      0.928      0.054 #> From state 3      0.190      0.175      0.635 #>  #>   #> Emission distribution for each of the dependent variables  #>  (at the group level):  #>   #> $p_vocalizing #>         Category 1 Category 2 Category 3 #> State 1      0.008      0.970      0.022 #> State 2      0.824      0.032      0.143 #> State 3      0.255      0.670      0.076 #>  #> $p_looking #>         Category 1 Category 2 #> State 1      0.231      0.769 #> State 2      0.065      0.935 #> State 3      0.187      0.813 #>  #> $t_vocalizing #>         Category 1 Category 2 Category 3 #> State 1      0.881      0.034      0.085 #> State 2      0.013      0.975      0.012 #> State 3      0.271      0.685      0.044 #>  #> $t_looking #>         Category 1 Category 2 #> State 1      0.035      0.965 #> State 2      0.204      0.796 #> State 3      0.128      0.872 #>  #>"},{"path":"/reference/prior_gamma.html","id":null,"dir":"Reference","previous_headings":"","what":"Specifying informative hyper-prior on the transition probability matrix gamma of the multilevel hidden Markov model — prior_gamma","title":"Specifying informative hyper-prior on the transition probability matrix gamma of the multilevel hidden Markov model — prior_gamma","text":"prior_gamma provides framework manually specify informative hyper-prior transition probability matrix gamma, creates object class mHMM_prior_gamma used function mHMM. Note hyper-prior distribution transition probabilities intercepts (, subject level covariates used, regression coefficients) Multinomial logit model used accommodate multilevel framework data, instead probabilities directly. set hyper-prior distributions consists multivariate Normal hyper-prior distribution vector means (.e., intercepts regression coefficients), Inverse Wishart hyper-prior distribution covariance matrix.","code":""},{"path":"/reference/prior_gamma.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Specifying informative hyper-prior on the transition probability matrix gamma of the multilevel hidden Markov model — prior_gamma","text":"","code":"prior_gamma(   m,   gamma_mu0,   gamma_K0 = NULL,   gamma_nu = NULL,   gamma_V = NULL,   n_xx_gamma = NULL )"},{"path":"/reference/prior_gamma.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Specifying informative hyper-prior on the transition probability matrix gamma of the multilevel hidden Markov model — prior_gamma","text":"m Numeric vector length 1 denoting number hidden states. gamma_mu0 list containing m matrices; one matrix row transition probability matrix gamma. matrix contains hypothesized hyper-prior mean values intercepts Multinomial logit model transition probabilities gamma. Hence, matrix consists one row (including covariates model) m - 1 columns. covariates used, number rows matrix list equal 1 + n_xx_gamma (.e., first row corresponds hyper-prior mean values intercepts, subsequent rows correspond hyper-prior mean values regression coefficients connected covariates). gamma_K0 Optional numeric vector length 1 (covariates used) denoting number hypothetical prior subjects set hyper-prior mean intercepts specified gamma_mu0 based. covariates used: Numeric vector length 1 + n_xx_gamma denoting number hypothetical prior subjects set intercepts (first value) set regression coefficients (subsequent values) based. gamma_nu Optional numeric vector length 1 denoting degrees freedom hyper-prior Inverse Wishart distribution covariance Multinomial logit intercepts. gamma_V Optional matrix m - 1 m - 1 containing variance-covariance matrix hyper-prior Inverse Wishart distribution covariance Multinomial logit intercepts. n_xx_gamma Optional numeric vector length 1 denoting number (level 2) covariates used predict transition probability matrix gamma. omitted, model assumes covariates used predict gamma.","code":""},{"path":"/reference/prior_gamma.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Specifying informative hyper-prior on the transition probability matrix gamma of the multilevel hidden Markov model — prior_gamma","text":"prior_gamma returns object class mHMM_prior_gamma,   containing informative hyper-prior values transition probability   matrix gamma multilevel hidden Markov model. object   specifically created formatted use function mHMM,   thoroughly checked correct input dimensions.   object contains following components:  m Numeric vector denoting number hidden states, used   checking equivalent general model properties specified   prior_gamma mHMM. gamma_mu0 list containing hypothesized hyper-prior mean   values intercepts Multinomial logit model transition   probability matrix gamma. gamma_K0 numeric vector denoting number hypothetical   prior subjects set hyper-prior mean intercepts specified   gamma_mu0 based. gamma_nu numeric vector denoting degrees freedom   hyper-prior Inverse Wishart distribution covariance   Multinomial logit intercepts. gamma_V matrix containing variance-covariance   hyper-prior Inverse Wishart distribution covariance   Multinomial logit intercepts. n_xx_gamma numeric vector denoting number (level 2)   covariates used predict transition probability matrix gamma.   covariates used, n_xx_gamma equals NULL.","code":""},{"path":"/reference/prior_gamma.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Specifying informative hyper-prior on the transition probability matrix gamma of the multilevel hidden Markov model — prior_gamma","text":"Estimation mHMM proceeds within Bayesian context, hence hyper-prior distribution defined group level parameters. Default, non-informative priors used unless specified otherwise user. row transition probability matrix set Multinomial logit intercepts, assumed follow multivariate normal distribution. Hence, hyper-prior distributions intercepts consists multivariate Normal hyper-prior distribution vector means, Inverse Wishart hyper-prior distribution covariance matrix. Note number states m values hypothesized hyper-prior mean values Multinomial logit intercepts specified user, default values available hyper-prior distribution parameters. Given hyper-priors specified intercepts Multinomial logit model intercepts instead probabilities transition probability matrix gamma directly, specifying hyper-prior can seem rather daunting. However, see function prob_to_int int_to_prob translating probabilities set Multinomial logit intercepts vice versa. Note gamma_K0, gamma_nu gamma_V assumed equal states. hyper-prior values gamma_K0, gamma_nu gamma_V manually specified, default values follows. gamma_K0 set 1, gamma_nu set 3 + m - 1, diagonal gamma_V (.e., variance) set 3 + m - 1 -diagonal elements (.e., covariance) set 0. addition, manual values hyper-prior gamma specified (, function prior_gamma used), elements matrices contained gamma_mu0 set 0 function mHMM. Note case covariates specified, hyper-prior parameter values inverse Wishart distribution covariance matrix remain unchanged, estimates regression coefficients covariates fixed subjects.","code":""},{"path":[]},{"path":"/reference/prior_gamma.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Specifying informative hyper-prior on the transition probability matrix gamma of the multilevel hidden Markov model — prior_gamma","text":"","code":"###### Example using package example data, see ?nonverbal # specifying general model properties: m <- 3 # representing a prior belief that switching to state 3 does not occur often and # state 3 has a relative short duration prior_prob_gamma <- matrix(c(0.70, 0.25, 0.05,                              0.25, 0.70, 0.05,                              0.30, 0.30, 0.40), nrow = m, ncol = m, byrow = TRUE)  # using the function prob_to_int to obtain intercept values for the above specified # transition probability matrix gamma prior_int_gamma <- prob_to_int(prior_prob_gamma) gamma_mu0 <- list(matrix(prior_int_gamma[1,], nrow = 1, ncol = m-1),                   matrix(prior_int_gamma[2,], nrow = 1, ncol = m-1),                   matrix(prior_int_gamma[3,], nrow = 1, ncol = m-1))  gamma_K0 <- 1 gamma_nu <- 5 gamma_V <- diag(5, m - 1)  manual_prior_gamma <- prior_gamma(m = m, gamma_mu0 = gamma_mu0,                                   gamma_K0 = gamma_K0, gamma_nu = gamma_nu,                                   gamma_V = gamma_V)   # using the informative hyper-prior in a model n_dep <- 4 q_emiss <- c(3, 2, 3, 2)  # specifying starting values start_TM <- diag(.7, m) start_TM[lower.tri(start_TM) | upper.tri(start_TM)] <- .1 start_EM <- list(matrix(c(0.05, 0.90, 0.05,                           0.90, 0.05, 0.05,                           0.55, 0.45, 0.05), byrow = TRUE,                         nrow = m, ncol = q_emiss[1]), # vocalizing patient                  matrix(c(0.1, 0.9,                           0.1, 0.9,                           0.1, 0.9), byrow = TRUE, nrow = m,                         ncol = q_emiss[2]), # looking patient                  matrix(c(0.90, 0.05, 0.05,                           0.05, 0.90, 0.05,                           0.55, 0.45, 0.05), byrow = TRUE,                         nrow = m, ncol = q_emiss[3]), # vocalizing therapist                  matrix(c(0.1, 0.9,                           0.1, 0.9,                           0.1, 0.9), byrow = TRUE, nrow = m,                         ncol = q_emiss[4])) # looking therapist  # Note that for reasons of running time, J is set at a ridiculous low value. # One would typically use a number of iterations J of at least 1000, # and a burn_in of 200. # \\donttest{ out_3st_infgamma <- mHMM(s_data = nonverbal,                     gen = list(m = m, n_dep = n_dep, q_emiss = q_emiss),                     start_val = c(list(start_TM), start_EM),                     gamma_hyp_prior = manual_prior_gamma,                     mcmc = list(J = 11, burn_in = 5)) #> Progress of the Bayesian mHMM algorithm:  #>    |                                                                               |                                                                      |   0%   |                                                                               |========                                                              |  11%   |                                                                               |================                                                      |  22%   |                                                                               |=======================                                               |  33%   |                                                                               |===============================                                       |  44%   |                                                                               |=======================================                               |  56%   |                                                                               |===============================================                       |  67%   |                                                                               |======================================================                |  78%   |                                                                               |==============================================================        |  89%   |                                                                               |======================================================================| 100% #> Total time elapsed (hh:mm:ss): 00:00:06 # } # \\dontshow{ out_3st_infgamma <- mHMM(s_data = nonverbal,                     gen = list(m = m, n_dep = n_dep, q_emiss = q_emiss),                     start_val = c(list(start_TM), start_EM),                     gamma_hyp_prior = manual_prior_gamma,                     mcmc = list(J = 6, burn_in = 3)) #> Progress of the Bayesian mHMM algorithm:  #>    |                                                                               |                                                                      |   0%   |                                                                               |==================                                                    |  25%   |                                                                               |===================================                                   |  50%   |                                                                               |====================================================                  |  75%   |                                                                               |======================================================================| 100% #> Total time elapsed (hh:mm:ss): 00:00:03 # }  out_3st_infgamma #> Number of subjects: 10  #>  #> 6 iterations used in the MCMC algorithm with a burn in of 3  #> Average Log likelihood over all subjects: -1550.2  #> Average AIC over all subjects: 3148.401  #>  #> Number of states used: 3  #>  #> Number of dependent variables used: 4  #>  summary(out_3st_infgamma) #> State transition probability matrix  #>  (at the group level):  #>   #>              To state 1 To state 2 To state 3 #> From state 1      0.908      0.033      0.058 #> From state 2      0.024      0.923      0.052 #> From state 3      0.184      0.184      0.632 #>  #>   #> Emission distribution for each of the dependent variables  #>  (at the group level):  #>   #> $p_vocalizing #>         Category 1 Category 2 Category 3 #> State 1      0.012      0.964      0.024 #> State 2      0.819      0.042      0.140 #> State 3      0.249      0.631      0.119 #>  #> $p_looking #>         Category 1 Category 2 #> State 1      0.247      0.753 #> State 2      0.139      0.861 #> State 3      0.217      0.783 #>  #> $t_vocalizing #>         Category 1 Category 2 Category 3 #> State 1      0.847      0.042      0.112 #> State 2      0.028      0.912      0.060 #> State 3      0.298      0.642      0.060 #>  #> $t_looking #>         Category 1 Category 2 #> State 1      0.098      0.902 #> State 2      0.318      0.682 #> State 3      0.115      0.885 #>  #>"},{"path":"/reference/prob_to_int.html","id":null,"dir":"Reference","previous_headings":"","what":"Transforming a set of probabilities to Multinomial logit regression intercepts — prob_to_int","title":"Transforming a set of probabilities to Multinomial logit regression intercepts — prob_to_int","text":"prob_to_int transforms set state transition categorical emission observation probabilities corresponding Multinomial logit regression intercepts. Note first category assumed reference category, hence intercept returned first state category.","code":""},{"path":"/reference/prob_to_int.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Transforming a set of probabilities to Multinomial logit regression intercepts — prob_to_int","text":"","code":"prob_to_int(prob_matrix)"},{"path":"/reference/prob_to_int.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Transforming a set of probabilities to Multinomial logit regression intercepts — prob_to_int","text":"prob_matrix matrix number states categories columns number rows determined user, rows summing one. obtaining set Multinomial logit regression intercepts complete transition probability matrix gamma categorical emission distribution matrix, number rows equals number states m.","code":""},{"path":"/reference/prob_to_int.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Transforming a set of probabilities to Multinomial logit regression intercepts — prob_to_int","text":"prob_to_int returns matrix containing Multinomial logit   regression intercepts, number columns equal (number   states categories - 1) number rows equal number rows   specified input matrix. first state / category assumed   reference category, hence intercept returned first   category.","code":""},{"path":"/reference/prob_to_int.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Transforming a set of probabilities to Multinomial logit regression intercepts — prob_to_int","text":"Designed ease specification informative hyper-prior values mean intercepts transition probability matrix gamma categorical emission distribution(s) multilevel hidden Markov model functions prior_gamma prior_emiss_cat. check performed correct specifications dimensions.","code":""},{"path":[]},{"path":"/reference/prob_to_int.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Transforming a set of probabilities to Multinomial logit regression intercepts — prob_to_int","text":"","code":"# example for transition probability matrix gamma with 3 states m <- 3 gamma_prob <- matrix(c(0.6, 0.2, 0.2,                        0.1, 0.8, 0.1,                        0.1, 0.1, 0.8), ncol = m, nrow = m, byrow = TRUE) gamma_int <- prob_to_int(gamma_prob) gamma_int #>         [,1]    [,2] #> [1,] -1.0986 -1.0986 #> [2,]  2.0794  0.0000 #> [3,]  0.0000  2.0794"},{"path":"/reference/sim_mHMM.html","id":null,"dir":"Reference","previous_headings":"","what":"Simulate data using a multilevel hidden Markov model — sim_mHMM","title":"Simulate data using a multilevel hidden Markov model — sim_mHMM","text":"sim_mHMM simulates data multiple subjects, data categorical observations follow hidden Markov model (HMM) multilevel structure. multilevel structure implies subject allowed set parameters, parameters subject level (level 1) tied together population distribution level 2 corresponding parameters. shape population distribution parameters normal distribution. addition (natural /unexplained) heterogeneity subjects, subjects parameters can also depend covariate.","code":""},{"path":"/reference/sim_mHMM.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Simulate data using a multilevel hidden Markov model — sim_mHMM","text":"","code":"sim_mHMM(   n_t,   n,   gen,   gamma,   emiss_distr,   start_state = NULL,   xx_vec = NULL,   beta = NULL,   var_gamma = 0.1,   var_emiss = NULL,   return_ind_par = FALSE,   m,   n_dep,   q_emiss )"},{"path":"/reference/sim_mHMM.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Simulate data using a multilevel hidden Markov model — sim_mHMM","text":"n_t Numeric vector length 1 denoting length observed sequence simulated subject. simulate subject specific transition probability matrices gamma emission distributions (data), set t 0. n Numeric vector length 1 denoting number subjects data simulated. gen List containing following elements denoting general model properties: n_dep: numeric vector length 1 denoting number dependent variables q_emiss: numeric vector length n_dep denoting number observed categories categorical emission distribution dependent variables. gamma matrix m rows m columns containing average population transition probability matrix used simulating data. , probability switch hidden state (row ) hidden state j (column  j). emiss_distr list n_dep elements containing average population emission distribution(s) observations given hidden states dependent variables. element matrix m rows q_emiss[k] columns k n_dep emission distribution(s). , probability observing category q (column q) state (row ). start_state Optional numeric vector length 1 denoting state simulated state sequence start. left unspecified, simulated state time point 1 sampled initial state distribution (derived transition probability matrix gamma). xx_vec List 1 + n_dep vectors containing covariate(s) predict transition probability matrix gamma /(specific) emission distribution(s) emiss_distr using regression parameters specified beta (see ). first element list xx_vec used predict transition matrix. Subsequent elements list used predict emission distribution () dependent variable(s). means covariate used predict gamma emiss_distr can either covariate, different covariates, covariate certain elements none . point, possible use one covariate gamma emiss_distr. elements list, number observations vectors  equal number subjects simulated n. xx_vec omitted completely, xx_vec defaults NULL, resembling covariates . Specific elements list can also left empty (.e., set NULL) signify either transition probability matrix (one ) emission distribution(s) predicted covariates. beta List 1 + n_dep matrices containing regression   parameters predict gamma /emiss_distr combination   xx_vec using (Multinomial logistic) regression. first   matrix used predict transition probability matrix gamma.   subsequent matrices used predict emission distribution(s)   emiss_distr dependent variable(s). gamma   categorical emission distributions, one regression parameter specified   element gamma emiss_distr, following   exception. first element row gamma /  emiss_distr used reference category Multinomial   logistic regression. , regression parameters can specified   parameters. Hence, first element list beta   predict gamma consist matrix number rows equal   m number columns equal m - 1. categorical   emission distributions, subsequent elements list beta   predict emiss_distr consist matrix number rows   equal m number columns equal q_emiss[k] - 1   k n_dep emission distribution(s). See   details information. continuous emission distributions,   subsequent elements list beta consist matrix   number rows equal m 1 column. Note beta specified, xx_vec specified   well. beta omitted completely, beta defaults NULL,   resembling prediction gamma emiss_distr using   covariates. One elements list can also left empty   (.e., set NULL) signify either transition   probability matrix specific emission distribution predicted   covariates. var_gamma numeric vector length 1 denoting amount variance subjects transition probability matrix. Note value corresponds variance parameters Multinomial distribution (.e., intercepts regression equation Multinomial distribution used sample transition probability matrix), see details . addition, one variance value can specified complete transition probability matrix, hence variance assumed fixed across components. default equals 0.1, corresponds little variation subjects. one wants simulate data exactly HMM subjects, var_gamma set 0. Note data 1 subject simulated (.e., n = 1), var_gamma set 0. var_emiss numeric vector length n_dep denoting amount variance subjects emission distribution(s). Note value corresponds variance parameters Multinomial distribution (.e., intercepts regression equation Multinomial distribution used sample components emission distribution), see details . one variance value can specified emission distribution, hence variance assumed fixed across states across categories within state. default equals 0.1, corresponds little variation subjects given categorical observations. one wants simulate data exactly HMM subjects, var_emiss set vector 0's. Note data 1 subject simulated (.e., n = 1), var_emiss set vector 0's. return_ind_par logical scalar. subject specific transition probability matrix gamma emission probability matrix emiss_distr returned function (return_ind_par = TRUE) (return_ind_par = FALSE). default equals return_ind_par = FALSE. m argument m deprecated; please specify using input parameter gen. n_dep argument n_dep deprecated; please specify using input parameter n_dep. q_emiss argument q_emiss deprecated; please specify using input parameter q_emiss.","code":""},{"path":"/reference/sim_mHMM.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Simulate data using a multilevel hidden Markov model — sim_mHMM","text":"following components returned function sim_mHMM: states matrix containing simulated hidden state   sequences, one row per hidden state per subject. first column   indicates subject id number. second column contains simulated   hidden state sequence, consecutively subjects. Hence, id number   repeated rows (number repeats equal length   simulated hidden state sequence T subject). obs matrix containing simulated observed outputs,   one row per simulated observation per subject. first column indicates   subject id number. second column contains simulated observation   sequence, consecutively subjects. Hence, id number repeated   rows (number repeats equal length simulated   observation sequence T subject). gamma list containing n elements simulated   subject specific transition probability matrices gamma.   returned return_ind_par set TRUE. emiss_distr list containing n elements   simulated subject specific emission probability matrices   emiss_distr. returned return_ind_par set   TRUE.","code":""},{"path":"/reference/sim_mHMM.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Simulate data using a multilevel hidden Markov model — sim_mHMM","text":"simulating data, multilevel structure means parameters subject sampled population level distribution corresponding parameter. user specifies population distribution parameter: average population transition probability matrix variance, average population emission distribution variance. now, variance mean population parameters assumed fixed components transition probability matrix components emission distribution. One can simulate multivariate data. , hidden states depend 1 observed variable simultaneously. distributions multiple dependent variables multivariate data assumed independent. Note: subject specific) initial state distributions (.e., probability states first time point) needed simulate data obtained stationary distributions subject specific transition probability matrices gamma. beta: first element row gamma used reference category Multinomial logistic regression, first matrix list beta used predict transition probability matrix gamma number rows equal m number columns equal m - 1. first element first row corresponds probability switching state one state two. second element first row corresponds probability switching state one state three, . last element first row corresponds probability switching state one last state. principle holds second matrix list beta used predict categorical emission distribution(s) emiss_distr: first element first row corresponds probability observing category two state one. second element first row corresponds probability observing category three state one, . last element first row corresponds probability observing last category state one.","code":""},{"path":[]},{"path":"/reference/sim_mHMM.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Simulate data using a multilevel hidden Markov model — sim_mHMM","text":"","code":"# simulating data for 10 subjects with each 100 observations n_t     <- 100 n       <- 10 m       <- 3 n_dep   <- 1 q_emiss <- 4 gamma   <- matrix(c(0.8, 0.1, 0.1,                     0.2, 0.7, 0.1,                     0.2, 0.2, 0.6), ncol = m, byrow = TRUE) emiss_distr <- list(matrix(c(0.5, 0.5, 0.0, 0.0,                              0.1, 0.1, 0.8, 0.0,                              0.0, 0.0, 0.1, 0.9), nrow = m, ncol = q_emiss, byrow = TRUE)) data1 <- sim_mHMM(n_t = n_t, n = n, gen = list(m = m, n_dep = n_dep, q_emiss = q_emiss),                   gamma = gamma, emiss_distr = emiss_distr, var_gamma = 1, var_emiss = 1) head(data1$obs) #>      subj observation 1 #> [1,]    1             3 #> [2,]    1             3 #> [3,]    1             3 #> [4,]    1             2 #> [5,]    1             3 #> [6,]    1             3 head(data1$states) #>      subj state #> [1,]    1     2 #> [2,]    1     2 #> [3,]    1     2 #> [4,]    1     2 #> [5,]    1     2 #> [6,]    1     2  # including a covariate to predict (only) the transition probability matrix gamma beta      <- rep(list(NULL), 2) beta[[1]] <- matrix(c(0.5, 1.0,                      -0.5, 0.5,                       0.0, 1.0), byrow = TRUE, ncol = 2) xx_vec      <- rep(list(NULL),2) xx_vec[[1]] <-  c(rep(0,5), rep(1,5)) data2 <- sim_mHMM(n_t = n_t, n = n, gen = list(m = m, n_dep = n_dep, q_emiss = q_emiss),                   gamma = gamma, emiss_distr = emiss_distr, beta = beta, xx_vec = xx_vec,                   var_gamma = 1, var_emiss = 1)   # simulating subject specific transition probability matrices and emission distributions only n_t <- 0 n <- 5 m <- 3 n_dep   <- 1 q_emiss <- 4 gamma <- matrix(c(0.8, 0.1, 0.1,                   0.2, 0.7, 0.1,                   0.2, 0.2, 0.6), ncol = m, byrow = TRUE) emiss_distr <- list(matrix(c(0.5, 0.5, 0.0, 0.0,                              0.1, 0.1, 0.8, 0.0,                              0.0, 0.0, 0.1, 0.9), nrow = m, ncol = q_emiss, byrow = TRUE)) data3 <- sim_mHMM(n_t = n_t, n = n, gen = list(m = m, n_dep = n_dep, q_emiss = q_emiss),                   gamma = gamma, emiss_distr = emiss_distr, var_gamma = 1, var_emiss = 1) data3 #> $subject_gamma #> $subject_gamma[[1]] #>        [,1]   [,2]   [,3] #> [1,] 0.7513 0.0297 0.2190 #> [2,] 0.2518 0.6170 0.1312 #> [3,] 0.6966 0.1836 0.1198 #>  #> $subject_gamma[[2]] #>        [,1]   [,2]   [,3] #> [1,] 0.8187 0.1545 0.0268 #> [2,] 0.0938 0.7987 0.1075 #> [3,] 0.6266 0.1174 0.2560 #>  #> $subject_gamma[[3]] #>        [,1]   [,2]   [,3] #> [1,] 0.7645 0.1452 0.0904 #> [2,] 0.1755 0.7608 0.0637 #> [3,] 0.0760 0.1211 0.8030 #>  #> $subject_gamma[[4]] #>        [,1]   [,2]   [,3] #> [1,] 0.7185 0.1426 0.1389 #> [2,] 0.2835 0.3946 0.3219 #> [3,] 0.2632 0.3186 0.4183 #>  #> $subject_gamma[[5]] #>        [,1]   [,2]   [,3] #> [1,] 0.7383 0.1222 0.1395 #> [2,] 0.1487 0.8352 0.0161 #> [3,] 0.2248 0.0442 0.7310 #>  #>  #> $subject_emiss #> $subject_emiss[[1]] #> $subject_emiss[[1]][[1]] #>        [,1]   [,2]   [,3]   [,4] #> [1,] 0.2305 0.7695 0.0000 0.0000 #> [2,] 0.0328 0.0331 0.9342 0.0000 #> [3,] 0.0000 0.0000 0.0118 0.9882 #>  #>  #> $subject_emiss[[2]] #> $subject_emiss[[2]][[1]] #>        [,1]   [,2]   [,3]   [,4] #> [1,] 0.2427 0.7572 0.0000 0.0000 #> [2,] 0.0591 0.1862 0.7547 0.0000 #> [3,] 0.0000 0.0000 0.0502 0.9498 #>  #>  #> $subject_emiss[[3]] #> $subject_emiss[[3]][[1]] #>        [,1]   [,2]   [,3]   [,4] #> [1,] 0.2766 0.7234 0.0000 0.0000 #> [2,] 0.0274 0.0655 0.9071 0.0000 #> [3,] 0.0000 0.0000 0.0249 0.9751 #>  #>  #> $subject_emiss[[4]] #> $subject_emiss[[4]][[1]] #>        [,1]   [,2]   [,3]   [,4] #> [1,] 0.3764 0.6236 0.0000 0.0000 #> [2,] 0.0764 0.3289 0.5946 0.0001 #> [3,] 0.0000 0.0000 0.0301 0.9699 #>  #>  #> $subject_emiss[[5]] #> $subject_emiss[[5]][[1]] #>        [,1]   [,2]   [,3]   [,4] #> [1,] 0.4083 0.5917 0.0000 0.0000 #> [2,] 0.1706 0.1321 0.6973 0.0000 #> [3,] 0.0000 0.0000 0.0198 0.9802 #>  #>  #>   data4 <- sim_mHMM(n_t = n_t, n = n, gen = list(m = m, n_dep = n_dep, q_emiss = q_emiss),                   gamma = gamma, emiss_distr = emiss_distr, var_gamma = .5, var_emiss = .5) data4 #> $subject_gamma #> $subject_gamma[[1]] #>        [,1]   [,2]   [,3] #> [1,] 0.7159 0.1875 0.0966 #> [2,] 0.2497 0.5836 0.1667 #> [3,] 0.4758 0.2752 0.2490 #>  #> $subject_gamma[[2]] #>        [,1]   [,2]   [,3] #> [1,] 0.7986 0.0645 0.1369 #> [2,] 0.1547 0.7214 0.1238 #> [3,] 0.1228 0.4138 0.4634 #>  #> $subject_gamma[[3]] #>        [,1]   [,2]   [,3] #> [1,] 0.8011 0.0787 0.1202 #> [2,] 0.0948 0.8002 0.1050 #> [3,] 0.1332 0.3374 0.5295 #>  #> $subject_gamma[[4]] #>        [,1]   [,2]   [,3] #> [1,] 0.7655 0.0965 0.1380 #> [2,] 0.1206 0.7618 0.1175 #> [3,] 0.2327 0.0819 0.6854 #>  #> $subject_gamma[[5]] #>        [,1]   [,2]   [,3] #> [1,] 0.7680 0.0778 0.1542 #> [2,] 0.1382 0.6664 0.1954 #> [3,] 0.1704 0.1522 0.6775 #>  #>  #> $subject_emiss #> $subject_emiss[[1]] #> $subject_emiss[[1]][[1]] #>        [,1]   [,2]   [,3]   [,4] #> [1,] 0.2305 0.7695 0.0000 0.0000 #> [2,] 0.0826 0.0415 0.8759 0.0000 #> [3,] 0.0000 0.0000 0.0305 0.9695 #>  #>  #> $subject_emiss[[2]] #> $subject_emiss[[2]][[1]] #>        [,1]   [,2]   [,3]   [,4] #> [1,] 0.5337 0.4663 0.0000 0.0000 #> [2,] 0.1134 0.4399 0.4467 0.0000 #> [3,] 0.0000 0.0000 0.1681 0.8319 #>  #>  #> $subject_emiss[[3]] #> $subject_emiss[[3]][[1]] #>        [,1]   [,2]   [,3]   [,4] #> [1,] 0.4982 0.5018 0.0000 0.0000 #> [2,] 0.1121 0.3064 0.5814 0.0000 #> [3,] 0.0000 0.0000 0.1473 0.8527 #>  #>  #> $subject_emiss[[4]] #> $subject_emiss[[4]][[1]] #>        [,1]   [,2]   [,3]   [,4] #> [1,] 0.5899 0.4100 0.0000 0.0000 #> [2,] 0.0869 0.0853 0.8277 0.0000 #> [3,] 0.0000 0.0000 0.0334 0.9665 #>  #>  #> $subject_emiss[[5]] #> $subject_emiss[[5]][[1]] #>        [,1]   [,2]   [,3]   [,4] #> [1,] 0.5423 0.4577 0.0000 0.0000 #> [2,] 0.1114 0.0376 0.8510 0.0000 #> [3,] 0.0000 0.0000 0.0597 0.9403 #>  #>  #>"},{"path":"/reference/vit_mHMM.html","id":null,"dir":"Reference","previous_headings":"","what":"Obtain hidden state sequence for each subject using the Viterbi\r\nalgorithm — vit_mHMM","title":"Obtain hidden state sequence for each subject using the Viterbi\r\nalgorithm — vit_mHMM","text":"vit_mHMM obtains likely state sequence (subject) object class mHMM (generated function mHMM()), using (extended version ) Viterbi algorithm. also known global decoding.","code":""},{"path":"/reference/vit_mHMM.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Obtain hidden state sequence for each subject using the Viterbi\r\nalgorithm — vit_mHMM","text":"","code":"vit_mHMM(object, s_data, burn_in = NULL)"},{"path":"/reference/vit_mHMM.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Obtain hidden state sequence for each subject using the Viterbi\r\nalgorithm — vit_mHMM","text":"object object class mHMM, generated function mHMM. s_data matrix containing observations modeled, rows represent observations time. s_data, first column indicates subject id number. Hence, id number repeated rows equal number observations subject. subsequent columns contain dependent variable(s). Note dependent variables numeric, .e., (set ) factor variable(s). total number rows equal sum number observations subject, number columns equal number dependent variables (n_dep) + 1. number observations can vary subjects. burn_in number iterations discarded MCMC algorithm inferring transition probability matrix gamma emission distribution () dependent variable(s) subject s_data. omitted, defaults NULL burn_in specified function mHMM used.","code":""},{"path":"/reference/vit_mHMM.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Obtain hidden state sequence for each subject using the Viterbi\r\nalgorithm — vit_mHMM","text":"function vit_mHMM returns matrix containing   likely state point time. column represents subject,   row represents point time. sequence lengths differ   subjects, states none existing time points subjects filled  NA.","code":""},{"path":"/reference/vit_mHMM.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Obtain hidden state sequence for each subject using the Viterbi\r\nalgorithm — vit_mHMM","text":"Note local decoding also possible, inferring frequent state point time subject sampled state path iteration MCMC algorithm. information contained output object return_path function mHMM().","code":""},{"path":"/reference/vit_mHMM.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Obtain hidden state sequence for each subject using the Viterbi\r\nalgorithm — vit_mHMM","text":"Viterbi (1967). “Error bounds convolutional codes asymptotically optimum decoding algorithm.” IEEE transactions Information Theory, 13(2), 260--269. Rabiner LR (1989). “tutorial hidden Markov models selected applications speech recognition.” Proceedings IEEE, 77(2), 257--286.","code":""},{"path":[]},{"path":"/reference/vit_mHMM.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Obtain hidden state sequence for each subject using the Viterbi\r\nalgorithm — vit_mHMM","text":"","code":"###### Example on package example data, see ?nonverbal # First fit the multilevel HMM on the example data # \\donttest{ # specifying general model properties: m <- 2 n_dep <- 4 q_emiss <- c(3, 2, 3, 2)  # specifying starting values start_TM <- diag(.8, m) start_TM[lower.tri(start_TM) | upper.tri(start_TM)] <- .2 start_EM <- list(matrix(c(0.05, 0.90, 0.05, 0.90, 0.05, 0.05), byrow = TRUE,                         nrow = m, ncol = q_emiss[1]), # vocalizing patient                  matrix(c(0.1, 0.9, 0.1, 0.9), byrow = TRUE, nrow = m,                         ncol = q_emiss[2]), # looking patient                  matrix(c(0.90, 0.05, 0.05, 0.05, 0.90, 0.05), byrow = TRUE,                         nrow = m, ncol = q_emiss[3]), # vocalizing therapist                  matrix(c(0.1, 0.9, 0.1, 0.9), byrow = TRUE, nrow = m,                         ncol = q_emiss[4])) # looking therapist  # Fit the multilevel HMM model: # Note that for reasons of running time, J is set at a ridiculous low value. # One would typically use a number of iterations J of at least 1000, # and a burn_in of 200. out_2st <- mHMM(s_data = nonverbal, gen = list(m = m, n_dep = n_dep,                 q_emiss = q_emiss), start_val = c(list(start_TM), start_EM),                 mcmc = list(J = 3, burn_in = 1)) #> Progress of the Bayesian mHMM algorithm:  #>    |                                                                               |                                                                      |   0%   |                                                                               |======================================================================| 100% #> Total time elapsed (hh:mm:ss): 00:00:00  ###### obtain the most likely state sequence with the Viterbi algorithm states <- vit_mHMM(s_data = nonverbal, object = out_2st) # } ###### Example on simulated data # Simulate data for 10 subjects with each 100 observations: n_t <- 100 n <- 10 m <- 2 n_dep <- 1 q_emiss <- 3 gamma <- matrix(c(0.8, 0.2,                   0.3, 0.7), ncol = m, byrow = TRUE) emiss_distr <- list(matrix(c(0.5, 0.5, 0.0,                         0.1, 0.1, 0.8), nrow = m, ncol = q_emiss, byrow = TRUE)) data1 <- sim_mHMM(n_t = n_t, n = n, gen = list(m = m, n_dep = n_dep, q_emiss = q_emiss),                   gamma = gamma, emiss_distr = emiss_distr, var_gamma = .5, var_emiss = .5)  # Fit the model on the simulated data: # Note that for reasons of running time, J is set at a ridiculous low value. # One would typically use a number of iterations J of at least 1000, # and a burn_in of 200. out_2st_sim <- mHMM(s_data = data1$obs,                  gen = list(m = m, n_dep = n_dep, q_emiss = q_emiss),                  start_val = c(list(gamma), emiss_distr),                  mcmc = list(J = 11, burn_in = 5)) #> Progress of the Bayesian mHMM algorithm:  #>    |                                                                               |                                                                      |   0%   |                                                                               |========                                                              |  11%   |                                                                               |================                                                      |  22%   |                                                                               |=======================                                               |  33%   |                                                                               |===============================                                       |  44%   |                                                                               |=======================================                               |  56%   |                                                                               |===============================================                       |  67%   |                                                                               |======================================================                |  78%   |                                                                               |==============================================================        |  89%   |                                                                               |======================================================================| 100% #> Total time elapsed (hh:mm:ss): 00:00:01  ###### obtain the most likely state sequence with the Viterbi algorithm states <- vit_mHMM(s_data = data1$obs, object = out_2st_sim)"},{"path":"/news/index.html","id":"mhmmbayes-020","dir":"Changelog","previous_headings":"","what":"mHMMbayes 0.2.0","title":"mHMMbayes 0.2.0","text":"CRAN release: 2022-08-17","code":""},{"path":"/news/index.html","id":"speed-0-2-0","dir":"Changelog","previous_headings":"","what":"Speed","title":"mHMMbayes 0.2.0","text":"major improvement release increased speed mHMM() algorithm. forward algorithm used mHMM() now implemented c++ using Rcpp optimize computational speed call optim() mHMM() used create correct scalers Metropolis Hasting computationally intensive, especially long sequences data. new version, log likelihood function Multinomial distribution programmed efficient manner, obtaining Hessian based outcomes optim() done efficiently.","code":""},{"path":"/news/index.html","id":"manually-specifying-hyper-prior-distribution-paramter-values-0-2-0","dir":"Changelog","previous_headings":"","what":"Manually specifying hyper-prior distribution paramter values","title":"mHMMbayes 0.2.0","text":"Two new functions manually specify hyper-prior distribution parameter values multilevel hidden Markov model introduced: prior_emiss_cat(): manually specifying hyper-prior distribution parameter values categorical emission distribution(s), creating object class ‘mHMM_prior_emiss’. prior_gamma(): manually specifying hyper-prior distribution parameter values transition probability matrix gamma, creating object class ‘mHMM_prior_gamma’. Using manually specified hyper-prior distribution parameter values function mHMM() now thus done inputting object class ‘mHMM_prior_emiss’ /‘mHMM_prior_gamma’ input parameters emiss_hyp_prior gamma_hyp_prior, respectively, created functions. Note manually specifying hyper-prior distribution parameter values optional, default values available parameters.","code":""},{"path":"/news/index.html","id":"transforming-a-set-of-probabilities-to-multinomial-logit-regression-intercepts-and-vice-versa-0-2-0","dir":"Changelog","previous_headings":"","what":"Transforming a set of probabilities to Multinomial logit regression intercepts and vice versa","title":"mHMMbayes 0.2.0","text":"Manually specifying hyper-prior distribution parameter values done logit domain. , hyper-priors intercepts (, subject level covariates used, regression coefficients) Multinomial logit model used accommodate multilevel framework data, instead probabilities directly. logit domain might unfamiliar user compared probability domain, two functions introduced aid user: prob_to_int(): transforms set state transition categorical emission observation probabilities corresponding Multinomial logit regression intercepts. int_to_prob(): transforms set Multinomial logit regression intercepts corresponding state transition categorical emission observation probabilities","code":""},{"path":"/news/index.html","id":"manually-specifying-settings-of-the-proposal-distribution-of-the-random-walk-metropolis-sampler-0-2-0","dir":"Changelog","previous_headings":"","what":"Manually specifying settings of the proposal distribution of the Random Walk Metropolis sampler","title":"mHMMbayes 0.2.0","text":"Two new functions manually specify settings proposal distribution Random Walk (RW) Metropolis sampler multilevel hidden Markov model introduced: pd_RW_emiss_cat(): manually specifying setting RW proposal distribution categorical emission distribution(s), creating object class ‘mHMM_pdRW_emiss’. pd_RW_gamma(): manually specifying setting RW proposal distribution transition probability matrix gamma, creating object class ‘mHMM_pdRW_gamma’. Using manually specified settings proposal distribution Random Walk (RW) Metropolis sampler function mHMM() now thus done inputting object class ‘mHMM_pdRW_emiss’ /‘mHMM_pdRW_gamma’ input parameters emiss_sampler gamma_sampler, respectively, created functions. Note manually specifying setting RW proposal distribution optional, default values available parameters.","code":""},{"path":"/news/index.html","id":"sim_mhmm-0-2-0","dir":"Changelog","previous_headings":"","what":"sim_mHMM()","title":"mHMMbayes 0.2.0","text":"function sim_mHMM() used simulate data multiple subject - observation follow hidden Markov model (HMM) multilevel structure now allows simulation multivariate data. distributions multiple dependent variables multivariate data assumed independent given current hidden state. , input parameter ‘gen’ introduced, similar used e.g., function mHMM(). gen contains elements m, n_dep q_emiss. gen replaces input parameters m q_emiss. version, however, using m q_emiss specifying gen n_dep issues warning thus still allowed.","code":""},{"path":"/news/index.html","id":"other-minor-quite-technical-improvements-and-bug-fixes-0-2-0","dir":"Changelog","previous_headings":"","what":"Other minor (quite technical) improvements and bug fixes","title":"mHMMbayes 0.2.0","text":"implemented progress bar mHMM() indicate progress used algorithm probabilities returned summary(), obtain_emiss() obtain_gamma() now based MAP estimates intercepts Multinomial distribution instead MAP estimates probabilities. ensures returned probabilities sum 1. bug fix plotting subject posterior densities plot.mHMM(): iterations set 10 instead n_subj. fixed error tutorial vignette, section ‘Graphically displaying outcomes’. subject specific transition probability matrix gamma emission distributions: algorithm runs problems probabilities equal zero. avoid problem, small constant (0.0001) added probability iteration MCMC algorithm, done subject specific probabilities used forward algorithm, Multinomial intercept values group level parameters left untouched. intercept values gamma returned first iteration MCMC algorithm incorrect: gamma_int_bar[1,], transposed using .vector. matter calculations returned output help track intercept values gamma iteration MCMC algorithm. Fixed. fixed existing problems https://cran.rstudio.com//web/checks/check_results_mHMMbayes.html -> Escaped LaTeX specials: &","code":""},{"path":"/news/index.html","id":"mhmmbayes-0-1-1","dir":"Changelog","previous_headings":"","what":"mHMMbayes 0.1.1","title":"mHMMbayes 0.1.1","text":"CRAN release: 2019-10-30 Patch release solve noLD issues (tests without long double x86_64 Linux system) uncovered CRAN Package Check Results.","code":""},{"path":"/news/index.html","id":"mhmmbayes-0-1-1-1","dir":"Changelog","previous_headings":"","what":"mHMMbayes 0.1.0","title":"mHMMbayes 0.1.0","text":"CRAN release: 2019-10-30 First (official) version package!","code":""}]
